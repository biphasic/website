{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Event-based vision: CNN vs SNN\"\n",
    "\n",
    "commentable: true\n",
    "\n",
    "date: 2022-12-27\n",
    "lastmod: 2022-12-27\n",
    "draft: false\n",
    "\n",
    "tags: [\"SNN\", \"Event-based vision\"]\n",
    "summary: \"Why a CNN using a variable frame rate does the job.\"\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When recording data from event-based cameras, the camera normally timestamps events with a precision of microsecond resolution. This allows us to track extremely rapid motion such as a bullet shot from a gun or engine vibrations in the kHz range. A lot of day-to-day applications of event-based cameras however deal with things such as gesture recognition or other basic classification tasks, which record much slower movements. It is common practice to train a stateful spiking neural network to be able to integrate the information coming from the events which signify a change in illumination rather than an absolute grey-level value. But when training on GPUs, we always have to discretize time. So starting from microsecond resolution, how much of the information can you bin together until the network cannot distinguish it anymore?\n",
    "I'm going to show you that instead of employing costly backprop-through-time (BPTT), which scales linearly with the amount of time steps, we can simply integrate some of the input upfront and feed it to a stateless CNN.\n",
    "For that we're going to look at a classic benchmark, the IBM DVS gesture recognition task. It includes 11 hand gestures, some of which have clockwise/counter-clockwise which make it a bit harder for a stateless network to distinguish them.\n",
    "\n",
    "## The dataset\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/arm_roll.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/hand_clap.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/left_hand_clockwise.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/left_hand_counter_clockwise.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/left_hand_wave.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/right_hand_wave.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/right_hand_clockwise.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/right_hand_counter_clockwise.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/air_drums.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/air_guitar.gif)\n",
    "![](https://research.ibm.com/interactive/dvsgesture/images/other_gesture.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c00e5e7c7a569083cb991dfa106f557879cc0d1d84bf5b9d92fbb6bf680d358"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
