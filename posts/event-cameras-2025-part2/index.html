<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Event cameras in 2025, Part 2 | Gregor's Blog</title><meta name=keywords content="Event cameras"><meta name=description content="Technological challenges that are to be overcome before event cameras enter the mass market."><meta name=author content><link rel=canonical href=https://lenzgregor.com/posts/event-cameras-2025-part2/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://lenzgregor.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lenzgregor.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lenzgregor.com/favicon-32x32.png><link rel=apple-touch-icon href=https://lenzgregor.com/apple-touch-icon.png><link rel=mask-icon href=https://lenzgregor.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://lenzgregor.com/posts/event-cameras-2025-part2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.plot.ly/plotly-latest.min.js></script><meta property="og:url" content="https://lenzgregor.com/posts/event-cameras-2025-part2/"><meta property="og:site_name" content="Gregor's Blog"><meta property="og:title" content="Event cameras in 2025, Part 2"><meta property="og:description" content="Technological challenges that are to be overcome before event cameras enter the mass market."><meta property="og:locale" content="en-gb"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-13T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-13T22:38:37+00:00"><meta property="article:tag" content="Event Cameras"><meta name=twitter:card content="summary"><meta name=twitter:title content="Event cameras in 2025, Part 2"><meta name=twitter:description content="Technological challenges that are to be overcome before event cameras enter the mass market."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lenzgregor.com/posts/"},{"@type":"ListItem","position":2,"name":"Event cameras in 2025, Part 2","item":"https://lenzgregor.com/posts/event-cameras-2025-part2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Event cameras in 2025, Part 2","name":"Event cameras in 2025, Part 2","description":"Technological challenges that are to be overcome before event cameras enter the mass market.","keywords":["Event cameras"],"articleBody":"In Part 1 I provided a high-level overview of different industry sectors that could potentially see the adoption of event cameras. Apart from the challenge of finding the right application, there are several technological challenges before event cameras can reach a mass audience.\nSensor Capabilities Today’s most recent event cameras are summarised in the table below.\nCamera Supplier Sensor Model Name Year Resolution Dynamic Range (dB) Max Bandwidth (Mev/s) iniVation Gen2 DVS DAVIS346 2017 346×260 ~120 12 iniVation Gen3 DVS DVXPlorer 2020 640×480 90-110 165 Prophesee Sony IMX636 EVK4 2020 1280×720 120 1066 Prophesee GenX320 EVK3 2023 320×320 140 Samsung Gen4 DVS DVS-Gen4 2020 1280×960 1200 Insightness was sold to Sony, and CelePixel partnered with Omnivision, but hasn’t released a product in the past 5 years. Over the past decade, we have seen pixel arrays grow from 128x128 to 1280x720 (Prophesee’s HD sensor), but higher resolution is actually not always desirable. The last column in the table above describes the maximum number of million events per second that the sensor can handle, which results in GB/s of data for fast moving scenes. In addition, a paper by Gehrig and Scaramuzza suggests that in low-light and high-speed scenarios, performance of high-res cameras is actually worse than when using fewer, but bigger pixels, due to high per-pixel event rates that are noisy and cause ghosting artifacts.\nAs you can see from the table above, most of today’s event sensors are based on designs from 5 years ago. Different sectors have their own requirements depending on the application, so there are new designs underway. In areas such as defence, higher resolution and contrast sensitivity, as well as capturing the short/mid range infrared spectrum, is vital because range is so important. SCD USA made the MIRA 02Y-E available last year that includes an optional event-based readout, to enable tactical forces to detect laser sources. Using the event-based output, it advertises a frame rate of up to 1.2 kHz. In space, the distances to the captured objects are enormous, and therefore high resolution and light sensitivity are of utmost importance. As mentioned in Part 1, there are now companies focusing on building event sensors for aerospace and defence, given the growing allocation of resources in that sector.\nIn short-range applications such as eye tracking for wearables, a sensor at lower resolution but high dynamic range and ultra-low power modes is going to be more relevant. Prophesee’s GenX320 is designed for that.\nFor scientific applications, NovoViz recently announced a new SPAD (single photon avalanche diode) camera using event-based outputs, where outputting full frames would be way too costly.\nOver the next few years we’ll see new designs emerging, although I would argue that two decades of research using the binary event output format has mostly resulted in converting events to some form of image representation, in order to apply the tools and frameworks that are already mature. I think that’s why we’re seeing new hybrid vision sensors emerging, which try to rethink the event output format. At ISSCC 2023, two of three papers presenting new event sensors showed the introduction of asynchronous event frames.\nSensor Event output type Timing \u0026 synchronization Polarity info Typical max rate Sony 2.97 μm Binary event frames (two separate ON/OFF maps) Synchronous, ~580 µs “event frame” period 2 bits per pixel (positive \u0026 negative) ~1.4 GEvents/s Sony 1.22 μm, 35.6 MP Binary event frames with row-skipping \u0026 compression Variable frame sync, up to 10 kfps per RGB frame 2 bits per pixel (positive \u0026 negative) Up to 4.56 GEvents/s OmniVision 3-wafer Per-event address-event packets (x, y, t, polarity) Asynchronous, microsecond-level timestamps Single-bit polarity per event Up to 4.6 GEvents/s The Sony 2.97 μm chip uses aggressive circuit sharing so that four pixels share one comparator and analog front-end. Events are not streamed individually but are batched into binary event frames at fixed frequency every ~580 µs, with separate maps for ON and OFF polarity. This design keeps per-event energy extremely low (~57 pJ) and allows the sensor to reach ~1.4 GEvents/s without arbitration delays. The event output is already frame-like, and thus fits naturally into existing machine learning pipelines that expect regular image-like input at deterministic timing.\nThe Sony 1.22 μm hybrid sensor aimed at mobile devices combines a huge 35.6 MP RGB array with a 2 MP event array. Four 1.22 µm photodiodes form each event pixel (4.88 µm pitch). The event side operates in variable-rate event-frame mode, outputting up to 10 kfps inside each RGB frame period (see picture from the paper below). On-chip event-drop filters and compression dynamically reduce data volume while preserving critical motion information for downstream neural networks (e.g. deblurring or video frame interpolation). To me the asynchronous data capture of event frames that encode change seems like a practical way forward. I think that frame rates up to 100 Hz are sufficient for most applications.\nKodama et al. presented a Sony 1.22 μm hybrid sensor that outputs variable-rate binary event frames next to RGB.\nThe OmniVision 3-wafer is closer to the classic DVS concept but shows what’s possible: A dedicated 1MP event wafer with in-pixel time-to-digital converters stamps each event with microsecond accuracy. Skip-logic and four parallel readout channels give an impressive 4.6 GEvents/s throughput. It’s good for ultra-fast motion analysis or scientific experiments where every microsecond matters. Picture from the paper comparing RGB and event output below.\nGuo et al. presented a new generation of hybrid vision sensor that outputs binary events.\nI think that today’s binary microsecond spikes are rarely the right format for most applications. Much like Intel’s Loihi 2 shifted from binary spikes to richer spike payloads because they realised that the communication overhead was too high otherwise, future event cameras are becoming more practical by exploring frame-like representations. They could also emit something in between binary events and frames, such as multi-bit “micro-frames” or tokenizable spike packets. These would represent short-term local activity and could be directly ingested by ML models, reducing the need for preprocessing altogether.\nIdeally there’s a trade-off between information density and temporal resolution that can be chosen depending on the application. In either case, the event camera sensor has not reached its final form yet. People are still experimenting with how events should be represented in order to be compatible with modern machine learning methods.\nEvent Representations Most common approaches aggregate events into image-like representations such as 2d histograms, voxel grids, or time surfaces. These can then be used to fine-tune deep learning models that were pre-trained on RGB images. This leverages the breadth of existing tooling built for images and is compatible with GPU-accelerated training and inference. Moreover, it allows for adaptive frame rates, aggregating only when there’s activity and potentially saving on compute in case there’s little activity in the scene. But this method discards much of the fine temporal structure that event cameras provide today, and it’s also not as efficient as it could be: the tensors produced are full of zeros, and in order to get sufficient signal, you have to accumulate for hundreds of milliseconds if you’re capturing a slow activity. This becomes problematic for real-time applications where a long temporal context is needed but high responsiveness is crucial.\nWe still lack a representation for event streams that works well with modern ML architectures while preserving their sparsity. Event streams are a new data modality, just like images, audio, or text, but one for which we haven’t yet cracked the “tokenization problem.” At first sight, an event stream, one event after the other, is a perfect match for today’s capable sequence models. But a single binary event contains very little semantic information. Unlike a word in a sentence, which can encode a complex concept, even a dozen binary events reveal almost nothing about the scene. This makes direct tokenization of events inefficient. What we need is a representation that can summarize local spatiotemporal structure into meaningful, higher-level primitives, to represent event streams as a sequence of tokens that is directly dependent on activity in the scene. Less movement in the scene would result in fewer tokens being emitted, which saves compute.\nModels At their core, event cameras are change detectors, which means that we need memory in our machine learning models to remember where things were before they stopped moving. We can bake memory into the model architecture by using recurrence or attention. For example, Recurrent Vision Transformers and their variants maintain internal state across time and can handle temporally sparse inputs more naturally. These methods preserve temporal continuity, but there’s a catch: most of these methods still rely on dense, voxelized inputs. Even with more efficient state-space models replacing LSTMs and BPTT (Backpropagation Through Time) with much faster training strategies, we’re still processing a lot of zeros. Training is faster, but inference is still bottlenecked by inefficient representations. There are however some newer types of models that try to exploit the sparsity in event data, both temporally (inputs arrive irregularly) and spatially (any input has fewer zeros).\nGraph Neural Networks Graphs, especially dynamic, sparse graphs, are an interesting abstraction to explore. Each node could represent a small region of correlated activity in space and time, with edges encoding temporal or spatial relationships. Recent work such as DAGr, ACGR, eGSMV, or HUGNet v2 show that GNNs provide a natural fit for event data.\nDespite their differences, these papers converge on a common recipe: combine fast, event-level graph updates (for responsiveness at microsecond scales) with slower contextual aggregation (for stability and accuracy). DAGr fills blind time between low-rate frames using an asynchronous GNN; ACGR unifies frame and event nodes in a single sparse graph at ~200 Hz; eGSMV explicitly splits spatial vs. motion graphs; and HUGNet v2 mixes an event branch with a periodic aggregation branch, cutting prediction latency by three orders of magnitude while preserving accuracy. They avoid pure event-by-event updates because they are too noisy and costly, but batching everything into frames also defeats the purpose. GNNs strike a balance by structuring sparse events into dynamic graphs and then layering in context only where needed.\nThis hybrid design makes GNNs a strong candidate for the “tokenization” problem of event vision: they compress raw events into graph-structured tokens that carry more meaning than individual ON/OFF spikes, while remaining activity-driven and sparse. Still, these methods struggle with scalability because graph construction is memory- and bandwidth-heavy, and irregular node–edge layouts map poorly to today’s GPUs. Specialized accelerators for graph processing may ultimately be required if these representations are to run in real-world embedded systems. By combining event cameras with efficient “graph processors,” we could offload the task of building sparse graphs directly on-chip, producing representations that are ready for downstream learning. Temporally sparse, graph-based outputs could serve as a robust bridge between raw events and modern ML architectures.\nState-space Models Following the success of VMamba for RGB inputs, state-space models (SSMs) approach perception as a continuous-time dynamical system with a compact hidden state that can be discretized at any step size. This flexibility is particularly valuable for event cameras, because it allows a user to train at one input rate and deploy at another simply by changing the integration step, without needing to retrain. SSMs scale linearly in sequence length so you can extend (fine-grained) temporal context without exploding compute. They also maintain a cheap, always-on scene state that updates with each micro-batch of activity, which is particularly advantageous on embedded systems to save memory.\nZubić and colleagues show that combining S4/S5-style SSM layers with a lightweight Vision Transformer backbone leads to faster training that is about a third quicker than RNN-based recurrent transformers, and much smaller accuracy loss when the input frequency at deployment is higher than during training.\nYang et al. introduced SMamba, which builds on the Mamba/SSM idea and adds adaptive sparsification. By estimating spatio-temporal continuity, it discards blank or noisy tokens, prioritizes scans so informative regions interact earlier, and mixes channels through a global channel interaction step. On datasets such as Gen1, 1Mpx, and eTram, this approach reduces FLOPs by roughly 22–31 % relative to its dense baseline.\nFor optical flow, a spatio-temporal SSM encoder with convex upsampling can estimate dense flow from a single event volume, bypassing RAFT-style iterative refinement. This method achieves about 4.5 times faster inference and roughly eight times fewer MACs than a recent iterative method (TMA) while maintaining competitive endpoint error on DSEC and being about twice as efficient as EV-FlowNet, illustrating that SSMs can replace costly recurrence while preserving temporal precision.\nPRE-Mamba’s approach is interesting because it turns the event camera’s weakness of generating large amounts of data for dynamic scenes into a benefit. The authors use a multi-scale SSM inside a point-based pipeline over a 4D event cloud for weather-robust event deraining. The key architecture lesson is that minimal spatio-temporal clustering combined with an SSM can carry long temporal context efficiently and with a small parameter footprint.\nSeveral practical guidelines emerge for building with SSMs. They make it feasible to train once and deploy anywhere along the time axis: if a system must run at 10 Hz in the lab and 100 Hz on-device, it is enough to rescale the discretization step without any fine-tuning. In terms of architecture, the most stable pattern across papers is to use an SSM for temporal aggregation paired with a lightweight spatial mixer, such as local attention or a convolution, which preserves long memory without incurring transformer-scale spatial costs. Efficiency can be improved by exploiting sparsity without sacrificing global context: instead of relying on purely local or windowed attention, prune tokens based on spatio-temporal continuity, discard obvious background or noise, and still scan globally over what remains, following the SMamba strategy. For deployment, diagonal or parallel-scan variants such as S4D, S5, or Mamba-style selective scans are recommended because they run naturally in streaming mode.\nFor event vision, SSMs provide an effective “scene memory” primitive because they can handle long sequences more efficiently than transformers and support variable timing. The emerging recipe that scales well is to use small adaptive windows of voxel grids, micro-frames, or serialized patches, add light spatial mixing, apply an SSM for temporal modeling, and optionally incorporate sparsification to skip inactive regions. This keeps latency low when activity is sparse and allows increasing batch size during heavy traffic without rewriting the model or retraining for different rates.\nSpiking neural networks Biological neurons are extremely complex, and there’s a whole world to be modeled in each cell. The Chan-Zuckerberg initiative on virtual cells, and also DeepMind’s cell simulations go to great lengths in an effort to model it. Because cells are so complex, projects like the Human Brain Project (HBP), tried to simulate brain activity by using higher-level abstractions on a larger scale. HBP helped pave the way for spiking neural networks (SNNs), which are sometimes touted as a natural fit for event data. But in their traditional form, with binary activations and reset mechanisms, researchers get too attached to handcrafted abstractions, such as leaky integrate-and-fire (LIF) models, which have shown inferior performance compared to other architectures. I’m reminded of an Open Neuromorphic talk by Timoleon Moraitis last year, where he talks about drawing inspiration from biological principles, without dogmatically copying them.\nDeep learning started out with 32-bit floating point, dense representations, and neuromorphic started out on the other end of the spectrum at binary, extremely sparse representations. They are converging, with neuromorphic realising that binary events are expensive to transmit and process, and deep learning embracing 4-bit activations and 2:4 structured weight sparsity. A recent paper even proposes binary neural networks, which shows that the research community is surprisingly resistant to the learnings of the past. So let’s not get hung up on the artificial neuron model itself, and instead use what works well, because the field of machine learning is moving incredibly fast.\nOne more note on scale. SNNs in their classic form are plagued with all the difficulties of training RNNs, which means that it’s super slow to train and therefore unfeasible to scale up. When I initiated the SNN library benchmarks to find out which library can train the fastest, I got a snazzy comment by someone high up at Intel Labs, saying that neuromorphic is all about the need to find novel algorithms, and not about training fast. But that’s simply not true. Scale is important, and for scale you need fast training. There’s a strong bias in the neuromorphic community to focus on tiny models and efficiency. But nowadays, in order to get an efficient edge model that generalises well, you first want a larger AI model that you can optimise, using pruning, distillation, or quantisation. Or you use it as a teacher, and make the tiny model a student of a larger model. Even a 50-fold reduction in power consumption won’t convince any client if the model cannot cope with some distribution shifts in the input.\nProcessors Running models efficiently on event data is as much a hardware problem as a modelling one. GPUs remain the default accelerator, but they are poorly matched to the irregular memory access patterns of event streams. Even if the inputs are sparse, most compute in intermediate layers ends up dense, so energy savings from “skipping zeros” specifically for event inputs are negligible.\nState-space models have recently been shown to run well on neuromorphic substrates: for example, Meyer et al. mapped an S4D variant to Intel’s Loihi 2, exploiting diagonalized state updates to reduce inter-core traffic and outperforming a Jetson GPU in true online inference. This demonstrates that compact, recurrent stateful models can benefit from specialized hardware when communication costs dominate. Intel’s Loihi was the most advanced neuromorphic chip, and since large-scale modelling of the brain and constrained optimizations they’ve come a long way to exploit the asynchronous hardware in a more practical way. I would love to see an efficient edge chip coming out of this, although that’s not really Intel’s game I’m afraid.\nFor GNN-based event processing, new dedicated accelerators are emerging. The EvGNN hardware prototype (IEEE 2024) shows that integrating graph construction and message passing directly on-chip can reduce latency by an order of magnitude and improve energy efficiency by over 10× compared to GPUs. Crucially, it processes events asynchronously, triggering computation only when new data arrives, which aligns naturally with the event-camera principle. This kind of co-design, sensor output coupled tightly with graph-oriented hardware, may be necessary if graph representations are to be deployed beyond research demos.\nSome argue that because event cameras output extremely sparse data, we can save energy by skipping zeros in the input or in intermediate activations. But while the input might be much sparser than an RGB frame, the bulk of the computation happens in intermediate layers and works with higher-level representations, which are similar for both RGB and event inputs. Thus in AI accelerators we can’t exploit spatial event camera sparsity, and inference cost between RGB and event frames are essentially the same for reasonably sized models. We might get different input frame rates / temporal sparsity, but those can be exploited on GPUs as well.\nOn mixed-signal hardware, rules are different, but maintaining state is always expensive. You pay in power (multiplexing) or chip size (analog). A basic rule for analog to my understanding is: if you need to convert from analog to digital too often, for error correction or because you’re storing states, your efficiency gains go out of the window. Mythic AI had to painfully learn that and almost tanked, and also Rain AI pivoted from its original analog hardware and faces an uncertain future. The brain uses a mixture of analog (graded potentials, dendritic integration) and digital (spikes) signals and we can replicate this principle in silicon. But since the circuitry is the memory at the same time, it needs an incredible amount of space, and is organised in 3d. That’s really costly to do in silicon, and the major challenge is getting the heat out, which is much easier in 2d.\nThe asynchronous compute principle is key for event cameras, but naïve asynchrony is not constructive. Think about cars entering a roundabout, and the flow of traffic without any traffic lights. When the traffic volume is low, every car is more or less in constant motion, and latency to cross the roundabout is minimal. As the volume of traffic grows, a roundabout becomes inefficient, because the movement of any car depends on the decisions of cars nearby. For high traffic flow, it becomes more efficient to use traffic lights to batch process the traffic for multiple lanes at once, which achieves the highest throughput of cars. The same principle applies for events. When you have few pixels activated, you achieve the lowest latency when you process them as they come in one by one, like cars in a roundabout. But as the amount of events / s increases, you will want to process the events in batches, like with traffic lights. Ideally the size of the batch depends on the event rate.\nFor more info about neuromorphic chips, I refer you to Open Neuromorphic’s Hardware Guide.\nConclusion Event cameras won’t reach mainstream adoption until they break away from the legacy of microsecond-precision binary spikes and embrace output formats that carry richer, more structured information. The core challenge is representation: modern ML systems are built around structured tokens such as patches, words, or embeddings, not floods of binary impulses.\nA workable solution will likely follow a two-stage architecture: a lightweight, streaming tokenizer that aggregates local spatiotemporal activity into short-lived micro-features, followed by a stateful temporal model that reasons over these features efficiently. Such representations preserve sparsity, maintain temporal fidelity, and scale naturally under variable scene activity.\nIf I had to bet on a commercial trajectory today, it would be hybrid sensors that pair variable-rate event frames with standard RGB output, producing a sparse token stream processed by compact state-space models on embedded GPUs or specialized edge accelerators. Sacrificing some raw temporal resolution in exchange for more semantically meaningful, compressible aggregates is a practical and likely necessary trade-off. The goal is not to create pretty images but to produce machine-readable signals that map cleanly onto existing AI hardware.\nThe scalable recipe looks something like this: generate tokens that carry meaning, train them with a mix of cross-modal supervision and self-supervision that reflects real sensor noise, maintain a compact and cheap-to-update scene memory, and make computation conditional on activity rather than a fixed clock. Key research directions include dynamic graph representations for efficient tokenization, state-space models for low-latency inference at the edge, and lossy compression techniques that can shrink event streams without destroying semantic content.\nFinally, application needs should guide sensor and model design. Gesture recognition doesn’t require microsecond timing. Eye tracking doesn’t need megapixel resolution. And sometimes a motion sensor-triggered RGB camera is the most pragmatic solution. Event cameras don’t need to replace conventional vision, they just need to become usable enough, in the right form, to complement it.\n","wordCount":"3820","inLanguage":"en","datePublished":"2025-11-13T00:00:00Z","dateModified":"2025-11-13T22:38:37Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://lenzgregor.com/posts/event-cameras-2025-part2/"},"publisher":{"@type":"Organization","name":"Gregor's Blog","logo":{"@type":"ImageObject","url":"https://lenzgregor.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lenzgregor.com/ accesskey=h title="Gregor's Blog (Alt + H)">Gregor's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lenzgregor.com/ title=Home><span>Home</span></a></li><li><a href=https://lenzgregor.com/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://lenzgregor.com/>Home</a>&nbsp;»&nbsp;<a href=https://lenzgregor.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Event cameras in 2025, Part 2</h1><div class=post-meta><span title='2025-11-13 00:00:00 +0000 UTC'>November 13, 2025</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;3820 words</div></header><div class=post-content><p>In <a href=https://lenzgregor.com/posts/event-cameras-2025-part1/>Part 1</a> I provided a high-level overview of different industry sectors that could potentially see the adoption of event cameras. Apart from the challenge of finding the right application, there are several technological challenges before event cameras can reach a mass audience.</p><h2 id=sensor-capabilities>Sensor Capabilities<a hidden class=anchor aria-hidden=true href=#sensor-capabilities>#</a></h2><p>Today&rsquo;s most recent event cameras are summarised in the table below.</p><table><thead><tr><th>Camera Supplier</th><th>Sensor</th><th>Model Name</th><th>Year</th><th>Resolution</th><th>Dynamic Range (dB)</th><th>Max Bandwidth (Mev/s)</th></tr></thead><tbody><tr><td>iniVation</td><td>Gen2 DVS</td><td><a href=https://docs.inivation.com/hardware/current-products/davis346.html>DAVIS346</a></td><td>2017</td><td>346×260</td><td>~120</td><td>12</td></tr><tr><td>iniVation</td><td>Gen3 DVS</td><td><a href=https://docs.inivation.com/hardware/current-products/dvxplorer.html>DVXPlorer</a></td><td>2020</td><td>640×480</td><td>90-110</td><td>165</td></tr><tr><td>Prophesee</td><td><a href=https://www.prophesee.ai/event-based-sensor-imx636-sony-prophesee/>Sony IMX636</a></td><td><a href=https://www.prophesee.ai/event-camera-evk4/>EVK4</a></td><td>2020</td><td>1280×720</td><td>120</td><td>1066</td></tr><tr><td>Prophesee</td><td><a href=https://www.prophesee.ai/event-based-sensor-genx320/>GenX320</a></td><td><a href=https://www.prophesee.ai/evk-3-genx320-info/>EVK3</a></td><td>2023</td><td>320×320</td><td>140</td><td></td></tr><tr><td>Samsung</td><td>Gen4 DVS</td><td>DVS-Gen4</td><td>2020</td><td>1280×960</td><td></td><td>1200</td></tr></tbody></table><p>Insightness was sold to Sony, and CelePixel partnered with Omnivision, but hasn&rsquo;t released a product in the past 5 years. Over the past decade, we have seen pixel arrays grow from 128x128 to 1280x720 (Prophesee&rsquo;s HD sensor), but higher resolution is actually not always desirable. The last column in the table above describes the maximum number of million events per second that the sensor can handle, which results in GB/s of data for fast moving scenes. In addition, a paper by <a href=https://arxiv.org/abs/2203.14672>Gehrig and Scaramuzza</a> suggests that in low-light and high-speed scenarios, performance of high-res cameras is actually worse than when using fewer, but bigger pixels, due to high per-pixel event rates that are noisy and cause ghosting artifacts.</p><p>As you can see from the table above, most of today&rsquo;s event sensors are based on designs from 5 years ago. Different sectors have their own requirements depending on the application, so there are new designs underway.
In areas such as defence, higher resolution and contrast sensitivity, as well as capturing the short/mid range infrared spectrum, is vital because range is so important. SCD USA made the <a href=https://scdusa-ir.com/wp-content/uploads/2024/06/Mira_V1g.pdf>MIRA 02Y-E</a> available last year that includes an optional event-based readout, to enable tactical forces to detect laser sources. Using the event-based output, it advertises a frame rate of up to 1.2 kHz.
In space, the distances to the captured objects are enormous, and therefore high resolution and light sensitivity are of utmost importance.
As mentioned in <a href=https://lenzgregor.com/posts/event-cameras-2025-part1/>Part 1</a>, there are now companies focusing on building event sensors for aerospace and defence, given the growing allocation of resources in that sector.</p><p>In short-range applications such as eye tracking for wearables, a sensor at lower resolution but high dynamic range and ultra-low power modes is going to be more relevant. Prophesee&rsquo;s <a href=https://www.prophesee.ai/event-based-sensor-genx320/>GenX320</a> is designed for that.<br>For scientific applications, NovoViz recently <a href=https://www.tokyoupdates.metro.tokyo.lg.jp/en/post-1551/>announced</a> a new SPAD (single photon avalanche diode) camera using event-based outputs, where outputting full frames would be way too costly.</p><p>Over the next few years we&rsquo;ll see new designs emerging, although I would argue that two decades of research using the binary event output format has mostly resulted in converting events to some form of image representation, in order to apply the tools and frameworks that are already mature.
I think that&rsquo;s why we&rsquo;re seeing new hybrid vision sensors emerging, which try to rethink the event output format. At ISSCC 2023, two of three papers presenting new event sensors showed the introduction of asynchronous <em>event frames</em>.</p><table><thead><tr><th>Sensor</th><th>Event output type</th><th>Timing & synchronization</th><th>Polarity info</th><th>Typical max rate</th></tr></thead><tbody><tr><td><a href=https://ieeexplore.ieee.org/document/10067566>Sony 2.97 μm</a></td><td>Binary event frames (two separate ON/OFF maps)</td><td>Synchronous, ~580 µs “event frame” period</td><td>2 bits per pixel (positive & negative)</td><td>~1.4 GEvents/s</td></tr><tr><td><a href=https://ieeexplore.ieee.org/document/10067520>Sony 1.22 μm, 35.6 MP</a></td><td>Binary event frames with row-skipping & compression</td><td>Variable frame sync, up to 10 kfps per RGB frame</td><td>2 bits per pixel (positive & negative)</td><td>Up to 4.56 GEvents/s</td></tr><tr><td><a href=https://ieeexplore.ieee.org/document/10067476>OmniVision 3-wafer</a></td><td>Per-event address-event packets (x, y, t, polarity)</td><td>Asynchronous, microsecond-level timestamps</td><td>Single-bit polarity per event</td><td>Up to 4.6 GEvents/s</td></tr></tbody></table><p>The Sony 2.97 μm chip uses aggressive circuit sharing so that four pixels share one comparator and analog front-end. Events are not streamed individually but are batched into binary event frames at fixed frequency every ~580 µs, with separate maps for ON and OFF polarity. This design keeps per-event energy extremely low (~57 pJ) and allows the sensor to reach ~1.4 GEvents/s without arbitration delays. The event output is already frame-like, and thus fits naturally into existing machine learning pipelines that expect regular image-like input at deterministic timing.</p><p>The Sony 1.22 μm hybrid sensor aimed at mobile devices combines a huge 35.6 MP RGB array with a 2 MP event array. Four 1.22 µm photodiodes form each event pixel (4.88 µm pitch). The event side operates in variable-rate event-frame mode, outputting up to 10 kfps inside each RGB frame period (see picture from the paper below). On-chip event-drop filters and compression dynamically reduce data volume while preserving critical motion information for downstream neural networks (e.g. deblurring or video frame interpolation). To me the asynchronous data capture of <em>event frames</em> that encode change seems like a practical way forward. I think that frame rates up to 100 Hz are sufficient for most applications.</p><p><img alt=hybrid-vision-sensor-sony loading=lazy src=/posts/event-cameras-2025-part2/images/hvs-sony.png>
<em><a href=https://ieeexplore.ieee.org/document/10067520>Kodama et al.</a> presented a Sony 1.22 μm hybrid sensor that outputs variable-rate binary event frames next to RGB.</em></p><p>The OmniVision 3-wafer is closer to the classic DVS concept but shows what&rsquo;s possible: A dedicated 1MP event wafer with in-pixel time-to-digital converters stamps each event with microsecond accuracy. Skip-logic and four parallel readout channels give an impressive 4.6 GEvents/s throughput. It&rsquo;s good for ultra-fast motion analysis or scientific experiments where every microsecond matters. Picture from the paper comparing RGB and event output below.</p><p><img alt=hybrid-vision-sensor-sony loading=lazy src=/posts/event-cameras-2025-part2/images/hvs-omnivision.png>
<em><a href=https://ieeexplore.ieee.org/document/10067476>Guo et al.</a> presented a new generation of hybrid vision sensor that outputs binary events.</em></p><p>I think that today’s binary microsecond spikes are rarely the right format for most applications. Much like Intel’s <a href=https://open-neuromorphic.org/neuromorphic-computing/hardware/loihi-2-intel/>Loihi 2</a> shifted from binary spikes to richer spike payloads because they realised that the communication overhead was too high otherwise, future event cameras are becoming more practical by exploring frame-like representations. They could also emit something in between binary events and frames, such as multi-bit “micro-frames” or tokenizable spike packets. These would represent short-term local activity and could be directly ingested by ML models, reducing the need for preprocessing altogether.</p><p>Ideally there’s a trade-off between information density and temporal resolution that can be chosen depending on the application.
In either case, the event camera sensor has not reached its final form yet. People are still experimenting with how events should be represented in order to be compatible with modern machine learning methods.</p><h2 id=event-representations>Event Representations<a hidden class=anchor aria-hidden=true href=#event-representations>#</a></h2><p>Most common approaches aggregate events into <a href=https://tonic.readthedocs.io/en/latest/auto_examples/index.html#event-representations>image-like representations</a> such as 2d histograms, voxel grids, or time surfaces. These can then be used to fine-tune deep learning models that were pre-trained on RGB images. This leverages the breadth of existing tooling built for images and is compatible with GPU-accelerated training and inference. Moreover, it allows for adaptive frame rates, aggregating only when there’s activity and potentially saving on compute in case there’s little activity in the scene. But this method discards much of the fine temporal structure that event cameras provide today, and it’s also not as efficient as it could be: the tensors produced are full of zeros, and in order to get sufficient signal, you have to accumulate for hundreds of milliseconds if you’re capturing a slow activity. This becomes problematic for real-time applications where a long temporal context is needed but high responsiveness is crucial.</p><p>We still lack a representation for event streams that works well with modern ML architectures while preserving their sparsity. Event streams are a new data modality, just like images, audio, or text, but one for which we haven’t yet cracked the “tokenization problem.” At first sight, an event stream, one event after the other, is a perfect match for today’s capable sequence models. But a single binary event contains very little semantic information. Unlike a word in a sentence, which can encode a complex concept, even a dozen binary events reveal almost nothing about the scene. This makes direct tokenization of events inefficient. What we need is a representation that can summarize local spatiotemporal structure into meaningful, higher-level primitives, to represent event streams as a sequence of tokens that is directly dependent on activity in the scene. Less movement in the scene would result in fewer tokens being emitted, which saves compute.</p><h2 id=models>Models<a hidden class=anchor aria-hidden=true href=#models>#</a></h2><p>At their core, event cameras are change detectors, which means that we need memory in our machine learning models to remember where things were before they stopped moving.
We can bake memory into the model architecture by using recurrence or attention. For example, <a href=https://openaccess.thecvf.com/content/CVPR2023/html/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.html>Recurrent Vision Transformers</a> and their variants maintain internal state across time and can handle temporally sparse inputs more naturally. These methods preserve temporal continuity, but there’s a catch: most of these methods still rely on dense, voxelized inputs. Even with more efficient <a href=https://openaccess.thecvf.com/content/CVPR2024/html/Zubic_State_Space_Models_for_Event_Cameras_CVPR_2024_paper.html>state-space models</a> replacing LSTMs and BPTT (Backpropagation Through Time) with much faster training strategies, we’re still processing a lot of zeros. Training is faster, but inference is still bottlenecked by inefficient representations. There are however some newer types of models that try to exploit the sparsity in event data, both temporally (inputs arrive irregularly) and spatially (any input has fewer zeros).</p><h3 id=graph-neural-networks>Graph Neural Networks<a hidden class=anchor aria-hidden=true href=#graph-neural-networks>#</a></h3><p>Graphs, especially dynamic, sparse graphs, are an interesting abstraction to explore. Each node could represent a small region of correlated activity in space and time, with edges encoding temporal or spatial relationships. Recent work such as <a href=https://www.nature.com/articles/s41586-024-07409-w>DAGr</a>, <a href=https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Asynchronous_Collaborative_Graph_Representation_for_Frames_and_Events_CVPR_2025_paper.pdf>ACGR</a>, <a href=https://arxiv.org/abs/2507.15150>eGSMV</a>, or <a href=https://openaccess.thecvf.com/content/CVPR2025/html/Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for_CVPR_2025_paper.html>HUGNet v2</a> show that GNNs provide a natural fit for event data.</p><p>Despite their differences, these papers converge on a common recipe: combine fast, event-level graph updates (for responsiveness at microsecond scales) with slower contextual aggregation (for stability and accuracy). DAGr fills blind time between low-rate frames using an asynchronous GNN; ACGR unifies frame and event nodes in a single sparse graph at ~200 Hz; eGSMV explicitly splits spatial vs. motion graphs; and HUGNet v2 mixes an event branch with a periodic aggregation branch, cutting prediction latency by three orders of magnitude while preserving accuracy. They avoid pure event-by-event updates because they are too noisy and costly, but batching everything into frames also defeats the purpose. GNNs strike a balance by structuring sparse events into dynamic graphs and then layering in context only where needed.</p><p>This hybrid design makes GNNs a strong candidate for the “tokenization” problem of event vision: they compress raw events into graph-structured tokens that carry more meaning than individual ON/OFF spikes, while remaining activity-driven and sparse. Still, these methods struggle with scalability because graph construction is memory- and bandwidth-heavy, and irregular node–edge layouts map poorly to today’s GPUs. Specialized accelerators for graph processing may ultimately be required if these representations are to run in real-world embedded systems. By combining event cameras with efficient “graph processors,” we could offload the task of building sparse graphs directly on-chip, producing representations that are ready for downstream learning. Temporally sparse, graph-based outputs could serve as a robust bridge between raw events and modern ML architectures.</p><h3 id=state-space-models>State-space Models<a hidden class=anchor aria-hidden=true href=#state-space-models>#</a></h3><p>Following the success of <a href=https://arxiv.org/abs/2401.10166>VMamba</a> for RGB inputs, state-space models (SSMs) approach perception as a continuous-time dynamical system with a compact hidden state that can be discretized at any step size. This flexibility is particularly valuable for event cameras, because it allows a user to train at one input rate and deploy at another simply by changing the integration step, without needing to retrain. SSMs scale linearly in sequence length so you can extend (fine-grained) temporal context without exploding compute. They also maintain a cheap, always-on scene state that updates with each micro-batch of activity, which is particularly advantageous on embedded systems to save memory.</p><p><a href=https://openaccess.thecvf.com/content/CVPR2024/html/Zubic_State_Space_Models_for_Event_Cameras_CVPR_2024_paper.html>Zubić and colleagues</a> show that combining S4/S5-style SSM layers with a lightweight Vision Transformer backbone leads to faster training that is about a third quicker than RNN-based recurrent transformers, and much smaller accuracy loss when the input frequency at deployment is higher than during training.</p><p><a href=https://arxiv.org/abs/2501.11971>Yang et al.</a> introduced SMamba, which builds on the Mamba/SSM idea and adds adaptive sparsification. By estimating spatio-temporal continuity, it discards blank or noisy tokens, prioritizes scans so informative regions interact earlier, and mixes channels through a global channel interaction step. On datasets such as Gen1, 1Mpx, and eTram, this approach reduces FLOPs by roughly 22–31 % relative to its dense baseline.</p><p>For optical flow, a spatio-temporal SSM encoder with convex upsampling can estimate dense flow from a single event volume, bypassing RAFT-style iterative refinement. This method achieves about 4.5 times faster inference and roughly eight times fewer MACs than a recent iterative method (TMA) while maintaining competitive endpoint error on DSEC and being about twice as efficient as EV-FlowNet, illustrating that SSMs can replace costly recurrence while preserving temporal precision.</p><p><a href=https://arxiv.org/abs/2505.05307>PRE-Mamba</a>&rsquo;s approach is interesting because it turns the event camera&rsquo;s weakness of generating large amounts of data for dynamic scenes into a benefit. The authors use a multi-scale SSM inside a point-based pipeline over a 4D event cloud for weather-robust event deraining. The key architecture lesson is that minimal spatio-temporal clustering combined with an SSM can carry long temporal context efficiently and with a small parameter footprint.</p><p>Several practical guidelines emerge for building with SSMs. They make it feasible to train once and deploy anywhere along the time axis: if a system must run at 10 Hz in the lab and 100 Hz on-device, it is enough to rescale the discretization step without any fine-tuning. In terms of architecture, the most stable pattern across papers is to use an SSM for temporal aggregation paired with a lightweight spatial mixer, such as local attention or a convolution, which preserves long memory without incurring transformer-scale spatial costs. Efficiency can be improved by exploiting sparsity without sacrificing global context: instead of relying on purely local or windowed attention, prune tokens based on spatio-temporal continuity, discard obvious background or noise, and still scan globally over what remains, following the SMamba strategy. For deployment, diagonal or parallel-scan variants such as S4D, S5, or Mamba-style selective scans are recommended because they run naturally in streaming mode.</p><p>For event vision, SSMs provide an effective &ldquo;scene memory&rdquo; primitive because they can handle long sequences more efficiently than transformers and support variable timing. The emerging recipe that scales well is to use small adaptive windows of voxel grids, micro-frames, or serialized patches, add light spatial mixing, apply an SSM for temporal modeling, and optionally incorporate sparsification to skip inactive regions. This keeps latency low when activity is sparse and allows increasing batch size during heavy traffic without rewriting the model or retraining for different rates.</p><h3 id=spiking-neural-networks>Spiking neural networks<a hidden class=anchor aria-hidden=true href=#spiking-neural-networks>#</a></h3><p>Biological neurons are extremely complex, and there&rsquo;s a whole world to be modeled in each cell. The Chan-Zuckerberg initiative on <a href=https://chanzuckerberg.com/science/technology/virtual-cells/>virtual cells</a>, and also DeepMind&rsquo;s <a href=https://analyticsindiamag.com/ai-features/inside-google-deepminds-bold-vision-for-virtual-cell/>cell simulations</a> go to great lengths in an effort to model it.
Because cells are so complex, projects like the Human Brain Project (HBP), tried to simulate brain activity by using higher-level abstractions on a larger scale. HBP helped pave the way for spiking neural networks (SNNs), which are sometimes touted as a natural fit for event data. But in their traditional form, with binary activations and reset mechanisms, researchers get too attached to handcrafted abstractions, such as leaky integrate-and-fire (LIF) models, which have shown inferior performance compared to other architectures. I&rsquo;m reminded of an Open Neuromorphic <a href="https://www.youtube.com/live/5qctRLrVTKg?si=tLVi0MnbiVw9hwGo">talk by Timoleon Moraitis</a> last year, where he talks about drawing inspiration from biological principles, without dogmatically copying them.</p><p>Deep learning started out with 32-bit floating point, dense representations, and neuromorphic started out on the other end of the spectrum at binary, extremely sparse representations. They are converging, with neuromorphic realising that binary events are expensive to transmit and process, and deep learning embracing 4-bit activations and 2:4 structured weight sparsity. A recent paper even proposes <a href=https://arxiv.org/abs/2509.07025>binary neural networks</a>, which shows that the research community is surprisingly resistant to the learnings of the past.
So let&rsquo;s not get hung up on the artificial neuron model itself, and instead use what works well, because the field of machine learning is moving incredibly fast.</p><p>One more note on scale. SNNs in their classic form are plagued with all the difficulties of training RNNs, which means that it&rsquo;s super slow to train and therefore unfeasible to scale up. When I initiated the SNN library <a href=https://open-neuromorphic.org/blog/spiking-neural-network-framework-benchmarking/>benchmarks</a> to find out which library can train the fastest, I got a snazzy comment by someone high up at Intel Labs, saying that neuromorphic is all about the need to find novel algorithms, and not about training fast. But that&rsquo;s simply not true. Scale is important, and for scale you need fast training.
There&rsquo;s a strong bias in the neuromorphic community to focus on tiny models and efficiency. But nowadays, in order to get an efficient edge model that generalises well, you first want a larger AI model that you can optimise, using pruning, distillation, or quantisation. Or you use it as a teacher, and make the tiny model a <a href=https://www.nature.com/articles/s41598-025-94205-9.pdf>student</a> of a larger model. Even a 50-fold reduction in power consumption won&rsquo;t convince any client if the model cannot cope with some distribution shifts in the input.</p><h2 id=processors>Processors<a hidden class=anchor aria-hidden=true href=#processors>#</a></h2><p>Running models efficiently on event data is as much a hardware problem as a modelling one. GPUs remain the default accelerator, but they are poorly matched to the irregular memory access patterns of event streams. Even if the inputs are sparse, most compute in intermediate layers ends up dense, so energy savings from “skipping zeros” specifically for event inputs are negligible.</p><p>State-space models have recently been shown to run well on neuromorphic substrates: for example, <a href=https://ieeexplore.ieee.org/abstract/document/11065663>Meyer et al.</a> mapped an S4D variant to Intel’s Loihi 2, exploiting diagonalized state updates to reduce inter-core traffic and outperforming a Jetson GPU in true online inference. This demonstrates that compact, recurrent stateful models can benefit from specialized hardware when communication costs dominate. Intel&rsquo;s Loihi was the most advanced neuromorphic chip, and since large-scale modelling of the brain and constrained optimizations they&rsquo;ve come a long way to exploit the asynchronous hardware in a more practical way. I would love to see an efficient edge chip coming out of this, although that&rsquo;s not really Intel&rsquo;s game I&rsquo;m afraid.</p><p>For GNN-based event processing, new dedicated accelerators are emerging. The <a href=https://ieeexplore.ieee.org/abstract/document/10812004>EvGNN hardware</a>
prototype (IEEE 2024) shows that integrating graph construction and message passing directly on-chip can reduce latency by an order of magnitude and improve energy efficiency by over 10× compared to GPUs. Crucially, it processes events asynchronously, triggering computation only when new data arrives, which aligns naturally with the event-camera principle. This kind of co-design, sensor output coupled tightly with graph-oriented hardware, may be necessary if graph representations are to be deployed beyond research demos.</p><p>Some argue that because event cameras output extremely sparse data, we can save energy by skipping zeros in the input or in intermediate activations. But while the input might be much sparser than an RGB frame, the bulk of the computation happens in intermediate layers and works with higher-level representations, which are similar for both RGB and event inputs. Thus in AI accelerators we can&rsquo;t exploit spatial event camera sparsity, and inference cost between RGB and event frames are essentially the same for reasonably sized models. We might get different input frame rates / temporal sparsity, but those can be exploited on GPUs as well.</p><p>On mixed-signal hardware, rules are different, but maintaining state is always expensive. You pay in power (multiplexing) or chip size (analog). A basic rule for analog to my understanding is: if you need to convert from analog to digital too often, for error correction or because you&rsquo;re storing states, your efficiency gains go out of the window. <a href=https://mythic.ai/>Mythic AI</a> had to painfully learn that and <a href=https://www.reddit.com/r/technology/comments/yvjgwu/analog_ai_chip_startup_mythic_runs_out_of_money/>almost tanked</a>, and also <a href=https://rain.ai/>Rain AI</a> pivoted from its original analog hardware and faces <a href=https://startupwired.com/2025/05/16/rain-ai-the-rise-and-fall-of-a-chipmaking-challenger/>an uncertain future</a>. The brain uses a mixture of analog (graded potentials, dendritic integration) and digital (spikes) signals and we can replicate this principle in silicon. But since the circuitry is the memory at the same time, it needs an incredible amount of space, and is organised in 3d. That&rsquo;s really costly to do in silicon, and the major challenge is getting the heat out, which is much easier in 2d.</p><p>The asynchronous compute principle is key for event cameras, but naïve asynchrony is not constructive. Think about cars entering a roundabout, and the flow of traffic without any traffic lights. When the traffic volume is low, every car is more or less in constant motion, and latency to cross the roundabout is minimal. As the volume of traffic grows, a roundabout becomes inefficient, because the movement of any car depends on the decisions of cars nearby. For high traffic flow, it becomes more efficient to use traffic lights to <code>batch process</code> the traffic for multiple lanes at once, which achieves the highest throughput of cars.
The same principle applies for events. When you have few pixels activated, you achieve the lowest latency when you process them as they come in one by one, like cars in a roundabout. But as the amount of events / s increases, you will want to process the events in batches, like with traffic lights. Ideally the size of the batch depends on the event rate.</p><p>For more info about neuromorphic chips, I refer you to <a href=https://open-neuromorphic.org/neuromorphic-computing/hardware/>Open Neuromorphic&rsquo;s Hardware Guide</a>.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Event cameras won&rsquo;t reach mainstream adoption until they break away from the legacy of microsecond-precision binary spikes and embrace output formats that carry richer, more structured information. The core challenge is representation: modern ML systems are built around structured tokens such as patches, words, or embeddings, not floods of binary impulses.</p><p>A workable solution will likely follow a two-stage architecture: a lightweight, streaming tokenizer that aggregates local spatiotemporal activity into short-lived micro-features, followed by a stateful temporal model that reasons over these features efficiently. Such representations preserve sparsity, maintain temporal fidelity, and scale naturally under variable scene activity.</p><p>If I had to bet on a commercial trajectory today, it would be hybrid sensors that pair variable-rate event frames with standard RGB output, producing a sparse token stream processed by compact state-space models on embedded GPUs or specialized edge accelerators. Sacrificing some raw temporal resolution in exchange for more semantically meaningful, compressible aggregates is a practical and likely necessary trade-off. The goal is not to create pretty images but to produce machine-readable signals that map cleanly onto existing AI hardware.</p><p>The scalable recipe looks something like this: generate tokens that carry meaning, train them with a mix of
cross-modal supervision and self-supervision that reflects real sensor noise, maintain a compact and
cheap-to-update scene memory, and make computation conditional on activity rather than a fixed clock. Key research directions include dynamic graph representations for efficient tokenization, state-space models for low-latency inference at the edge, and lossy compression techniques that can shrink event streams without destroying semantic content.</p><p>Finally, application needs should guide sensor and model design. Gesture recognition doesn&rsquo;t require microsecond timing. Eye tracking doesn&rsquo;t need megapixel resolution. And sometimes a motion sensor-triggered RGB camera is the most pragmatic solution. Event cameras don&rsquo;t need to replace conventional vision, they just need to become usable enough, in the right form, to complement it.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://lenzgregor.com/tags/event-cameras/>Event Cameras</a></li></ul><nav class=paginav><a class=next href=https://lenzgregor.com/posts/event-cameras-2025-part1/><span class=title>Next »</span><br><span>Event cameras in 2025, Part 1</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on x" href="https://x.com/intent/tweet/?text=Event%20cameras%20in%202025%2c%20Part%202&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f&amp;hashtags=Eventcameras"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f&amp;title=Event%20cameras%20in%202025%2c%20Part%202&amp;summary=Event%20cameras%20in%202025%2c%20Part%202&amp;source=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f&title=Event%20cameras%20in%202025%2c%20Part%202"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on whatsapp" href="https://api.whatsapp.com/send?text=Event%20cameras%20in%202025%2c%20Part%202%20-%20https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on telegram" href="https://telegram.me/share/url?text=Event%20cameras%20in%202025%2c%20Part%202&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Event%20cameras%20in%202025%2c%20Part%202&u=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://lenzgregor.com/>Gregor's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>