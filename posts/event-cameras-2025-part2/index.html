<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Event cameras in 2025, Part 2 | Gregor's Blog</title><meta name=keywords content="Event cameras"><meta name=description content="Technological challenges that are to be overcome before event cameras enter the mass market."><meta name=author content><link rel=canonical href=https://lenzgregor.com/posts/event-cameras-2025-part2/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://lenzgregor.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lenzgregor.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lenzgregor.com/favicon-32x32.png><link rel=apple-touch-icon href=https://lenzgregor.com/apple-touch-icon.png><link rel=mask-icon href=https://lenzgregor.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://lenzgregor.com/posts/event-cameras-2025-part2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.plot.ly/plotly-latest.min.js></script><meta property="og:url" content="https://lenzgregor.com/posts/event-cameras-2025-part2/"><meta property="og:site_name" content="Gregor's Blog"><meta property="og:title" content="Event cameras in 2025, Part 2"><meta property="og:description" content="Technological challenges that are to be overcome before event cameras enter the mass market."><meta property="og:locale" content="en-gb"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-20T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-17T10:28:10+01:00"><meta property="article:tag" content="Event Cameras"><meta name=twitter:card content="summary"><meta name=twitter:title content="Event cameras in 2025, Part 2"><meta name=twitter:description content="Technological challenges that are to be overcome before event cameras enter the mass market."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lenzgregor.com/posts/"},{"@type":"ListItem","position":2,"name":"Event cameras in 2025, Part 2","item":"https://lenzgregor.com/posts/event-cameras-2025-part2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Event cameras in 2025, Part 2","name":"Event cameras in 2025, Part 2","description":"Technological challenges that are to be overcome before event cameras enter the mass market.","keywords":["Event cameras"],"articleBody":"In Part 1 I provided a high level overview of different industry sectors that could potentially see the adoption of event cameras. Apart from the challenge of finding the right application, there are several technological challenges before event cameras can reach a mass audience.\nSensor Capabilities Today’s most recent event cameras are summarised in the table below.\nCamera Supplier Sensor Model Name Year Resolution Dynamic Range (dB) Max Bandwidth (Mev/s) iniVation Gen2 DVS DAVIS346 2017 346×260 ~120 12 iniVation Gen3 DVS DVXPlorer 2020 640×480 90-110 165 Prophesee Sony IMX636 EVK4 2020 1280×720 120 1066 Prophesee GenX320 EVK3 2023 320×320 140 Samsung Gen4 DVS DVS-Gen4 2020 1280×960 1200 Insightness was sold to Sony, and CelePixel partnered with Omnivision, but hasn’t released anything in the past 5 years. Over the past decade, we have seen resolution grow from 128x128 to HD, but that’s actually not always good. The last column in the table above describes the number of million events per second, which can easily be reached when the camera is moving fast, such on a drone. A paper by Gehrig and Scaramuzza suggests that in low light and high speed scenarios, performance of high res cameras is actually worse than when using fewer, but bigger pixels, due to high per-pixel event rates that are noisy and cause ghosting artifacts.\nIn areas such as defence, higher resolution and contrast sensitivity, as well as capturing the short/mid range infrared spectrum, is going to be desirable, because range is so important. SCD USA made the MIRA 02Y-E available last year that includes an optional event-based readout, to enable tactical forces to detect laser sources. Using the event-based output, it advertises a frame rate of up to 1.2 kHz. In space, the distances to the captured objects are enormous, and therefore high resolution and light sensitivity are of utmost importance.\nIn short range applications such as eye tracking for wearables, a GenX320 at lower resolution but high dynamic range and ultra low power modes is going to be more interesting. For scientific applications, NovoViz recently announced a new SPAD (single photon avalanche diode) camera using event-based outputs!\nOne thing is clear: today’s binary microsecond spikes are rarely the right format. Much like Intel’s Loihi 2 shifted from binary spikes to richer spike payloads because they realised that the communication overhead was too high otherwise, future event cameras could emit multi-bit “micro-frames” or tokenizable spike packets. These would represent short-term local activity and could be directly ingested by ML models, reducing the need for preprocessing altogether. Ideally there’s a trade-off between information density and temporal resolution that can be chosen depending on the application.\nA key trend are hybrid vision sensors that combine rgb and event frames. At ISSCC 2023, three papers showed new generations of hybrid vision sensors, which output both RGB frames at fixed rates and events in between.\nSensor Event output type Timing \u0026 synchronization Polarity info Typical max rate Sony 2.97 μm Binary event frames (two separate ON/OFF maps) Synchronous, ~580 µs “event frame” period 2 bits per pixel (positive \u0026 negative) ~1.4 GEvents/s OmniVision 3-wafer Per-event address-event packets (x, y, t, polarity) Asynchronous, microsecond-level timestamps Single-bit polarity per event Up to 4.6 GEvents/s Sony 1.22 μm, 35.6 MP Binary event frames with row-skipping \u0026 compression Variable frame sync, up to 10 kfps per RGB frame 2 bits per pixel (positive \u0026 negative) Up to 4.56 GEvents/s The Sony 2.97 μm chip uses aggressive circuit sharing so that four pixels share one comparator and analog front-end. Events are not streamed individually but are batched into binary event frames every ~580 µs, with separate maps for ON and OFF polarity. This design keeps per-event energy extremely low (~57 pJ) and allows the sensor to reach ~1.4 GEvents/s without arbitration delays. Because output is already frame-like, it fits naturally into existing machine learning pipelines that expect regular image-like input at deterministic timing. The OmniVision 3-wafer is different: a true asynchronous event stream is preserved. A dedicated 1MP event wafer with in-pixel time-to-digital converters stamps each event with microsecond accuracy. Skip-logic and four parallel readout channels give a 4.6 GEvents/s throughput. This is closer to the classic DVS concept, ideal for ultra-fast motion analysis or scientific experiments where every microsecond matters. The integrated image signal processor can fuse the dense 15MP RGB video with the sparse event stream in hardware for applications such as 10 kfps slow-motion videos. The Sony 1.22 μm hybrid sensor aimed at mobile devices combines a huge 35.6 MP RGB array with a 2 MP event array. Four 1.22 µm photodiodes form each event pixel (4.88 µm pitch). The event side operates in variable-rate event-frame mode, outputting up to 10 kfps inside each RGB frame period. On-chip event-drop filters and compression dynamically reduce data volume while preserving critical motion information for downstream neural networks (e.g. deblurring or video frame interpolation). It is a practical demonstration that event frames and RGB can be tightly synchronized so that a phone SoC can consume both without exotic drivers.\nKodama et al. presented a sensor that outputs variable-rate binary event frames next to RGB.\nGuo et al. presented a new generation of hybrid vision sensor that outputs binary events.\nI find the trend towards event frames interested an in line with what most researchers have been feeding their machine learning models anyway. In either case, the event camera sensor has not reached its final form yet. The question is always in what way events should be represented in order to be compatible with modern machine learning methods.\nEvent Representations Most common approaches aggregate events into image-like representations such as 2d histograms, voxel grids, or time surfaces. These are then used to fine-tune deep learning models that were pre-trained on RGB images. This leverages the breadth of existing tooling built for images and is compatible with GPU-accelerated training and inference. Moreover, it allows for adaptive frame rates, aggregating only when there’s activity and potentially saving on compute. However, this method discards much of the fine temporal structure that makes event cameras valuable in the first place. We still lack a representation for event streams that works well with modern ML architectures and preserves their sparsity. Event streams are a new data modality, just like images, audio, or text, but one for which we haven’t yet cracked the “tokenization problem.” A single ON or OFF event contains very little semantic information. Unlike a word in a sentence, which can encode a concept, even a dozen events reveal almost nothing about the scene. This makes direct tokenization of events inefficient and ineffective. What we need is a representation that can summarize local spatiotemporal structure into meaningful, higher-level primitives. Something akin to a “visual word” for events.\nIt’s also inherently inefficient: the tensors produced are full of zeros, and latency grows with the size of the memory window. This becomes problematic for real-time applications where a long temporal context is needed but high responsiveness is crucial.\nI think that graphs, especially dynamic, sparse graphs, are an interesting abstraction to be explored. Each node could represent a small region of correlated activity in space and time, with edges encoding temporal or spatial relationships. Recent work such as HugNet v2, DAGr, or EvGNN hardware apply Graph Neural Networks (GNNs) to event data. But several challenges remain: to generate such a graph, we need a lot of memory for all those events, and the upredictable number of incoming events makes computation extremely inefficient. This is where specialized hardware accelerators will need to come in, because dynamically fetching events is expensive. By combining event cameras with efficient “graph processors,” we could offload the task of building sparse graphs directly on-chip, producing representations that are ready for downstream learning. Temporally sparse, graph-based outputs could serve as a robust bridge between raw events and modern ML architectures.\nIf you want to preserve sparsity, you need tokens that mean something. Individual ON/OFF events are too atomic to be useful tokens, so a practical middle ground is a two‑stage model: a lightweight, streaming “tokenizer” that clusters local spatiotemporal activity into short‑lived micro‑features, followed by a stateful temporal model that reasons over those features. The tokenizer can be as simple as centroiding event bursts in a small spatial neighborhood with a short time constant, or as involved as a dynamic graph builder that fuses polarity, age, and motion cues. Either way, the goal is to transform a flood of spikes into a bounded, variable‑rate set of tokens with stable meaning. Next let’s explore the type of models that work well with event camera data.\nMachine Learning Models At their core, event cameras are change detectors, which means that we need memory in our machine learning models to remember where things were before they stopped moving. We can bake memory into the model architecture by using recurrence or attention. For example, Recurrent Vision Transformers and their variants maintain internal state across time and can handle temporally sparse inputs more naturally. These methods preserve temporal continuity, but there’s a catch: most of these methods still rely on dense, voxelized inputs. Even with more efficient state-space models replacing LSTMs and BPTT (Backpropagation Through Time), we’re still processing a lot of zeros. Training is faster, but inference is still bottlenecked by inefficient representations.\nNowadays larger AI models are being pruned, distilled, and quantised to provide efficient edge models that can generalise well. Even TinyML models are students of a larger model. We have to say goodbye to the idea of training tiny models from scratch for commercial event camera applications, because they won’t perform well enough in the real world.\nSpiking neural networks (SNNs) are sometimes touted as a natural fit for event data. But in their traditional form, with binary activations and reset mechanisms, leaky integrate-and-fire (LIF) neurons are handcrafted biological abstractions. If we learned anything from machine learning, it’s that handcrafted designs are inherently flawed. And neurons are an incredibly complex thing to model, as efforts such as CZI’s Virtual Cells and DeepMind’s cell simulations show. So let’s not get hung up on the artificial neuron model itself, and instead use what works well, because the field is moving incredibly fast.\nI’m very optimistic about state space models (SSMs) for event vision. Instead of baking memory into heavy recurrence or dense attention, an SSM treats the scene’s latent dynamics as a continuous-time system and then discretizes only for inference. This means a single trained model can adapt to many operating modes: you can run it at different inference rates or even update state event-by-event with variable time steps—without retraining—simply by changing the integration step. That flexibility is a good match for sensors whose activity is unpredictable.\nProcessors Meyer et al. implemented a S4D SSM on Intel’s Loihi 2, constraining the state space to be diagonal so that each neuron evolves independently. They mapped these one-dimensional state updates directly to Loihi’s programmable neurons and carefully placed layers to reduce inter-core communication, which resulted in much lower latency and energy use than a Jetson GPU in true online processing. I think it’s a compelling demonstration that SSMs can be run efficiently on stateful AI accelerator hardware and I’m curious what else is coming out of that.\nSome people argue that because event cameras output extremely sparse data, we can save energy by skipping zeros in the input or in intermediate activations. But I don’t buy that argument because while the input might be much sparser than an RGB frame, the bulk of the computation actually happens in intermediate layers and works with higher level representations, which are hopefully similar for both RGB and event inputs. That means that in AI accelerators we can’t exploit spatial event camera sparsity, and inference cost between RGB and event frames are essentially the same. Of course we might get different input frame rates / temporal sparsity, but those can be exploited on GPUs as well.\nKeep in mind that on mixed-signal hardware, rules are different. There’s a breadth of new materials being explored, memristors and spintronics. The basic rule for analog is: if you need to convert from analog to digital too often, for error correction or because you’re storing states or other intermediate values, your efficiency gains go out of the window. Mythic AI had to painfully learn that and almost tanked, and also Rain AI pivoted from its original analog hardware and faces an uncertain future. The brain uses a mixture of analog (graded potentials, dendritic integration) and digital (spikes) signals and we can replicate this principle in silicon. But since the circuitry is the memory at the same time, it needs an incredible amount of space, and is organised in 3d. That’s really costly to do in silicon, and the major challenge is getting the heat out, which is much easier in 2d.\nI think that the asynchronous compute principle is key for event cameras, but we need to realise that naïve asynchrony is not constructive. Think about a roundabout, and how it manages the flow of traffic without any traffic lights. When the traffic volume is low, every car is more or less in constant motion, and latency to cross the roundabout is minimal. As the volume of traffic grows, a roundabout becomes inefficient, because the movement of any car depends on the decisions of cars nearby. For high traffic flow, it becomes more efficient to use traffic lights to batch process the traffic for multiple lanes at once, which achieves the highest throughput of cars. The same principle applies for events. When you have few pixels activated, you achieve the lowest latency when you process them as they come in, as in a roundabout. But as the amount of events / s gets larger, for example because you’re moving the camera on a car or a drone, you need to get out the traffic lights, and start and stop larger batches of events. Ideally the size of the batch depends on the event rate.\nFor more info about neuromorphic chips, I refer you to Open Neuromorphic’s Hardware Guide.\nConclusion Here are my main points:\nEvent cameras won’t go mainstream until they move away from binary events and to richer output formats, whether from the sensor directly or an attached preprocessor. Event cameras follow the trajectory of other sensors that were developed and improved within the context of defence applications. We need an efficient representation that is compatible with modern ML architectures. It might well be event frames in the end. Keep it practical. Biologically-inspired approaches should not distract from deployment-grade ML solutions. The recipe that scales is: build a token stream that carries meaning, train it with cross‑modal supervision and self‑supervision that reflects real sensor noise, keep a compact scene memory that is cheap to update, and make computation conditional on activity rather than on a fixed clock.\nBinary events don’t contain enough information on their own, so they must be aggregated in one form or another. Event sensors might move from binary outputs toward richer encodings at the pixel level, attach a dedicated processor to output richer representations, or they simply output what the world already knows well: another form of frames. While many researchers (including me) originally set out to work with binary events directly, I think it is time to swallow a bitter pill and accept that computer vision will depend on frames for the foreseeable future.\nMy bet is currently on the latter, because the simplest solutions tend to win.\nDeep learning started out with 32 bit floating point, dense representations, and neuromorphic started out on the other end of the spectrum at binary, extremely sparse representations. They are converging, with neuromorphic realising that binary events are expensive to transmit, and deep learning embracing 4 bit activations and 2:4 sparsity.\nInteresting research directions for event cameras today are about dynamic graph representations for efficient tokenization, state space models for efficient inference, lossy compression for smaller file sizes. To unlock the full potential of event cameras, we need to solve the representation problem to make it compatible with modern deep learning hardware and software, while preserving the extreme sparsity of the data. Also we shouldn’t be too focused on biologically-inspired processing if we want this thing to scale anytime soon. I think that either the sensors must evolve to emit richer, token-friendly outputs, or they must be paired with dedicated pre-processors that produce high-level, potentially graph-based abstractions. Once that happens, event cameras become easy enough to work with to reach the mainstream.\nUltimately, the application dictates the design. Gesture recognition does not need microsecond temporal resolution. Eye tracking doesn’t need HD spatial resolution. And sometimes a motion sensor that will wake a standard camera will be the easiest solution.\n","wordCount":"2781","inLanguage":"en","datePublished":"2025-08-20T00:00:00Z","dateModified":"2025-09-17T10:28:10+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://lenzgregor.com/posts/event-cameras-2025-part2/"},"publisher":{"@type":"Organization","name":"Gregor's Blog","logo":{"@type":"ImageObject","url":"https://lenzgregor.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lenzgregor.com/ accesskey=h title="Gregor's Blog (Alt + H)">Gregor's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lenzgregor.com/ title=Home><span>Home</span></a></li><li><a href=https://lenzgregor.com/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://lenzgregor.com/>Home</a>&nbsp;»&nbsp;<a href=https://lenzgregor.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Event cameras in 2025, Part 2</h1><div class=post-meta><span title='2025-08-20 00:00:00 +0000 UTC'>August 20, 2025</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;2781 words</div></header><div class=post-content><p>In <a href=https://lenzgregor.com/posts/event-cameras-2025-part1/>Part 1</a> I provided a high level overview of different industry sectors that could potentially see the adoption of event cameras. Apart from the challenge of finding the right application, there are several technological challenges before event cameras can reach a mass audience.</p><h2 id=sensor-capabilities>Sensor Capabilities<a hidden class=anchor aria-hidden=true href=#sensor-capabilities>#</a></h2><p>Today&rsquo;s most recent event cameras are summarised in the table below.</p><table><thead><tr><th>Camera Supplier</th><th>Sensor</th><th>Model Name</th><th>Year</th><th>Resolution</th><th>Dynamic Range (dB)</th><th>Max Bandwidth (Mev/s)</th></tr></thead><tbody><tr><td>iniVation</td><td>Gen2 DVS</td><td><a href=https://docs.inivation.com/hardware/current-products/davis346.html>DAVIS346</a></td><td>2017</td><td>346×260</td><td>~120</td><td>12</td></tr><tr><td>iniVation</td><td>Gen3 DVS</td><td><a href=https://docs.inivation.com/hardware/current-products/dvxplorer.html>DVXPlorer</a></td><td>2020</td><td>640×480</td><td>90-110</td><td>165</td></tr><tr><td>Prophesee</td><td><a href=https://www.prophesee.ai/event-based-sensor-imx636-sony-prophesee/>Sony IMX636</a></td><td><a href=https://www.prophesee.ai/event-camera-evk4/>EVK4</a></td><td>2020</td><td>1280×720</td><td>120</td><td>1066</td></tr><tr><td>Prophesee</td><td><a href=https://www.prophesee.ai/event-based-sensor-genx320/>GenX320</a></td><td><a href=https://www.prophesee.ai/evk-3-genx320-info/>EVK3</a></td><td>2023</td><td>320×320</td><td>140</td><td></td></tr><tr><td>Samsung</td><td>Gen4 DVS</td><td>DVS-Gen4</td><td>2020</td><td>1280×960</td><td></td><td>1200</td></tr></tbody></table><p>Insightness was sold to Sony, and CelePixel partnered with Omnivision, but hasn&rsquo;t released anything in the past 5 years. Over the past decade, we have seen resolution grow from 128x128 to HD, but that&rsquo;s actually not always good. The last column in the table above describes the number of million events per second, which can easily be reached when the camera is moving fast, such on a drone. A paper by <a href=https://arxiv.org/abs/2203.14672>Gehrig and Scaramuzza</a> suggests that in low light and high speed scenarios, performance of high res cameras is actually worse than when using fewer, but bigger pixels, due to high per-pixel event rates that are noisy and cause ghosting artifacts.</p><p>In areas such as defence, higher resolution and contrast sensitivity, as well as capturing the short/mid range infrared spectrum, is going to be desirable, because range is so important. SCD USA made the <a href=https://scdusa-ir.com/wp-content/uploads/2024/06/Mira_V1g.pdf>MIRA 02Y-E</a> available last year that includes an optional event-based readout, to enable tactical forces to detect laser sources. Using the event-based output, it advertises a frame rate of up to 1.2 kHz. In space, the distances to the captured objects are enormous, and therefore high resolution and light sensitivity are of utmost importance.</p><p>In short range applications such as eye tracking for wearables, a <a href=https://www.prophesee.ai/event-based-sensor-genx320/>GenX320</a> at lower resolution but high dynamic range and ultra low power modes is going to be more interesting.
For scientific applications, NovoViz recently <a href=https://www.tokyoupdates.metro.tokyo.lg.jp/en/post-1551/>announced</a> a new SPAD (single photon avalanche diode) camera using event-based outputs!</p><p>One thing is clear: today’s binary microsecond spikes are rarely the right format. Much like Intel’s <a href=https://open-neuromorphic.org/neuromorphic-computing/hardware/loihi-2-intel/>Loihi 2</a> shifted from binary spikes to richer spike payloads because they realised that the communication overhead was too high otherwise, future event cameras could emit multi-bit “micro-frames” or tokenizable spike packets. These would represent short-term local activity and could be directly ingested by ML models, reducing the need for preprocessing altogether. Ideally there’s a trade-off between information density and temporal resolution that can be chosen depending on the application.</p><p>A key trend are hybrid vision sensors that combine rgb and event frames. At ISSCC 2023, three papers showed new generations of hybrid vision sensors, which output both RGB frames at fixed rates and events in between.</p><table><thead><tr><th>Sensor</th><th>Event output type</th><th>Timing & synchronization</th><th>Polarity info</th><th>Typical max rate</th></tr></thead><tbody><tr><td><a href=https://ieeexplore.ieee.org/document/10067566>Sony 2.97 μm</a></td><td>Binary event frames (two separate ON/OFF maps)</td><td>Synchronous, ~580 µs “event frame” period</td><td>2 bits per pixel (positive & negative)</td><td>~1.4 GEvents/s</td></tr><tr><td><a href=https://ieeexplore.ieee.org/document/10067476>OmniVision 3-wafer</a></td><td>Per-event address-event packets (x, y, t, polarity)</td><td>Asynchronous, microsecond-level timestamps</td><td>Single-bit polarity per event</td><td>Up to 4.6 GEvents/s</td></tr><tr><td><a href=https://ieeexplore.ieee.org/document/10067520>Sony 1.22 μm, 35.6 MP</a></td><td>Binary event frames with row-skipping & compression</td><td>Variable frame sync, up to 10 kfps per RGB frame</td><td>2 bits per pixel (positive & negative)</td><td>Up to 4.56 GEvents/s</td></tr></tbody></table><p>The Sony 2.97 μm chip uses aggressive circuit sharing so that four pixels share one comparator and analog front-end. Events are not streamed individually but are batched into binary event frames every ~580 µs, with separate maps for ON and OFF polarity. This design keeps per-event energy extremely low (~57 pJ) and allows the sensor to reach ~1.4 GEvents/s without arbitration delays. Because output is already frame-like, it fits naturally into existing machine learning pipelines that expect regular image-like input at deterministic timing.
The OmniVision 3-wafer is different: a true asynchronous event stream is preserved. A dedicated 1MP event wafer with in-pixel time-to-digital converters stamps each event with microsecond accuracy. Skip-logic and four parallel readout channels give a 4.6 GEvents/s throughput. This is closer to the classic DVS concept, ideal for ultra-fast motion analysis or scientific experiments where every microsecond matters. The integrated image signal processor can fuse the dense 15MP RGB video with the sparse event stream in hardware for applications such as 10 kfps slow-motion videos.
The Sony 1.22 μm hybrid sensor aimed at mobile devices combines a huge 35.6 MP RGB array with a 2 MP event array. Four 1.22 µm photodiodes form each event pixel (4.88 µm pitch). The event side operates in variable-rate event-frame mode, outputting up to 10 kfps inside each RGB frame period. On-chip event-drop filters and compression dynamically reduce data volume while preserving critical motion information for downstream neural networks (e.g. deblurring or video frame interpolation). It is a practical demonstration that event frames and RGB can be tightly synchronized so that a phone SoC can consume both without exotic drivers.</p><p><img alt=hybrid-vision-sensor-sony loading=lazy src=/posts/event-cameras-2025-part2/images/hvs-sony.png>
<em><a href=https://ieeexplore.ieee.org/document/10067520>Kodama et al.</a> presented a sensor that outputs variable-rate binary event frames next to RGB.</em></p><p><img alt=hybrid-vision-sensor-sony loading=lazy src=/posts/event-cameras-2025-part2/images/hvs-omnivision.png>
<em><a href=https://ieeexplore.ieee.org/document/10067476>Guo et al.</a> presented a new generation of hybrid vision sensor that outputs binary events.</em></p><p>I find the trend towards event frames interested an in line with what most researchers have been feeding their machine learning models anyway. In either case, the event camera sensor has not reached its final form yet. The question is always in what way events should be represented in order to be compatible with modern machine learning methods.</p><h2 id=event-representations>Event Representations<a hidden class=anchor aria-hidden=true href=#event-representations>#</a></h2><p>Most common approaches aggregate events into <a href=https://tonic.readthedocs.io/en/latest/auto_examples/index.html#event-representations>image-like representations</a> such as 2d histograms, voxel grids, or time surfaces. These are then used to fine-tune deep learning models that were pre-trained on RGB images. This leverages the breadth of existing tooling built for images and is compatible with GPU-accelerated training and inference. Moreover, it allows for adaptive frame rates, aggregating only when there’s activity and potentially saving on compute. However, this method discards much of the fine temporal structure that makes event cameras valuable in the first place.
We still lack a representation for event streams that works well with modern ML architectures and preserves their sparsity. Event streams are a new data modality, just like images, audio, or text, but one for which we haven’t yet cracked the “tokenization problem.” A single ON or OFF event contains very little semantic information. Unlike a word in a sentence, which can encode a concept, even a dozen events reveal almost nothing about the scene. This makes direct tokenization of events inefficient and ineffective. What we need is a representation that can summarize local spatiotemporal structure into meaningful, higher-level primitives. Something akin to a “visual word” for events.</p><p>It’s also inherently inefficient: the tensors produced are full of zeros, and latency grows with the size of the memory window. This becomes problematic for real-time applications where a long temporal context is needed but high responsiveness is crucial.</p><p>I think that graphs, especially dynamic, sparse graphs, are an interesting abstraction to be explored. Each node could represent a small region of correlated activity in space and time, with edges encoding temporal or spatial relationships. Recent work such as <a href=https://openaccess.thecvf.com/content/CVPR2025/html/Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for_CVPR_2025_paper.html>HugNet v2</a>, <a href=https://www.nature.com/articles/s41586-024-07409-w>DAGr</a>, or <a href=https://ieeexplore.ieee.org/abstract/document/10812004>EvGNN hardware</a> apply Graph Neural Networks (GNNs) to event data. But several challenges remain: to generate such a graph, we need a lot of memory for all those events, and the upredictable number of incoming events makes computation extremely inefficient. This is where specialized hardware accelerators will need to come in, because dynamically fetching events is expensive. By combining event cameras with efficient “graph processors,” we could offload the task of building sparse graphs directly on-chip, producing representations that are ready for downstream learning. Temporally sparse, graph-based outputs could serve as a robust bridge between raw events and modern ML architectures.</p><p>If you want to preserve sparsity, you need tokens that mean something. Individual ON/OFF events are too atomic to be useful tokens, so a practical middle ground is a two‑stage model: a lightweight, streaming “tokenizer” that clusters local spatiotemporal activity into short‑lived micro‑features, followed by a stateful temporal model that reasons over those features. The tokenizer can be as simple as centroiding event bursts in a small spatial neighborhood with a short time constant, or as involved as a dynamic graph builder that fuses polarity, age, and motion cues. Either way, the goal is to transform a flood of spikes into a bounded, variable‑rate set of tokens with stable meaning. Next let&rsquo;s explore the type of models that work well with event camera data.</p><h2 id=machine-learning-models>Machine Learning Models<a hidden class=anchor aria-hidden=true href=#machine-learning-models>#</a></h2><p>At their core, event cameras are change detectors, which means that we need memory in our machine learning models to remember where things were before they stopped moving.
We can bake memory into the model architecture by using recurrence or attention. For example, <a href=https://openaccess.thecvf.com/content/CVPR2023/html/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.html>Recurrent Vision Transformers</a> and their variants maintain internal state across time and can handle temporally sparse inputs more naturally. These methods preserve temporal continuity, but there’s a catch: most of these methods still rely on dense, voxelized inputs. Even with more efficient <a href=https://openaccess.thecvf.com/content/CVPR2024/html/Zubic_State_Space_Models_for_Event_Cameras_CVPR_2024_paper.html>state-space models</a> replacing LSTMs and BPTT (Backpropagation Through Time), we’re still processing a lot of zeros. Training is faster, but inference is still bottlenecked by inefficient representations.</p><p>Nowadays larger AI models are being pruned, distilled, and quantised to provide efficient edge models that can generalise well. Even TinyML models are <a href=https://www.nature.com/articles/s41598-025-94205-9.pdf>students</a> of a larger model. We have to say goodbye to the idea of training tiny models from scratch for commercial event camera applications, because they won&rsquo;t perform well enough in the real world.</p><p>Spiking neural networks (SNNs) are sometimes touted as a natural fit for event data. But in their traditional form, with binary activations and reset mechanisms, leaky integrate-and-fire (LIF) neurons are handcrafted biological abstractions. If we learned anything from machine learning, it&rsquo;s that handcrafted designs are inherently flawed. And neurons are an incredibly complex thing to model, as efforts such as <a href=https://chanzuckerberg.com/science/technology/virtual-cells/>CZI’s Virtual Cells</a> and <a href=https://analyticsindiamag.com/ai-features/inside-google-deepminds-bold-vision-for-virtual-cell/>DeepMind’s cell simulations</a> show. So let&rsquo;s not get hung up on the artificial neuron model itself, and instead use what works well, because the field is moving incredibly fast.</p><p>I’m very optimistic about state space models (SSMs) for event vision. Instead of baking memory into heavy recurrence or dense attention, an SSM treats the scene’s latent dynamics as a continuous-time system and then discretizes only for inference. This means a single trained model can adapt to many operating modes: you can run it at different inference rates or even update state event-by-event with variable time steps—without retraining—simply by changing the integration step. That flexibility is a good match for sensors whose activity is unpredictable.</p><h2 id=processors>Processors<a hidden class=anchor aria-hidden=true href=#processors>#</a></h2><p>Meyer et al. implemented a S4D SSM on Intel’s Loihi 2, constraining the state space to be diagonal so that each neuron evolves independently.
They mapped these one-dimensional state updates directly to Loihi’s programmable neurons and carefully placed layers to reduce inter-core communication, which resulted in much lower latency and energy use than a Jetson GPU in true online processing.
I think it’s a compelling demonstration that SSMs can be run efficiently on stateful AI accelerator hardware and I&rsquo;m curious what else is coming out of that.</p><p>Some people argue that because event cameras output extremely sparse data, we can save energy by skipping zeros in the input or in intermediate activations. But I don&rsquo;t buy that argument because while the input might be much sparser than an RGB frame, the bulk of the computation actually happens in intermediate layers and works with higher level representations, which are hopefully similar for both RGB and event inputs. That means that in AI accelerators we can&rsquo;t exploit spatial event camera sparsity, and inference cost between RGB and event frames are essentially the same. Of course we might get different input frame rates / temporal sparsity, but those can be exploited on GPUs as well.</p><p>Keep in mind that on mixed-signal hardware, rules are different. There&rsquo;s a breadth of new materials being explored, memristors and spintronics. The basic rule for analog is: if you need to convert from analog to digital too often, for error correction or because you&rsquo;re storing states or other intermediate values, your efficiency gains go out of the window. <a href=https://mythic.ai/>Mythic AI</a> had to painfully learn that and <a href=https://www.reddit.com/r/technology/comments/yvjgwu/analog_ai_chip_startup_mythic_runs_out_of_money/>almost tanked</a>, and also <a href=https://rain.ai/>Rain AI</a> pivoted from its original analog hardware and faces <a href=https://startupwired.com/2025/05/16/rain-ai-the-rise-and-fall-of-a-chipmaking-challenger/>an uncertain future</a>. The brain uses a mixture of analog (graded potentials, dendritic integration) and digital (spikes) signals and we can replicate this principle in silicon. But since the circuitry is the memory at the same time, it needs an incredible amount of space, and is organised in 3d. That&rsquo;s really costly to do in silicon, and the major challenge is getting the heat out, which is much easier in 2d.</p><p>I think that the asynchronous compute principle is key for event cameras, but we need to realise that naïve asynchrony is not constructive. Think about a roundabout, and how it manages the flow of traffic without any traffic lights. When the traffic volume is low, every car is more or less in constant motion, and latency to cross the roundabout is minimal. As the volume of traffic grows, a roundabout becomes inefficient, because the movement of any car depends on the decisions of cars nearby. For high traffic flow, it becomes more efficient to use traffic lights to <code>batch process</code> the traffic for multiple lanes at once, which achieves the highest throughput of cars.
The same principle applies for events. When you have few pixels activated, you achieve the lowest latency when you process them as they come in, as in a roundabout. But as the amount of events / s gets larger, for example because you&rsquo;re moving the camera on a car or a drone, you need to get out the traffic lights, and start and stop larger batches of events. Ideally the size of the batch depends on the event rate.</p><p>For more info about neuromorphic chips, I refer you to <a href=https://open-neuromorphic.org/neuromorphic-computing/hardware/>Open Neuromorphic&rsquo;s Hardware Guide</a>.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Here are my main points:</p><ul><li>Event cameras won’t go mainstream until they move away from binary events and to richer output formats, whether from the sensor directly or an attached preprocessor.</li><li>Event cameras follow the trajectory of other sensors that were developed and improved within the context of defence applications.</li><li>We need an efficient representation that is compatible with modern ML architectures. It might well be event frames in the end.</li><li>Keep it practical. Biologically-inspired approaches should not distract from deployment-grade ML solutions.</li></ul><p>The recipe that scales is: build a token stream that carries meaning, train it with cross‑modal supervision and self‑supervision that reflects real sensor noise, keep a compact scene memory that is cheap to update, and make computation conditional on activity rather than on a fixed clock.</p><p>Binary events don&rsquo;t contain enough information on their own, so they must be aggregated in one form or another. Event sensors might move from binary outputs toward richer encodings at the pixel level, attach a dedicated processor to output richer representations, or they simply output what the world already knows well: another form of frames. While many researchers (including me) originally set out to work with binary events directly, I think it is time to swallow a bitter pill and accept that computer vision will depend on frames for the foreseeable future.<br>My bet is currently on the latter, because the simplest solutions tend to win.</p><p>Deep learning started out with 32 bit floating point, dense representations, and neuromorphic started out on the other end of the spectrum at binary, extremely sparse representations. They are converging, with neuromorphic realising that binary events are expensive to transmit, and deep learning embracing 4 bit activations and 2:4 sparsity.</p><p>Interesting research directions for event cameras today are about dynamic graph representations for efficient tokenization, state space models for efficient inference, lossy compression for smaller file sizes. To unlock the full potential of event cameras, we need to solve the representation problem to make it compatible with modern deep learning hardware and software, while preserving the extreme sparsity of the data.
Also we shouldn’t be too focused on biologically-inspired processing if we want this thing to scale anytime soon. I think that either the sensors must evolve to emit richer, token-friendly outputs, or they must be paired with dedicated pre-processors that produce high-level, potentially graph-based abstractions. Once that happens, event cameras become easy enough to work with to reach the mainstream.</p><p>Ultimately, the application dictates the design. Gesture recognition does not need microsecond temporal resolution. Eye tracking doesn&rsquo;t need HD spatial resolution. And sometimes a motion sensor that will wake a standard camera will be the easiest solution.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://lenzgregor.com/tags/event-cameras/>Event Cameras</a></li></ul><nav class=paginav><a class=next href=https://lenzgregor.com/posts/event-cameras-2025-part1/><span class=title>Next »</span><br><span>Event cameras in 2025, Part 1</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on x" href="https://x.com/intent/tweet/?text=Event%20cameras%20in%202025%2c%20Part%202&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f&amp;hashtags=Eventcameras"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f&amp;title=Event%20cameras%20in%202025%2c%20Part%202&amp;summary=Event%20cameras%20in%202025%2c%20Part%202&amp;source=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f&title=Event%20cameras%20in%202025%2c%20Part%202"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on whatsapp" href="https://api.whatsapp.com/send?text=Event%20cameras%20in%202025%2c%20Part%202%20-%20https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on telegram" href="https://telegram.me/share/url?text=Event%20cameras%20in%202025%2c%20Part%202&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Event cameras in 2025, Part 2 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Event%20cameras%20in%202025%2c%20Part%202&u=https%3a%2f%2flenzgregor.com%2fposts%2fevent-cameras-2025-part2%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://lenzgregor.com/>Gregor's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>