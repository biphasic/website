<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Training spiking neural networks, fast. | Gregor's Blog</title><meta name=keywords content="SNN"><meta name=description content="How to use caching and EXODUS to speed up training by a factor of more than 10."><meta name=author content><link rel=canonical href=https://lenzgregor.com/posts/train-snns-fast/><link crossorigin=anonymous href=/assets/css/stylesheet.2211ca3164be7830024f6aad2b3a2e520843a64f8f048445c3401c1249aa051d.css integrity="sha256-IhHKMWS+eDACT2qtKzouUghDpk+PBIRFw0AcEkmqBR0=" rel="preload stylesheet" as=style><link rel=icon href=https://lenzgregor.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://lenzgregor.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://lenzgregor.com/favicon-32x32.png><link rel=apple-touch-icon href=https://lenzgregor.com/apple-touch-icon.png><link rel=mask-icon href=https://lenzgregor.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://lenzgregor.com/posts/train-snns-fast/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=https://cdn.plot.ly/plotly-latest.min.js></script><meta property="og:url" content="https://lenzgregor.com/posts/train-snns-fast/"><meta property="og:site_name" content="Gregor's Blog"><meta property="og:title" content="Training spiking neural networks, fast."><meta property="og:description" content="How to use caching and EXODUS to speed up training by a factor of more than 10."><meta property="og:locale" content="en-gb"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-14T06:31:35+00:00"><meta property="article:tag" content="SNN"><meta property="og:image" content="https://lenzgregor.com/posts/train-snns-fast/featured.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://lenzgregor.com/posts/train-snns-fast/featured.png"><meta name=twitter:title content="Training spiking neural networks, fast."><meta name=twitter:description content="How to use caching and EXODUS to speed up training by a factor of more than 10."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://lenzgregor.com/posts/"},{"@type":"ListItem","position":2,"name":"Training spiking neural networks, fast.","item":"https://lenzgregor.com/posts/train-snns-fast/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Training spiking neural networks, fast.","name":"Training spiking neural networks, fast.","description":"How to use caching and EXODUS to speed up training by a factor of more than 10.","keywords":["SNN"],"articleBody":"When training a spiking neural network (SNN), one might think about how the learning rate or model size affect training time. But when it comes to training faster, optimizing data movement is crucial. 3 out of the first 4 points in this list weighted after potential speed-up have to do with how data is shaped and moved around between actual computations. It makes a huge difference, because training faster means getting results faster!\nFor this post we train an SNN on the Heidelberg Spiking Speech Commands dataset to do audio stream classification. We’ll benchmark different data loading strategies using Tonic and show that with the right strategy, we can achieve a more than 10-fold speed-up compared to the naïve approach.\nFor all our benchmarks, we already assume multiple worker threads and pinning the host memory. We’ll increase throughput by using different forms of caching to disk or GPU. By applying deterministic transformations upfront and saving the new tensor, we can save a lot of time during training. This tutorial is run on a machine with Ubuntu 20.04, an Intel Core i7-8700K CPU @ 3.70GHz, a Samsung SSD 850 and an NVIDIA GeForce RTX 3080 GPU.\nAll data from neuromorphic datasets in Tonic is provided as NxD numpy arrays. We’ll need to transform this into a dense tensor to serve it to the GPU, and we’ll also do some downsampling of time steps. Let’s first define the transform. We know that samples of audio input data in this dataset are 0.8-1.2s long across 700 frequency channels at microsecond resolution. We’ll downsample each sample to 100 channels, bin events every 4 ms to one frame and cut samples that are longer than 1s. That leaves us with a maximum of 250 time steps per sample.\nfrom tonic import transforms dt = 4000 # all time units in Tonic in us encoding_dim = 100 dense_transform = transforms.Compose( [ transforms.Downsample(spatial_factor=encoding_dim / 700), transforms.CropTime(max=1e6), transforms.ToFrame( sensor_size=(encoding_dim, 1, 1), time_window=dt, include_incomplete=True ), ] ) Next we load the training dataset and assign the transform.\nfrom tonic import datasets dense_dataset = datasets.SSC(\"./data\", split=\"train\", transform=dense_transform) Let’s plot one such dense tensor sample:\nNext we define a spiking model. We use a simple integrate-and-fire (IAF) feed-forward architecture. For each dataloading method, we’re going to test two different models. One is a Sinabs model which is pretty much pure PyTorch plus for loops and the second one is an EXODUS model, which is also based on PyTorch but vectorizes gradient computation for the time dimension using custom CUDA code. Both models compute the same activations and gradients, but the latter provides a significant speedup.\nimport torch.nn as nn import sinabs.layers as sl import sinabs.exodus.layers as el class SNN(nn.Sequential): def __init__(self, backend, hidden_dim: int = 128): assert backend == sl or backend == el super().__init__( nn.Linear(encoding_dim, hidden_dim), backend.IAF(), nn.Linear(hidden_dim, hidden_dim), backend.IAF(), nn.Linear(hidden_dim, hidden_dim), backend.IAF(), nn.Linear(hidden_dim, 35), ) sinabs_model = SNN(backend=sl).cuda() exodus_model = SNN(backend=el).cuda() 1. Naïve dataloading For the first benchmark we test the most common setup without any caching. We load every sample from an hdf5 file on disk which provides us with a numpy array in memory. For each sample, we apply our dense_transform defined earlier to create a dense tensor which we can then batch together with other samples and feed it to the network.\nFigure 1: For every sample, we apply our transform ToFrame. The speed depends a lot on the CPU and the amount of worker threads used. import sinabs import timeit import tonic import pandas as pd from torch.utils.data import DataLoader dataloader_kwargs = dict( batch_size=128, shuffle=True, drop_last=True, pin_memory=True, collate_fn=tonic.collation.PadTensors(batch_first=True), num_workers=4, ) naive_dataloader = DataLoader(dense_dataset, **dataloader_kwargs) def training_loop(dataloader, model): for data, targets in iter(dataloader): data, targets = data.squeeze().cuda(), targets.cuda() sinabs.reset_states(model) output = model(data) loss = nn.functional.cross_entropy(output.sum(1), targets) loss.backward() from timeit import timeit time1 = timeit(lambda: training_loop(naive_dataloader, sinabs_model), number=1) time2 = timeit(lambda: training_loop(naive_dataloader, exodus_model), number=1) The Sinabs model takes more than two minutes per epoch using the simple strategy, which is far from exciting. By contrast, we can already see the huge speedup that EXODUS provides, reducing epoch time by a third! These results are our baseline with the basic dataloading.\nDisk caching Let’s try to be a bit smarter now. ToFrame is a deterministic transform, so for the same sample we’ll always receive the same transformed data. Given that we might train for 100 epochs, which looks at each sample 100 times, that’s a lot of wasted compute! Now we’re going to cache, which means save, those transformed samples to disk during the first epoch, so that we don’t need to recompute them later on! To do this we simply wrap our previous dataset in a DiskCachedDataset and provide the cache path. When a new sample is about to be loaded, that class will first check if the transformed sample is already in the cache on disk and if it isn’t, it will retrieve the original sample, apply the transform, cache it to disk and then serve it. This caching process slows down training in the first epoch, but it pays off afterwards!\nFigure 2: During the first epoch, samples are transformed and then cached to disk. Afterwards, the transformed sample is loaded from disk straight away. disk_cached_dataset = tonic.DiskCachedDataset( dataset=dense_dataset, cache_path=f\"cache/{dense_dataset.__class__.__name__}/train/{encoding_dim}/{dt}\", ) disk_cached_dataloader = DataLoader(disk_cached_dataset, **dataloader_kwargs) # cache on disk already available time3 = timeit(lambda: training_loop(disk_cached_dataloader, sinabs_model), number=1) time4 = timeit(lambda: training_loop(disk_cached_dataloader, exodus_model), number=1) We brought down epoch training time to 14s for the EXODUS model by not having to recompute the ToFrame transform! The speedup comes at the expense of disk space. How much disk space does it cost you may ask? The size of the original dataset file is 2.65 GB compared to the generated cache folder of 1.04 GB, which is not too bad!\nThe original dataset contained numpy events, whereas the cache folder contains dense tensors. We can compress the dense tensors that much because by default Tonic uses lightweight compression during caching. Disk-caching is generally applicable when training SNNs because it saves you the time to transform your events to dense tensors. Of course you could apply any other deterministic transform before caching it, and also easily apply augmentations to the cached samples as described in this tutorial!\nNow we notice one more thing. Overall GPU utilisation rate at this point is at ~80%, which means that the GPU is still idling the rest of the time, waiting for new data to arrive. So we can try to go even faster!\nGPU caching Instead of loading dense tensors from disk, we can try to cram all our dataset onto the GPU! The issue is that with dense tensors this wouldn’t work as they would occupy too much memory. But events are already an efficient format right? So we’ll store the events on the GPU as sparse tensors and then simply inflate them as needed by calling to_dense() for each sample. This method is obviously bound by GPU memory so works with rather small datasets such as the one we’re testing. However, once you’re setup, you can train with blazing speed. For GPU caching we are going to:\nCreate a new sparse dataset on the fly by loading them from the disk cache and calling to_sparse() on the transformed tensors. Create a new dataloader that now uses a single thread. Inflate sparse tensors to dense versions by calling to_dense() in the training loop. Figure 3: During the first epoch, transformed samples are loaded onto the GPU and stored in a list of sparse tensors. Whenever a new sample is needed, it is inflated by to_dense() and fed to the network. This process is almost instantaneous and now bound by what your model can process. The sparse tensor dataset takes about 5.7 GB of GPU memory. Not very efficient, but also not terrible. What about training speeds?\ndef gpu_training_loop(model): for data, targets in iter(sparse_tensor_dataloader): data = data.to_dense() sinabs.reset_states(model) output = model(data) loss = nn.functional.cross_entropy(output.sum(1), targets) loss.backward() time5 = timeit(lambda: gpu_training_loop(sinabs_model), number=1) time6 = timeit(lambda: gpu_training_loop(exodus_model), number=1) We’re down to less than 9s per epoch for the EXODUS model, which is another 40% less than disk-caching and an almost 15-fold improvement over the original Sinabs model using the naïve dataloading approach! Now we’re really exploiting the GPU as much as possible with a utilisation percentage of ~99%. All this without any qualitative impact on gradient computation.\nConclusion The principle of caching can be applied to any data that you apply deterministic transformations to, but it pays off particularly well for event-based data. By using cached samples and not having to recompute the same transformations every time, we save ourselves a lot of time during training. If the data already sits on the GPU when it is requested, the speedup is really high. After all, there is a reason why neural network accelerators heavily optimise memory caching to minimise time and energy spent on data movement. So when should you use either disk- or GPU-caching?\nDisk-caching: Broadly applicable, useful if you apply deterministic transformations to each sample and you train for many epochs. Not ideal if you’re low on disk space. GPU-caching: Only really suitable for small datasets and a bit more intricate to setup, but well worth the effort if you want to explore many different architectures / training parameters due to the speed of iteration. As a last note, you might be wondering why we don’t cache to the host memory instead of reading from a disk cache. This is totally possible, but the bottleneck at that point really is moving the data onto the GPU, which takes time. Whether the data sits in host memory or is loaded from disk using multiple worker threads doesn’t make much of a difference, because the GPU cannot handle the data movement. Since on disk we have much more space available than in RAM, we normally choose to do that.\nThis tutorial is available here for you to run, where you’ll also find some other training templates.\nAcknowledgements: Thanks a lot to Omar Oubari, Mina Khoei and Fabrizio Ottati for the feedback.\n","wordCount":"1666","inLanguage":"en","image":"https://lenzgregor.com/posts/train-snns-fast/featured.png","datePublished":"2022-11-27T00:00:00Z","dateModified":"2025-11-14T06:31:35Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://lenzgregor.com/posts/train-snns-fast/"},"publisher":{"@type":"Organization","name":"Gregor's Blog","logo":{"@type":"ImageObject","url":"https://lenzgregor.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://lenzgregor.com/ accesskey=h title="Gregor's Blog (Alt + H)">Gregor's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://lenzgregor.com/ title=Home><span>Home</span></a></li><li><a href=https://lenzgregor.com/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://lenzgregor.com/>Home</a>&nbsp;»&nbsp;<a href=https://lenzgregor.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Training spiking neural networks, fast.</h1><div class=post-meta><span title='2022-11-27 00:00:00 +0000 UTC'>November 27, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1666 words</div></header><div class=post-content><p>When training a spiking neural network (SNN), one might think about how the learning rate or model size affect training time. But when it comes to training <em>faster</em>, optimizing data movement is crucial. 3 out of the first 4 points in <a href=https://www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/>this list</a> weighted after potential speed-up have to do with how data is shaped and moved around between actual computations. It makes a huge difference, because training faster means getting results faster!</p><p>For this post we train an SNN on the <a href=https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/>Heidelberg Spiking Speech Commands</a> dataset to do audio stream classification. We&rsquo;ll benchmark different data loading strategies using <a href=https://github.com/neuromorphs/tonic>Tonic</a> and show that with the right strategy, we can achieve a more than 10-fold speed-up compared to the naïve approach.</p><p>For all our benchmarks, we already assume multiple worker threads and pinning the host memory. We&rsquo;ll increase throughput by using different forms of caching to disk or GPU. By applying deterministic transformations upfront and saving the new tensor, we can save a lot of time during training.
This tutorial is run on a machine with Ubuntu 20.04, an Intel Core i7-8700K CPU @ 3.70GHz, a Samsung SSD 850 and an NVIDIA GeForce RTX 3080 GPU.</p><p>All data from neuromorphic datasets in Tonic is provided as NxD numpy arrays. We&rsquo;ll need to transform this into a dense tensor to serve it to the GPU, and we&rsquo;ll also do some downsampling of time steps. Let&rsquo;s first define the transform. We know that samples of audio input data in this dataset are 0.8-1.2s long across 700 frequency channels at microsecond resolution. We&rsquo;ll <a href=https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.Downsample.html#tonic.transforms.Downsample>downsample</a> each sample to 100 channels, <a href=https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.ToFrame.html#tonic.transforms.ToFrame>bin</a> events every 4 ms to one frame and <a href=https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.CropTime.html#tonic.transforms.CropTime>cut</a> samples that are longer than 1s. That leaves us with a maximum of 250 time steps per sample.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tonic <span style=color:#f92672>import</span> transforms
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dt <span style=color:#f92672>=</span> <span style=color:#ae81ff>4000</span>  <span style=color:#75715e># all time units in Tonic in us</span>
</span></span><span style=display:flex><span>encoding_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dense_transform <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>Downsample(spatial_factor<span style=color:#f92672>=</span>encoding_dim <span style=color:#f92672>/</span> <span style=color:#ae81ff>700</span>),
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>CropTime(max<span style=color:#f92672>=</span><span style=color:#ae81ff>1e6</span>),
</span></span><span style=display:flex><span>        transforms<span style=color:#f92672>.</span>ToFrame(
</span></span><span style=display:flex><span>            sensor_size<span style=color:#f92672>=</span>(encoding_dim, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>), time_window<span style=color:#f92672>=</span>dt, include_incomplete<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>        ),
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Next we load the training dataset and assign the transform.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tonic <span style=color:#f92672>import</span> datasets
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dense_dataset <span style=color:#f92672>=</span> datasets<span style=color:#f92672>.</span>SSC(<span style=color:#e6db74>&#34;./data&#34;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>, transform<span style=color:#f92672>=</span>dense_transform)
</span></span></code></pre></div><p>Let&rsquo;s plot one such dense tensor sample:</p><div id=chart-result0 style=width:100%;height:500px></div><script>fetch("https://lenzgregor.com/data/charts/result0.json").then(e=>e.json()).then(e=>{Plotly.newPlot("chart-result0",e.data,e.layout,{responsive:!0,displayModeBar:!0,displaylogo:!1,modeBarButtonsToRemove:["lasso2d","select2d"]})})</script><p>Next we define a spiking model. We use a simple integrate-and-fire (IAF) feed-forward architecture. For each dataloading method, we&rsquo;re going to test two different models. One is a <a href=https://sinabs.readthedocs.io>Sinabs</a> model which is pretty much pure PyTorch plus for loops and the second one is an <a href=https://github.com/synsense/sinabs-exodus>EXODUS</a> model, which is also based on PyTorch but vectorizes gradient computation for the time dimension using custom CUDA code. Both models compute the same activations and gradients, but the latter provides a significant speedup.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sinabs.layers <span style=color:#66d9ef>as</span> sl
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> sinabs.exodus.layers <span style=color:#66d9ef>as</span> el
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SNN</span>(nn<span style=color:#f92672>.</span>Sequential):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, backend, hidden_dim: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> backend <span style=color:#f92672>==</span> sl <span style=color:#f92672>or</span> backend <span style=color:#f92672>==</span> el
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span><span style=color:#a6e22e>__init__</span>(
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(encoding_dim, hidden_dim),
</span></span><span style=display:flex><span>            backend<span style=color:#f92672>.</span>IAF(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style=display:flex><span>            backend<span style=color:#f92672>.</span>IAF(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style=display:flex><span>            backend<span style=color:#f92672>.</span>IAF(),
</span></span><span style=display:flex><span>            nn<span style=color:#f92672>.</span>Linear(hidden_dim, <span style=color:#ae81ff>35</span>),
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sinabs_model <span style=color:#f92672>=</span> SNN(backend<span style=color:#f92672>=</span>sl)<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>exodus_model <span style=color:#f92672>=</span> SNN(backend<span style=color:#f92672>=</span>el)<span style=color:#f92672>.</span>cuda()
</span></span></code></pre></div><h2 id=1-naïve-dataloading>1. Naïve dataloading<a hidden class=anchor aria-hidden=true href=#1-naïve-dataloading>#</a></h2><p>For the first benchmark we test the most common setup without any caching. We load every sample from an hdf5 file on disk which provides us with a numpy array in memory. For each sample, we apply our <code>dense_transform</code> defined earlier to create a dense tensor which we can then batch together with other samples and feed it to the network.</p><figure><img src=images/caching1.svg alt="Naïve caching"><figcaption>Figure 1: For every sample, we apply our transform ToFrame. The speed depends a lot on the CPU and the amount of worker threads used.</figcaption></figure><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sinabs
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> timeit
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tonic
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dataloader_kwargs <span style=color:#f92672>=</span> dict(
</span></span><span style=display:flex><span>    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>    shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    drop_last<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    pin_memory<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    collate_fn<span style=color:#f92672>=</span>tonic<span style=color:#f92672>.</span>collation<span style=color:#f92672>.</span>PadTensors(batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>),
</span></span><span style=display:flex><span>    num_workers<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>naive_dataloader <span style=color:#f92672>=</span> DataLoader(dense_dataset, <span style=color:#f92672>**</span>dataloader_kwargs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>training_loop</span>(dataloader, model):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> data, targets <span style=color:#f92672>in</span> iter(dataloader):
</span></span><span style=display:flex><span>        data, targets <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>squeeze()<span style=color:#f92672>.</span>cuda(), targets<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>        sinabs<span style=color:#f92672>.</span>reset_states(model)
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> model(data)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>cross_entropy(output<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>), targets)
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> timeit <span style=color:#f92672>import</span> timeit
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>time1 <span style=color:#f92672>=</span> timeit(<span style=color:#66d9ef>lambda</span>: training_loop(naive_dataloader, sinabs_model), number<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>time2 <span style=color:#f92672>=</span> timeit(<span style=color:#66d9ef>lambda</span>: training_loop(naive_dataloader, exodus_model), number<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><div id=chart-result1 style=width:100%;height:500px></div><script>fetch("https://lenzgregor.com/data/charts/result1.json").then(e=>e.json()).then(e=>{Plotly.newPlot("chart-result1",e.data,e.layout,{responsive:!0,displayModeBar:!0,displaylogo:!1,modeBarButtonsToRemove:["lasso2d","select2d"]})})</script><p>The Sinabs model takes more than two minutes per epoch using the simple strategy, which is far from exciting. By contrast, we can already see the huge speedup that EXODUS provides, reducing epoch time by a third! These results are our baseline with the basic dataloading.</p><h2 id=disk-caching>Disk caching<a hidden class=anchor aria-hidden=true href=#disk-caching>#</a></h2><p>Let&rsquo;s try to be a bit smarter now. <code>ToFrame</code> is a deterministic transform, so for the same sample we&rsquo;ll always receive the same transformed data. Given that we might train for 100 epochs, which looks at each sample 100 times, that&rsquo;s a lot of wasted compute! Now we&rsquo;re going to cache, which means save, those transformed samples to disk during the first epoch, so that we don&rsquo;t need to recompute them later on! To do this we simply wrap our previous dataset in a <a href=https://tonic.readthedocs.io/en/latest/reference/data_classes.html#tonic.DiskCachedDataset>DiskCachedDataset</a> and provide the cache path. When a new sample is about to be loaded, that class will first check if the transformed sample is already in the cache on disk and if it isn&rsquo;t, it will retrieve the original sample, apply the transform, cache it to disk and then serve it. This caching process slows down training in the first epoch, but it pays off afterwards!</p><figure><img src=images/caching2.svg alt="Disk caching"><figcaption>Figure 2: During the first epoch, samples are transformed and then cached to disk. Afterwards, the transformed sample is loaded from disk straight away.</figcaption></figure><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>disk_cached_dataset <span style=color:#f92672>=</span> tonic<span style=color:#f92672>.</span>DiskCachedDataset(
</span></span><span style=display:flex><span>    dataset<span style=color:#f92672>=</span>dense_dataset,
</span></span><span style=display:flex><span>    cache_path<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;cache/</span><span style=color:#e6db74>{</span>dense_dataset<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__<span style=color:#e6db74>}</span><span style=color:#e6db74>/train/</span><span style=color:#e6db74>{</span>encoding_dim<span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{</span>dt<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>disk_cached_dataloader <span style=color:#f92672>=</span> DataLoader(disk_cached_dataset, <span style=color:#f92672>**</span>dataloader_kwargs)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># cache on disk already available</span>
</span></span><span style=display:flex><span>time3 <span style=color:#f92672>=</span> timeit(<span style=color:#66d9ef>lambda</span>: training_loop(disk_cached_dataloader, sinabs_model), number<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>time4 <span style=color:#f92672>=</span> timeit(<span style=color:#66d9ef>lambda</span>: training_loop(disk_cached_dataloader, exodus_model), number<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><div id=chart-result2 style=width:100%;height:500px></div><script>fetch("https://lenzgregor.com/data/charts/result2.json").then(e=>e.json()).then(e=>{Plotly.newPlot("chart-result2",e.data,e.layout,{responsive:!0,displayModeBar:!0,displaylogo:!1,modeBarButtonsToRemove:["lasso2d","select2d"]})})</script><p>We brought down epoch training time to 14s for the EXODUS model by not having to recompute the <code>ToFrame</code> transform! The speedup comes at the expense of disk space. How much disk space does it cost you may ask? The size of the original dataset file is 2.65 GB compared to the generated cache folder of 1.04 GB, which is not too bad!</p><p>The original dataset contained numpy events, whereas the cache folder contains dense tensors. We can compress the dense tensors that much because by default Tonic uses lightweight compression during caching. Disk-caching is generally applicable when training SNNs because it saves you the time to transform your events to dense tensors. Of course you could apply any other deterministic transform before caching it, and also easily apply augmentations to the cached samples as described in <a href=https://tonic.readthedocs.io/en/latest/tutorials/fast_dataloading.html>this tutorial</a>!</p><p>Now we notice one more thing. Overall GPU utilisation rate at this point is at ~80%, which means that the GPU is still idling the rest of the time, waiting for new data to arrive. So we can try to go even faster!</p><h2 id=gpu-caching>GPU caching<a hidden class=anchor aria-hidden=true href=#gpu-caching>#</a></h2><p>Instead of loading dense tensors from disk, we can try to cram all our dataset onto the GPU! The issue is that with dense tensors this wouldn&rsquo;t work as they would occupy too much memory. But events are already an efficient format right? So we&rsquo;ll store the events on the GPU as sparse tensors and then simply inflate them as needed by calling to_dense() for each sample. This method is obviously bound by GPU memory so works with rather small datasets such as the one we&rsquo;re testing. However, once you&rsquo;re setup, you can train with <em>blazing</em> speed. For GPU caching we are going to:</p><ol><li>Create a new sparse dataset on the fly by loading them from the disk cache and calling to_sparse() on the transformed tensors.</li><li>Create a new dataloader that now uses a single thread.</li><li>Inflate sparse tensors to dense versions by calling to_dense() in the training loop.</li></ol><figure><img src=images/caching3.svg alt="Disk caching"><figcaption>Figure 3: During the first epoch, transformed samples are loaded onto the GPU and stored in a list of sparse tensors. Whenever a new sample is needed, it is inflated by to_dense() and fed to the network. This process is almost instantaneous and now bound by what your model can process.</figcaption></figure><p>The sparse tensor dataset takes about 5.7 GB of GPU memory. Not very efficient, but also not terrible. What about training speeds?</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>gpu_training_loop</span>(model):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> data, targets <span style=color:#f92672>in</span> iter(sparse_tensor_dataloader):
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>to_dense()
</span></span><span style=display:flex><span>        sinabs<span style=color:#f92672>.</span>reset_states(model)
</span></span><span style=display:flex><span>        output <span style=color:#f92672>=</span> model(data)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>functional<span style=color:#f92672>.</span>cross_entropy(output<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>), targets)
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>time5 <span style=color:#f92672>=</span> timeit(<span style=color:#66d9ef>lambda</span>: gpu_training_loop(sinabs_model), number<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>time6 <span style=color:#f92672>=</span> timeit(<span style=color:#66d9ef>lambda</span>: gpu_training_loop(exodus_model), number<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><div id=chart-result3 style=width:100%;height:500px></div><script>fetch("https://lenzgregor.com/data/charts/result3.json").then(e=>e.json()).then(e=>{Plotly.newPlot("chart-result3",e.data,e.layout,{responsive:!0,displayModeBar:!0,displaylogo:!1,modeBarButtonsToRemove:["lasso2d","select2d"]})})</script><p>We&rsquo;re down to less than 9s per epoch for the EXODUS model, which is another 40% less than disk-caching and an almost 15-fold improvement over the original Sinabs model using the naïve dataloading approach! Now we&rsquo;re really exploiting the GPU as much as possible with a utilisation percentage of ~99%. All this without any qualitative impact on gradient computation.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>The principle of caching can be applied to any data that you apply deterministic transformations to, but it pays off particularly well for event-based data. By using cached samples and not having to recompute the same transformations every time, we save ourselves a lot of time during training. If the data already sits on the GPU when it is requested, the speedup is really high. After all, there is a reason why neural network accelerators heavily optimise memory caching to minimise time and energy spent on data movement. So when should you use either disk- or GPU-caching?</p><ul><li><strong>Disk-caching</strong>: Broadly applicable, useful if you apply deterministic transformations to each sample and you train for many epochs. Not ideal if you&rsquo;re low on disk space.</li><li><strong>GPU-caching</strong>: Only really suitable for small datasets and a bit more intricate to setup, but well worth the effort if you want to explore many different architectures / training parameters due to the speed of iteration.</li></ul><p>As a last note, you might be wondering why we don&rsquo;t cache to the host memory instead of reading from a disk cache. This is totally possible, but the bottleneck at that point really is moving the data onto the GPU, which takes time. Whether the data sits in host memory or is loaded from disk using multiple worker threads doesn&rsquo;t make much of a difference, because the GPU cannot handle the data movement. Since on disk we have much more space available than in RAM, we normally choose to do that.</p><p>This tutorial is available <a href=https://github.com/biphasic/snn-training-templates/blob/main/posts/training-snns-faster/index.ipynb>here</a> for you to run, where you&rsquo;ll also find some other training templates.</p><p>Acknowledgements: Thanks a lot to Omar Oubari, Mina Khoei and Fabrizio Ottati for the feedback.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://lenzgregor.com/tags/snn/>SNN</a></li></ul><nav class=paginav><a class=prev href=https://lenzgregor.com/posts/file-format-benchmarking/><span class=title>« Prev</span><br><span>Efficient compression for event-based data</span>
</a><a class=next href=https://lenzgregor.com/posts/event-cameras/><span class=title>Next »</span><br><span>Rethinking the way our cameras see.</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Training spiking neural networks, fast. on x" href="https://x.com/intent/tweet/?text=Training%20spiking%20neural%20networks%2c%20fast.&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f&amp;hashtags=SNN"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training spiking neural networks, fast. on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f&amp;title=Training%20spiking%20neural%20networks%2c%20fast.&amp;summary=Training%20spiking%20neural%20networks%2c%20fast.&amp;source=https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training spiking neural networks, fast. on reddit" href="https://reddit.com/submit?url=https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f&title=Training%20spiking%20neural%20networks%2c%20fast."><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training spiking neural networks, fast. on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training spiking neural networks, fast. on whatsapp" href="https://api.whatsapp.com/send?text=Training%20spiking%20neural%20networks%2c%20fast.%20-%20https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training spiking neural networks, fast. on telegram" href="https://telegram.me/share/url?text=Training%20spiking%20neural%20networks%2c%20fast.&amp;url=https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Training spiking neural networks, fast. on ycombinator" href="https://news.ycombinator.com/submitlink?t=Training%20spiking%20neural%20networks%2c%20fast.&u=https%3a%2f%2flenzgregor.com%2fposts%2ftrain-snns-fast%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://lenzgregor.com/>Gregor's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>