<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Gregor's Blog</title><link>https://lenzgregor.com/posts/</link><description>Recent content in Posts on Gregor's Blog</description><generator>Hugo -- 0.152.2</generator><language>en-gb</language><atom:link href="https://lenzgregor.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Event cameras in 2025, Part 1</title><link>https://lenzgregor.com/posts/event-cameras-2025-part1/</link><pubDate>Mon, 13 Oct 2025 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/event-cameras-2025-part1/</guid><description>Built for speed, looking for the mainstream: Promising commercial markets for event cameras.</description><content:encoded><![CDATA[<p>Earlier this year, I stepped down as CTO of <a href="https://neurobus.ai">Neurobus</a> and transitioned to a role in the field of robotics in London. Despite that shift, I still believe in the potential of event cameras, especially for edge computing. Their asynchronous data capture model is promising, but the technology isn’t quite there yet. In two parts, I want to outline the main markets that I think could drive the adoption of event cameras and also talk about what’s currently holding the technology back.</p>
<h2 id="industry-landscape">Industry landscape</h2>
<p>Fifteen years ago, the market for event cameras barely existed. Today, it’s worth around 220 million dollars. That rate of growth is actually in line with how most sensor technologies develop. LiDAR, for example, was originally created in the 1970s for military and aerospace applications. It took decades before it found its way into mainstream products, with broader adoption only starting in the 2010s when autonomous vehicles began to emerge. Time-of-flight sensors were originally explored in the 1980s, but only became widespread after Apple introduced Face ID in 2015.</p>
<p>Event cameras appear to be following the same trajectory. They’ve been tested across various industries, but none have yet revealed a compelling, large-scale use case that pushes them into the mainstream. When we started Neurobus in 2023, there were already several companies building neuromorphic hardware such as event-based sensors and processors. What was missing was a focus on the software and real-world applications. Camera makers were happy to ship dev kits, but few people were actually close to end users. Understanding when and why an event camera outperforms a traditional RGB sensor requires deep domain knowledge. That’s the gap I tried to bridge.</p>
<p>It quickly became clear that finding the right applications is incredibly difficult. Adoption doesn’t just depend on technical merit. It needs mature software, supporting hardware, and a well-defined use case. In that sense, event cameras are still early in their journey. Over the past year, I’ve explored several sectors to understand where they might gain a foothold.</p>
<h2 id="space">Space</h2>
<p>The private space sector has grown quickly in the last two decades, largely thanks to SpaceX. By driving down launch costs to a fraction of what they used to be, the company has made it far easier to get satellites into orbit. Here is a lovely graph showing launch costs over the past decades. Notice the logarithmic y axis! Those launch costs are going to continue to drop as rockets with bigger payloads, such as Starship, are developed.</p>
<iframe src="https://ourworldindata.org/grapher/cost-space-launches-low-earth-orbit?tab=chart" loading="lazy" style="width: 100%; height: 600px; border: 0px none;" allow="web-share; clipboard-write"></iframe>
<h3 id="space-situational-awareness-ssa-from-the-ground">Space Situational Awareness (SSA) from the ground</h3>
<p>With more satellites in orbit, the risk of collisions increases, and that’s where space situational awareness, or SSA, comes into play. At the moment, the US maintains a catalogue of orbital objects and shares it freely with the world, but it’s unlikely that this will remain free forever. Other countries are starting to build their own tracking capabilities, particularly for objects in Low Earth Orbit (LEO), which spans altitudes up to 2,000 km. SSA is mostly handled from the ground, using powerful RADAR systems. These systems act like virtual fences that detect any object passing through, even those as small as eight centimeters at an altitude of 1,500 km. RADARs are expensive to build and operate, but their range and reliability are unmatched. For SSA solutions on the ground, optical systems play a smaller role of real-time tracking of specific objects. People built ground-based <a href="https://www.westernsydney.edu.au/icns/research_projects/current_projects/astrosite">event camera SSA systems</a>, but it is not clear what advantages they bring over conventional, high resolution, integrating sensors. There’s nothing that I know of up there that is spinning so fast that you need microsecond resolution to capture it.</p>
<h3 id="space-domain-awareness-sda-in-orbit">Space Domain Awareness (SDA) in orbit</h3>
<p>As orbit becomes more crowded and militarized, the need to monitor areas in real time is growing, especially regions not visible from existing ground stations (as in, anywhere other than your country and allies). Doing this from space itself offers a significant advantage, but using RADAR in orbit isn’t practical due to the power constraints of small satellites. Instead, optical sensors that can quickly pivot to observe specific areas are a better fit. To achieve good coverage, you’d need a large number of these satellites, which means that payloads must be compact and low-cost. This is where event cameras could come in. Their power efficiency makes them ideal for persistent monitoring, especially in a sparse visual environment like space. Since they only capture changes in brightness, pointing them into mostly dark space allows them to exploit sparsity very well. The data they generate is already compressed, reducing the bandwidth needed to transmit observations back to Earth. For low-power surveillance satellites in LEO, that’s a significant advantage.</p>
<h3 id="earth-observation-eo">Earth Observation (EO)</h3>
<p>In Earth observation, optical sensors sit on satellites that need to orbit as low as possible in order to increase angular resolution / swath. They revolve around the Earth roughly every 90 minutes, capturing the texture-rich surface. Using an event camera for that would just generate enormous amounts of data that is of the wrong kind anyway because in EO you are interested in multispectral bands and high spatial resolution. However there is a case that might make it worthwhile: when you compensate for the lateral motion of the satellite and fixate the event camera on a specific spot for continuous monitoring. Such systems exist today already (check out <a href="https://github.com/QingyongHu/VISO">this dataset</a>) to monitor airports, city traffic and probably some missile launch sites. Using an event camera for that would reduce processing to a bare minimum, and provide high temporal resolution objects that move. The low data rate would also allow for super low bandwidth live video streaming! Unless we’re talking constellations of 10k+ satellites however, it remains a niche use case for the military to monitor enemy terrain.</p>
<p><img alt="event-cameras-in-space" loading="lazy" src="/posts/event-cameras-2025-part1/images/space.jpg">
<em>Potential applications of event cameras in orbit. 1. Space Domain Awareness (SDA) is about receiving requests from the ground to monitor an object in LEO in real time. 2. Video live streaming Earth observation (EO). Compensating for the satellite&rsquo;s lateral motion, event cameras could monitor sites for ~10 minutes per orbit, depending on the altitude. 3. Star tracking. Lots of redundant background means that we can focus on the signal of interest using little processing.</em></p>
<h3 id="in-orbit-servicing">In-orbit servicing</h3>
<p>In-orbit servicing includes approaching and grabbing debris to de-orbit it or docking with another satellite or spacecraft for refueling or repair. These operations are delicate and often span several hours, typically controlled from the ground. With the number of satellites in orbit continuously increasing and <a href="https://en.wikipedia.org/wiki/List_of_space_stations#Planned_and_proposed">as many as 10 space stations planned</a> to be built within the next decade, reliable and autonomous docking solutions will become essential. Current state-of-the-art systems already use stereo RGB and LiDAR, but event cameras might offer benefits in challenging lighting conditions or when very fast, reactive maneuvers are needed in emergency situations. I think that in-orbit servicing has many other challenges before event cameras fix the most pressing problem in that area.</p>
<h3 id="star-tracking">Star tracking</h3>
<p>Star trackers are a standard subsystem on most satellites. Companies like <a href="https://sodern.com/en/star-trackers/">Sodern</a> sell compact, relatively low-cost units that point upward into the sky to determine a satellite’s orientation relative to known star constellations. Today’s trackers typically deliver attitude estimates at around 10–30 Hz, consuming 3–7 W depending on the model. For most missions, that’s good enough, but there’s room for improvement.
Since stars appear as sparse, bright points against an almost entirely dark background, they align perfectly with the event camera’s strengths. Instead of continuously integrating the whole scene, an event-based tracker could focus only on the few pixels where stars actually appear, cutting down on unnecessary processing. In principle, this allows attitude updates at kilohertz rates while using less compute and bandwidth. Faster updates could improve control loops during high-dynamic maneuvers or enable more precise pointing for small satellites that lack bulky stabilization hardware.
From a software perspective, the task remains relatively simple: once the events are clustered into star positions, the rest of the pipeline is the conventional map-matching problem of aligning observations to a star catalogue. No complex machine learning is needed. The main challenge, as with any space payload, lies in making the hardware resilient to radiation and thermal extremes.</p>
<h3 id="revenue-generation">Revenue generation</h3>
<p>In space applications, cost isn’t the limiting factor, which would make it a great place to start testing equipment that is not mass produced yet. Space-grade systems already command premium prices, so a $1,000+ event camera is not out of place. Compared to SDA and EO video streaming, which can generate recurring revenue as a service by providing recent and real-time data, in-orbit servicing systems or star trackers are more likely to be sold as one-off hardware solutions, which makes the business case less scalable.
In either case, there’s a growing need for advanced vision systems that can operate efficiently on the edge in space. Right now, the space market for event cameras is still at an early stage, but the interest is real.</p>
<h2 id="manufacturing--science">Manufacturing / Science</h2>
<p>In industrial vision, specifically in manufacturing and scientific environments, objects move fast and precision matters. These settings are full of high-speed conveyor belts, precise milling machines, and equipment that needs real-time monitoring. On paper, this seems like a great match for event cameras, which excel in capturing rapid motion with minimal latency. But the reality is more complicated.
In most factories, computer vision systems are already deeply integrated into broader automation pipelines. Changing a single sensor, even for one with better temporal precision, often isn’t worth the disruption. If a factory wants slightly better accuracy, they can usually just upgrade to a 100+ Hz version of their existing system. Need to count bottles flying past on a line? A cheap line-scanning camera will do the trick. Want to monitor vibrations on a machine? A one-dollar vibration sensor is simpler and more reliable.</p>
<p>Some also consider battery-powered monitoring devices for warehouses and other low-maintenance settings, where low-power vision sensors could make sense. But even there, the appeal is limited in my experience. Someone still has to replace or recharge the battery eventually, and most computer vision systems already do a good enough job without needing an event-based solution.
That said, there are niche applications where event cameras could shine. High-speed scientific imaging is one example, such as tracking combustion events in engines, analysing lightning, or sorting high-throughput cell flows in cytometry. Today, these tasks often rely on bulky high-speed cameras like a <a href="https://www.phantomhighspeed.com/">Phantom</a>, which require artificial lighting, heavy cooling systems, and massive data pipelines, often just to record a few seconds of footage.</p>
<p>Event cameras could offer a much more compact and energy-efficient alternative. They don’t need high-speed links or huge data buffers, and they can be powered through a USB port and still achieve sub ms latency. I think that event cameras could become a competitor for the use of high speed cameras in scientific settings, with a focus on small form factor and mobile applications. One challenge and active research area here is reconstructing high-quality video frames from events. The good news is that we’re seeing steady progress, and there’s even a public leaderboard tracking the latest benchmarks <a href="https://ercanburak.github.io/evreal.html">here</a>. However, these methods currently consume an entire 100W GPU so they’re done offline. One of the biggest hurdles is collecting good ground truth data for what reconstructed frames should look like, which is why most researchers still rely on simulation. But if such reconstruction models get very good, I could imagine a business model where a user uploads the raw events, chooses the desired fps, and gets back videos with up to 10k fps. Pricing is per frame reconstructed, be it 10 Hz for 2 hours or 1 kHz for 1 second.</p>
<p><img alt="high-speed-rendering" loading="lazy" src="/posts/event-cameras-2025-part1/images/science.jpg">
<em>A comparison between a high-speed camera and an event camera. The high-speed camera workflow shows three steps: recording, postprocessing with video compression, and the result as high frame rate RGB video. This path requires handling a large amount of data. The event camera workflow shows recording followed by frame reconstruction, leading to high frame rate greyscale video, while generating much less data overall. The diagram highlights how event cameras provide efficient high-speed imaging compared to traditional high-speed cameras.</em></p>
<h2 id="automotive">Automotive</h2>
<p>Event cameras in cars have had several testers, but none have stuck to it for now. While the technical case is strong, the path to adoption is complex, shaped by legacy systems, cost constraints, and the structure of the automotive supply chain.
Modern cars already rely on a robust stack of sensors. RGB cameras, LiDAR, RADAR, and ultrasonic sensors all work together to enable functions like adaptive cruise control, lane keeping, emergency braking, and parking assistance. These systems are designed to be redundant and resilient across various weather and lighting conditions. For a new sensor like an event camera to be added, it must address a specific problem that the current setup cannot. In the chart below, I marked the strengths and weaknesses of sensors currently employed in cars. I rated each one on a scale from 1 to 5. Event cameras (orange) have a considerable overlap with RGB cameras (green), apart from the performance in glare or high contrast scenarios, which is covered well by RADAR (red).</p>

<div id="chart-automotive_sensors" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/automotive_sensors.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-automotive_sensors', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>Nevertheless, the unique selling point of combined high temporal resolution and high dynamic range could be a differentiator. For example, detecting fast-moving objects to avoid collisions could become safety-critical. But in practice, these are edge cases, and current systems already perform well enough in most of them. The reality of integration is that automotive development happens within a well-defined supply chain. Original equipment manufacturers (OEM) like Toyota or Mercedes-Benz rarely integrate sensors themselves. Instead, they depend on Tier 1 suppliers like Bosch or Valeo to deliver complete perception modules. Those Tier 1s work with Tier 2 suppliers who provide the actual components, including sensors and chips.</p>
<p>For event cameras to make it into a production vehicle, they need to be part of a fully validated module offered by a Tier 1 supplier. This includes software, calibration, diagnostics, and integration support. For startups that focus on event camera use cases, this creates a huge barrier, as the margins are already thin. You need a path to integration that matches the way the industry actually builds cars. Companies like NVIDIA are even starting to reshape this landscape. Their Drive Hyperion platform bundles sensors and compute into a single integrated solution, reducing the role of traditional Tier 1 suppliers. Hyperion already supports a carefully selected list of cameras, LiDARs, and RADARs, along with tools for simulation, data generation, and sensor calibration. Event cameras aren’t on that list yet. That means research teams inside OEMs have no easy way to test or simulate their output, let alone train models on it.</p>
<p>The way automotive AI systems are being designed has also changed. Instead of having separate modules for tasks like lane detection or pedestrian recognition, modern approaches rely on end-to-end learning. Raw sensor data is fed into a large neural network that directly outputs steering and acceleration commands. This architecture scales better but makes it harder to add a new modality. Adding a sensor like an event camera doesn’t just mean collecting or simulating new data. It also means rewriting the training pipeline and handling synchronization with other sensors. Most OEMs are still trying to get good reliability from their existing stack. They’re not in a rush to adopt something fundamentally new, especially if it comes without mature tooling.</p>
<p>Cost is another serious constraint. Automotive suppliers operate on tight margins, and every component is scrutinized. For instance, regulators in Europe and elsewhere are mandating automatic emergency braking. On paper, this sounds like a perfect opportunity for event cameras, especially to detect pedestrians at night. But in reality, carmakers are more likely to spend 3 extra dollars to improve their headlights than to introduce a new sensor that complicates the system. In fact, the industry trend is toward reducing the number of sensors. Fewer sensors mean simpler calibration, fewer failure modes, and lower integration overhead. From that perspective, adding an event camera can feel like a step in the wrong direction unless one is able to replace another modality altogether.</p>
<p>One area where event cameras might gain traction sooner is in the cabin. Driver and passenger monitoring systems are becoming mandatory in many regions. These systems typically use a combination of RGB and infrared cameras to detect gaze direction, drowsiness, and presence. An event camera could potentially replace both sensors, offering better performance in high-contrast lighting conditions, such as when bright headlights shine into the cabin at night. Cabin monitoring systems are often independent from the main driving compute platform, they have faster iteration cycles, and the integration hurdles are lower. Once an event camera is proven in this domain, it could gradually be expanded to support gesture control, seat occupancy, or mood estimation.</p>
<p>Visual light communication (VLC) could become a relevant application in autonomous vehicles. The idea is simple: LEDs that are already in our environment—traffic lights, street lamps, brake lights, even roadside signs—can modulate their intensity at kilohertz rates to broadcast short messages, while a receiver on the vehicle decodes them optically. Event cameras are a particularly good fit for this because they combine microsecond temporal resolution with useful spatial resolution, letting a single sensor both localize the source and decode high-frequency flicker without the rolling-shutter or motion-blur issues that plague standard frame sensors. <a href="https://woven-visionai.github.io/evlc-dataset/">Recent work</a> from Woven by Toyota is a good snapshot of where this is headed: they released an event-based VLC dataset with synchronized frames, events, and motion capture and demonstrated LED beacons flickering at 5 kHz encoded via inter-blink intervals. While VLC is not going to be the main driver to integrate event cameras into cars, it&rsquo;s one &lsquo;part of the package&rsquo; application.</p>
<p>Automotive adoption moves slowly. Getting into a car platform can take five to ten years, and the technical hurdles are only part of the story. To succeed, companies developing event cameras need staying power and ideally, strategic partnerships with Tier 1 suppliers or compute platform providers. For a small startup, this is a tough road to walk alone. For the moment, in-cabin sensing might be the most realistic starting point.</p>
<h2 id="defence">Defence</h2>
<p>Many of the technologies we now take for granted started with defence: GPS, the internet, radar, night vision, even early AI. Defence has always been an early adopter of bleeding-edge tech, not because it’s trendy, but because the stakes demand it. Systems need to function in low visibility, track fast-moving targets, and operate independently in environments where there’s no GPS, no 5G, and no time to wait for remote instructions. In such cases, autonomy is a requirement and modern military operations are increasingly autonomous. Drone swarms, for example, don’t rely on one pilot per unit anymore. A central command issues a mission, and the swarm executes it even deep behind enemy lines. That shift toward onboard intelligence makes the case for sensors that are low-latency, low-power, and can extract meaningful information with minimal compute. That’s where event cameras can play a role. Their high temporal resolution and efficiency make them well suited to motion detection and fast reaction loops in the field.</p>
<p><img alt="drone-detection" loading="lazy" src="/posts/event-cameras-2025-part1/images/drone-detection.jpeg">
<em>Drone detection based on time surfaces at the European Defence Tech Hackathon</em></p>
<p>We put this into practice at the European Defence Tech Hackathon in Paris last December. The Ukrainian military had outlined their biggest challenges, and drones topped the list by a mile. Over 1.2 million were deployed in Ukraine last year alone <a href="https://mod.gov.ua/news/minoboroni-peredalo-ponad-1-2-mln-droniv-dlya-sil-oboroni-shhe-100-000-nadijdut-do-kinczya-grudnya">according to its Ministry of Defence</a>, most of them manually piloted First Person View (FPV) drones. They include variants that carry a spool of lightweight optical fibre, often 10 km long, that allows the pilot to control the drone by wire, without radio signals, see the photo below. And Ukraine&rsquo;s target for 2025 is a staggering <a href="https://www.forbes.com/sites/davidaxe/2025/03/12/45-million-drones-is-a-lot-of-drones-its-ukraines-new-production-target-for-2025/">4.5 million</a>. Main supply routes are now completely <a href="https://www.youtube.com/watch?v=ltYPXOSddOg">covered in anti-drone nets</a>, and fields close to the frontline are <a href="https://x.com/Archer83Able/status/1927381503303987606">covered with optical fibre</a>. Both sides are racing to automate anti-drone systems. At that hackathon in December, we developed an event-based drone detection system and won first place. That experience made it clear that the demand is real! Taking down an enemy drone can mean saving a soldier’s life. There’s also a pragmatic reason why the defence sector is attractive: volume. Every drone, loitering munition, or autonomous ground vehicle is a potential autonomous system. Event cameras aren’t the only option, but they’re a good candidate when fast response times are crucial and power budgets are tight.</p>
<p><img alt="fpv-optical-fibre-drone" loading="lazy" src="/posts/event-cameras-2025-part1/images/fpv-drone.jpg">
<em>An FPV drone with an optical fibre spool attached. Photo by Maxym Marusenko/NurPhoto</em></p>
<p>The European Union <a href="https://www.brusselstimes.com/eu-affairs/1562906/eu-defence-escape-clause-on-right-track-but-uncertainty-about-loans-for-joint-procurement">has committed €800 billion</a> to defence and technological sovereignty. Whether that funding reaches startups effectively is another question, but the political intent is clear. Europe wants to control more of its military tech stack, and that opens the door to new players with homegrown solutions. Already today we see many new defence startups on the scene, a lot of them focusing on AI and autonomy.
Defence comes with a lot of red tape, whether it’s access to real data, the reliance on slow government funding, the fact that it can resemble a walled garden, or simply the limited options in terms of exits. But out of all the sectors I’ve looked into, defence stands out as the most likely place for event cameras to find product-market fit first. There’s real demand, shorter adoption cycles, and a willingness to experiment. There are new companies like Optera in <a href="https://optera.au/">Australia</a> and <a href="https://tempo-sense.com/">TempoSense</a> in the US (recent <a href="https://tub-rip.github.io/eventvision2025/slides/2025CVPRW_Tempo_Sense.pdf">slides</a> with more info) that are experimenting with making event sensors for the defence sector, and <a href="https://www.prophesee.ai/event-based-vision-defense-aerospace/">Prophesee</a> in Europe now <a href="https://framos.com/events/imaging-next-2025/">openly</a> presents their work on drone navigation, detection and anti drone tech. Also Leonardo, the Italian defence company, released a <a href="https://arxiv.org/abs/2409.16099">paper</a> experimenting with event cameras for drone detection.</p>
<h2 id="wearables">Wearables</h2>
<p>Back in 2021, I explored the use of event cameras for eye tracking. I had conversations with several experts in the field, and their feedback was clear: for most mobile gaze tracking applications, even a simple 20 Hz camera was good enough. In research setups that aim to study microsaccades or other rapid eye movements, the high temporal resolution of event cameras could be useful. But even then, a regular 120 Hz camera might still get the job done.</p>
<p>What I didn’t fully appreciate back then was the importance of power consumption in wearable devices. My thinking was centered around AR and VR headsets, which already include high refresh rate displays that consume significant power. In that context, saving a few milliwatts didn’t seem that important. But smart glasses are a different story. They need to run for hours or days, and every bit of energy efficiency matters to prolong battery life and allow for slimmer designs.</p>
<p>Prophesee recently announced a partnership with <a href="https://www.tobii.com/">Tobii</a>, who are a major supplier of eye tracking solutions. <a href="https://www.zinnlabs.com/">Zinn Labs</a>, one of the early adopters of event-based gaze tracking, were acquired in February 2025. These developments suggest that there is traction for the technology, especially in applications where power efficiency and responsiveness are key. According to Tobi Delbruck from ETH Zurich, if spectacles catch on like smartphones, then this will be a true mass production of event vision sensors. That said, the broader question remains whether the smart glasses market will scale any time soon. Event cameras may be a good fit from a technical perspective, but the commercial success of wearables will depend on factors beyond just sensor performance, such as the maturity of the software ecosystem.</p>
<p><img alt="zinn-labs" loading="lazy" src="/posts/event-cameras-2025-part1/images/zinn-labs.jpg">
<em>Prototype by Zinn Labs that includes a GenX320 sensor.</em></p>
<h2 id="a-note-on-robotics">A Note on Robotics</h2>
<p>Even though fast sensors should be great for fine-grained, low-latency control loops, the field currently faces very different challenges at the moment, at least for building Autonomous Mobile Robots or Humanoids. Controlling an arm or a leg using processing-heavy Visual Language Action (VLA) models is extremely challenging, and neither input frame rate, nor dynamic range are the limitations. Even once more performant models become available, you&rsquo;ll have to deal with the same challenges as in the Automotive sector, which is that adding a new modality needs lots of new (simulated) data.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Event cameras have come a long way, but they are still searching for the right entry points into the mainstream. The most promising early markets seem to be in defence, where speed and efficiency are critical for drones and autonomous systems, and in wearables, where power constraints make their efficiency truly valuable. Other sectors like space, automotive, and manufacturing show interesting opportunities, but adoption is likely to remain slower and more niche for now.
The trajectory of this technology suggests that with persistence and the right applications, event cameras will carve out their role in the broader sensor landscape.</p>
<p>In Part 2, I will discuss the technological hurdles that event cameras are facing today.</p>
]]></content:encoded></item><item><title>Morocco Desert Trip 2025</title><link>https://lenzgregor.com/posts/morocco-desert-2025/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/morocco-desert-2025/</guid><description>A journey through the Moroccan desert for star gazing and cultural exploration.</description><content:encoded><![CDATA[<p>When I was 22, I went to New Zealand with my dad and experienced one of the most memorable star gazing nights on Mount Cook, with stars visible down to the horizon, in the clear skies of a no flight zone. It was magical, and I wanted to do something similar with my wife. Star gazing is best done in remote places, and because I had never been to the African continent, I thought why not go to the desert? I looked up the date for New Moon, when the night is darkest, 6 months in advance. This is how our trip to the Moroccan desert came about.</p>
<p>From London, we flew directly to Ouarzazate, which is east of the Atlas mountains. In our rented car from <a href="https://zizdratours.com/#">Zizdra Tours</a>, which we were very happy with, we drove via Tinghir to Merzouga, to make our way to the Sahara via the <em>route of 1000 Kasbahs</em>, which are traditional fortresses, often made of earth and clay, that served as residences for leaders and defenses against invasions between the 17th to 19th century. You can find a <a href="https://maps.app.goo.gl/TNmGZPe5RD9PrQPf8?g_st=ac">list</a> of all the hotels we stayed at and places we visited.</p>
<p><img alt="Route through the desert" loading="lazy" src="/posts/morocco-desert-2025/photos/gmaps.png">
<em>Our <a href="https://maps.app.goo.gl/dY68ydCaDhZusHCD9">route</a> from Ouarzazate to Errachidia by car in 8 days. It was about 500km in total.</em></p>
<h2 id="ouarzazate---lots-of-kasbahs">Ouarzazate - lots of Kasbahs</h2>
<p>Luckily for us London dwellers, there&rsquo;s a direct Ryanair flight to Ouarzazate, which is quite a drastic change of scenery. Taking off at the warehouse that is Stansted in cold weather, on a flight that was at ~15% capacity, you step on the tarmac to sunshine and 25 degrees at a tiny airport. If you happen to fly there as well, make sure you buy and install your esim before takeoff, as there&rsquo;s no wifi at the airport and mobile data roaming costs are exorbitant. For the night, we stayed at <a href="https://maps.app.goo.gl/oxW4jaTxobUo8JUX8">Berbère Palace</a>, where we ordered 2 couscous and a salad, but what they brought us was food for two days!</p>
<p>From Ouarzazate it&rsquo;s about a 40 minute drive to <a href="https://maps.app.goo.gl/p2U7Wmh3oyYtQAvh7">Aït Benhaddou</a>, a Berber castle that is a UNESCO World Heritage site. Scenes from both Gladiator and a dozen other movies were filmed there. It was impressive to see, and full of local artists that exhibited their artworks. We hired a local guide for 200 Dirham (~20 Euros), to show us around and explain the rich history of the Kasbah.</p>
<p><img alt="Aït Benhaddou Kasbah" loading="lazy" src="/posts/morocco-desert-2025/photos/ait-ben-haddou.jpg">
<em>The impressive Aït Benhaddou Kasbah, a UNESCO World Heritage site where many movies have been filmed.</em></p>
<p>The second Kasbah we visited was <a href="https://maps.app.goo.gl/2v7Z27zEYQn5T6nX6">Taourirt</a>, directly in Ouarzazate, again with a local guide. It was under renovation because of an earthquake in 2023. He told us about how the Amazigh, the people living in the area before the Arabs arrived, became known as Berber, and the multitude of French, Arabic and Amazigh culture, while sharing a cup of mint tea. Did you know that in Morocco they use three alphabets? Arabic, Latin, and Tamazight.</p>
<p><img alt="Tamazight alphabet" loading="lazy" src="/posts/morocco-desert-2025/photos/tamazhir.jpg">
<em>Tamazight, with its own alphabet, is spoken in Amazigh (Berber) communities across Morocco, Algeria, Tunisia, Libya, and parts of Mauritania.</em></p>
<p>In town, my wife found a wonderfully decorated <a href="https://maps.app.goo.gl/PYYQKVRxR3W3SQpC7">French restaurant</a> with an indoor pool and great food. After some days of eating tagine and couscous, it was a welcome change.</p>
<h2 id="tinghir---beautiful-palm-groves">Tinghir - beautiful palm groves</h2>
<p>About a 2.5h drive to the east, in an oasis, is Tinghir, a remote town with beautiful palm groves. We stopped at a peaceful <a href="https://maps.app.goo.gl/pWy6PrqDXQg52Uku7">café with a fish pond</a> from which we went on an idyllic hike in the shadows of palm trees, crossing a river, walking through the ruins of an abandoned kasbah, and returning to the cafe for some sweet mint tea. It was one of those serendipitous encounters, and I highly recommend it if you make it to Tinghir. Close by, another walk took us through the impressive <a href="https://maps.app.goo.gl/N56djVt13mZ3ui1u7">Todgha gorge</a> with cliffs 300m high on either side. Back at the hotel, we joined the hotel staff playing traditional Berber music in the evening. Because I knew a bit of percussion, we immediately bonded, so before leaving the hotel, they surprised us with an authentic Moroccan experience by making us wear their traditional wedding outfits, so we could get married a second time! The staff at <a href="https://maps.app.goo.gl/ceWmVgeKCfF2QVDc6">Maison d&rsquo;hôte Igrane</a> was incredibly friendly and made the stay in Tinghir feel like an authentic experience.</p>
<p><img alt="A walk in the palm groves" loading="lazy" src="/posts/morocco-desert-2025/photos/tinghir.jpg">
<em>My wife posing in the midst of the ruins of a kasbah, just next to a beautiful palm grove.</em></p>
<h2 id="merzouga-and-desert-camp">Merzouga and desert camp</h2>
<p>We then drove another 2.5h towards Merzouga. Our destination was the <a href="https://maps.app.goo.gl/NzeNGQrVF8fVczNRA">Sahara Pearl</a> hotel which is located at the border to the Sahara, and offers postcard views of a scenery that could be in Aladdin. This was the best hotel we stayed at, because of its breathtaking views in combination with a modern room (the hotel was built in 2024). We rode camels, sandboarded, and watched the sun rise and set, casting dramatic shadows on the dunes. Most of the time we just chilled at the pool though and ordered food!</p>
<p><img alt="Sahara Pearl view" loading="lazy" src="/posts/morocco-desert-2025/photos/sahara-pearl.jpg">
<em>View from the hotel pool at the Sahara Pearl hotel, which borders the Erg Chebbi sand dunes of the Sahara.</em></p>
<p>Then finally came our night at the Sahara Stars desert camp, during new moon. After a 30 minute camel ride with 4x4s racing past to move luggage and goods to and from the camp, we arrived at a tent village, with private bathrooms, air conditioning and a restaurant. I have to say that I expected a more intimate setting with a few tents, but our camp was one of many in the area, which didn&rsquo;t make it feel as secluded as I thought, but in exchange the amenities were decent. After the lights went out at 11pm, it was star gazing time, and lying on top of a dune, I watched the beautiful night sky, with artificial lights from human civilisation only visible in the far distance. It felt very magical! I took the photo below with my phone using a 4 minute exposure, leaning against a chair. It turned out not bad, I think!</p>
<video controls width="100%">
  <source src="photos/stars.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<p><em>The magical night sky over the Sahara Desert during new moon - the main reason for our trip.</em></p>
<h2 id="marrakech">Marrakech</h2>
<p>Our last stop was two nights in Marrakech after taking a domestic flight from Errachidia, where we returned our rental car. Upon arrival, the traffic there was out of this world, something I haven&rsquo;t seen before. The taxi app (try <a href="https://indrive.com/">inDrive</a>) told me to wear a seatbelt, but most of the cars simply didn&rsquo;t have any. We stayed in the Medina, the old town, in a Ryad near Bab Aylan. The Medina is full of narrow streets lined with market stalls that try to cram as much stuff in there as possible. Many streets are bustling with tourists, and normally you&rsquo;d think that it&rsquo;s about as packed as it can get. And then there&rsquo;s the motorscooters, the bikes and even taxis that whizz past you, regardless. It does take a while to get used to, but it does seem to work out for people. We visited <a href="https://maps.app.goo.gl/7hmHj7mYMEA268rdA">Madrassa Ben Youssef</a>, a 16th-century Islamic school renowned for its stunning Moroccan-Andalusian architecture, the <a href="https://maps.app.goo.gl/iYWP7monMAQEgkw9A">House of Photography</a> with many portraits from the last century, the <a href="https://maps.app.goo.gl/eyEsVS83WLK4pVfh6">Yves Saint Laurent museum</a>, which was a bit underwhelming, and the spa at the <a href="https://maps.app.goo.gl/JuCNShw1UwVDTbDv7">Royal Mansour hotel</a>, a luxury resort. For eating out I recommend the <a href="https://maps.app.goo.gl/m5QAge9s4Zk3DUYE9">Oban</a> restaurant with great French cuisine and cocktails.</p>
<p><img alt="Marrakech market stalls" loading="lazy" src="/posts/morocco-desert-2025/photos/marrakesh.jpg">
<em>Market stalls in the Medina, next to a busy road.</em></p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>Our trip to the Moroccan desert was something out of the ordinary. It&rsquo;s easily accessible from Europe, the southeast is not touristy, and it&rsquo;s overall reasonably priced. You&rsquo;ll get by everywhere with English/Spanish, as Moroccans know many languages, although I found that you&rsquo;ll get more in-depth conversations with locals if you know French. Some people rent drivers that take them from Marrakech to the desert and up to Fes/Casablanca, but those are 10+h trips in the car. Driving ourselves was no problem at all, as the roads were in great condition. Just be aware that you&rsquo;ll easily get a ticket for speeding, as there is a lot of police present.</p>
<p>Cash is king in the remote areas, so make sure you withdraw enough to pay for restaurants and hotels if you want to avoid a 3-5% card fee. Haggling is part of buying things, especially so in Marrakech.
I found people to be really friendly, and everyone was open for a chat. Although streets are sometimes dimly lit at night, it was always safe. During our trip we got in touch with the locals, we admired the beautiful nature, and especially the scenery of the Sahara. I would recommend it!</p>
]]></content:encoded></item><item><title>SNN library benchmarks</title><link>https://lenzgregor.com/posts/framework-benchmarking/</link><pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/framework-benchmarking/</guid><description>Comparing the most popular SNN frameworks for gradient-based optimization on top of PyTorch.</description><content:encoded>&lt;p>Check out the article on &lt;a href="https://open-neuromorphic.org/p/snn-library-benchmarks/">https://open-neuromorphic.org/p/snn-library-benchmarks/&lt;/a>&lt;/p>
</content:encoded></item><item><title>Efficient compression for event-based data</title><link>https://lenzgregor.com/posts/file-format-benchmarking/</link><pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/file-format-benchmarking/</guid><description>Choosing a good trade-off between disk footprint and file loading times.</description><content:encoded>&lt;p>Check out the article on &lt;a href="https://open-neuromorphic.org/p/efficient-compression-for-event-based-data/">https://open-neuromorphic.org/p/efficient-compression-for-event-based-data/&lt;/a>&lt;/p>
</content:encoded></item><item><title>Training spiking neural networks, fast.</title><link>https://lenzgregor.com/posts/train-snns-fast/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/train-snns-fast/</guid><description>How to use caching and EXODUS to speed up training by a factor of more than 10.</description><content:encoded><![CDATA[<p>When training a spiking neural network (SNN), one might think about how the learning rate or model size affect training time. But when it comes to training <em>faster</em>, optimizing data movement is crucial. 3 out of the first 4 points in <a href="https://www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/">this list</a> weighted after potential speed-up have to do with how data is shaped and moved around between actual computations. It makes a huge difference, because training faster means getting results faster!</p>
<p>For this post we train an SNN on the <a href="https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/">Heidelberg Spiking Speech Commands</a> dataset to do audio stream classification. We&rsquo;ll benchmark different data loading strategies using <a href="https://github.com/neuromorphs/tonic">Tonic</a> and show that with the right strategy, we can achieve a more than 10-fold speed-up compared to the naïve approach.</p>
<p>For all our benchmarks, we already assume multiple worker threads and pinning the host memory. We&rsquo;ll increase throughput by using different forms of caching to disk or GPU. By applying deterministic transformations upfront and saving the new tensor, we can save a lot of time during training.
This tutorial is run on a machine with Ubuntu 20.04, an Intel Core i7-8700K CPU @ 3.70GHz, a Samsung SSD 850 and an NVIDIA GeForce RTX 3080 GPU.</p>
<p>All data from neuromorphic datasets in Tonic is provided as NxD numpy arrays. We&rsquo;ll need to transform this into a dense tensor to serve it to the GPU, and we&rsquo;ll also do some downsampling of time steps. Let&rsquo;s first define the transform. We know that samples of audio input data in this dataset are 0.8-1.2s long across 700 frequency channels at microsecond resolution. We&rsquo;ll <a href="https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.Downsample.html#tonic.transforms.Downsample">downsample</a> each sample to 100 channels, <a href="https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.ToFrame.html#tonic.transforms.ToFrame">bin</a> events every 4 ms to one frame and <a href="https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.CropTime.html#tonic.transforms.CropTime">cut</a> samples that are longer than 1s. That leaves us with a maximum of 250 time steps per sample.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tonic <span style="color:#f92672">import</span> transforms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dt <span style="color:#f92672">=</span> <span style="color:#ae81ff">4000</span>  <span style="color:#75715e"># all time units in Tonic in us</span>
</span></span><span style="display:flex;"><span>encoding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dense_transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>Downsample(spatial_factor<span style="color:#f92672">=</span>encoding_dim <span style="color:#f92672">/</span> <span style="color:#ae81ff">700</span>),
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>CropTime(max<span style="color:#f92672">=</span><span style="color:#ae81ff">1e6</span>),
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>ToFrame(
</span></span><span style="display:flex;"><span>            sensor_size<span style="color:#f92672">=</span>(encoding_dim, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), time_window<span style="color:#f92672">=</span>dt, include_incomplete<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Next we load the training dataset and assign the transform.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tonic <span style="color:#f92672">import</span> datasets
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dense_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>SSC(<span style="color:#e6db74">&#34;./data&#34;</span>, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;train&#34;</span>, transform<span style="color:#f92672">=</span>dense_transform)
</span></span></code></pre></div><p>Let&rsquo;s plot one such dense tensor sample:</p>

<div id="chart-result0" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result0.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result0', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>Next we define a spiking model. We use a simple integrate-and-fire (IAF) feed-forward architecture. For each dataloading method, we&rsquo;re going to test two different models. One is a <a href="https://sinabs.readthedocs.io">Sinabs</a> model which is pretty much pure PyTorch plus for loops and the second one is an <a href="https://github.com/synsense/sinabs-exodus">EXODUS</a> model, which is also based on PyTorch but vectorizes gradient computation for the time dimension using custom CUDA code. Both models compute the same activations and gradients, but the latter provides a significant speedup.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sinabs.layers <span style="color:#66d9ef">as</span> sl
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sinabs.exodus.layers <span style="color:#66d9ef">as</span> el
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SNN</span>(nn<span style="color:#f92672">.</span>Sequential):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, backend, hidden_dim: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> backend <span style="color:#f92672">==</span> sl <span style="color:#f92672">or</span> backend <span style="color:#f92672">==</span> el
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(encoding_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            backend<span style="color:#f92672">.</span>IAF(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            backend<span style="color:#f92672">.</span>IAF(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            backend<span style="color:#f92672">.</span>IAF(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, <span style="color:#ae81ff">35</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sinabs_model <span style="color:#f92672">=</span> SNN(backend<span style="color:#f92672">=</span>sl)<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>exodus_model <span style="color:#f92672">=</span> SNN(backend<span style="color:#f92672">=</span>el)<span style="color:#f92672">.</span>cuda()
</span></span></code></pre></div><h2 id="1-naïve-dataloading">1. Naïve dataloading</h2>
<p>For the first benchmark we test the most common setup without any caching. We load every sample from an hdf5 file on disk which provides us with a numpy array in memory. For each sample, we apply our <code>dense_transform</code> defined earlier to create a dense tensor which we can then batch together with other samples and feed it to the network.</p>
<figure>
  <img
  src="images/caching1.svg"
  alt="Naïve caching">
  <figcaption>Figure 1: For every sample, we apply our transform ToFrame. The speed depends a lot on the CPU and the amount of worker threads used.</figcaption>
</figure>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> sinabs
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> timeit
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tonic
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataloader_kwargs <span style="color:#f92672">=</span> dict(
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>    shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    drop_last<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    pin_memory<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    collate_fn<span style="color:#f92672">=</span>tonic<span style="color:#f92672">.</span>collation<span style="color:#f92672">.</span>PadTensors(batch_first<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    num_workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>naive_dataloader <span style="color:#f92672">=</span> DataLoader(dense_dataset, <span style="color:#f92672">**</span>dataloader_kwargs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_loop</span>(dataloader, model):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> data, targets <span style="color:#f92672">in</span> iter(dataloader):
</span></span><span style="display:flex;"><span>        data, targets <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>cuda(), targets<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>        sinabs<span style="color:#f92672">.</span>reset_states(model)
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>cross_entropy(output<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>), targets)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> timeit <span style="color:#f92672">import</span> timeit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>time1 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(naive_dataloader, sinabs_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>time2 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(naive_dataloader, exodus_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div>
<div id="chart-result1" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result1.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result1', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>The Sinabs model takes more than two minutes per epoch using the simple strategy, which is far from exciting. By contrast, we can already see the huge speedup that EXODUS provides, reducing epoch time by a third! These results are our baseline with the basic dataloading.</p>
<h2 id="disk-caching">Disk caching</h2>
<p>Let&rsquo;s try to be a bit smarter now. <code>ToFrame</code> is a deterministic transform, so for the same sample we&rsquo;ll always receive the same transformed data. Given that we might train for 100 epochs, which looks at each sample 100 times, that&rsquo;s a lot of wasted compute! Now we&rsquo;re going to cache, which means save, those transformed samples to disk during the first epoch, so that we don&rsquo;t need to recompute them later on! To do this we simply wrap our previous dataset in a <a href="https://tonic.readthedocs.io/en/latest/reference/data_classes.html#tonic.DiskCachedDataset">DiskCachedDataset</a> and provide the cache path. When a new sample is about to be loaded, that class will first check if the transformed sample is already in the cache on disk and if it isn&rsquo;t, it will retrieve the original sample, apply the transform, cache it to disk and then serve it. This caching process slows down training in the first epoch, but it pays off afterwards!</p>
<figure>
  <img
  src="images/caching2.svg"
  alt="Disk caching">
  <figcaption>Figure 2: During the first epoch, samples are transformed and then cached to disk. Afterwards, the transformed sample is loaded from disk straight away.</figcaption>
</figure>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>disk_cached_dataset <span style="color:#f92672">=</span> tonic<span style="color:#f92672">.</span>DiskCachedDataset(
</span></span><span style="display:flex;"><span>    dataset<span style="color:#f92672">=</span>dense_dataset,
</span></span><span style="display:flex;"><span>    cache_path<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;cache/</span><span style="color:#e6db74">{</span>dense_dataset<span style="color:#f92672">.</span>__class__<span style="color:#f92672">.</span>__name__<span style="color:#e6db74">}</span><span style="color:#e6db74">/train/</span><span style="color:#e6db74">{</span>encoding_dim<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>dt<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>disk_cached_dataloader <span style="color:#f92672">=</span> DataLoader(disk_cached_dataset, <span style="color:#f92672">**</span>dataloader_kwargs)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># cache on disk already available</span>
</span></span><span style="display:flex;"><span>time3 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(disk_cached_dataloader, sinabs_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>time4 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(disk_cached_dataloader, exodus_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div>
<div id="chart-result2" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result2.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result2', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>We brought down epoch training time to 14s for the EXODUS model by not having to recompute the <code>ToFrame</code> transform! The speedup comes at the expense of disk space. How much disk space does it cost you may ask? The size of the original dataset file is 2.65 GB compared to the generated cache folder of 1.04 GB, which is not too bad!</p>
<p>The original dataset contained numpy events, whereas the cache folder contains dense tensors. We can compress the dense tensors that much because by default Tonic uses lightweight compression during caching. Disk-caching is generally applicable when training SNNs because it saves you the time to transform your events to dense tensors. Of course you could apply any other deterministic transform before caching it, and also easily apply augmentations to the cached samples as described in <a href="https://tonic.readthedocs.io/en/latest/tutorials/fast_dataloading.html">this tutorial</a>!</p>
<p>Now we notice one more thing. Overall GPU utilisation rate at this point is at ~80%, which means that the GPU is still idling the rest of the time, waiting for new data to arrive. So we can try to go even faster!</p>
<h2 id="gpu-caching">GPU caching</h2>
<p>Instead of loading dense tensors from disk, we can try to cram all our dataset onto the GPU! The issue is that with dense tensors this wouldn&rsquo;t work as they would occupy too much memory. But events are already an efficient format right? So we&rsquo;ll store the events on the GPU as sparse tensors and then simply inflate them as needed by calling to_dense() for each sample. This method is obviously bound by GPU memory so works with rather small datasets such as the one we&rsquo;re testing. However, once you&rsquo;re setup, you can train with <em>blazing</em> speed. For GPU caching we are going to:</p>
<ol>
<li>Create a new sparse dataset on the fly by loading them from the disk cache and calling to_sparse() on the transformed tensors.</li>
<li>Create a new dataloader that now uses a single thread.</li>
<li>Inflate sparse tensors to dense versions by calling to_dense() in the training loop.</li>
</ol>
<figure>
  <img
  src="images/caching3.svg"
  alt="Disk caching">
  <figcaption>Figure 3: During the first epoch, transformed samples are loaded onto the GPU and stored in a list of sparse tensors. Whenever a new sample is needed, it is inflated by to_dense() and fed to the network. This process is almost instantaneous and now bound by what your model can process.</figcaption>
</figure>
<p>The sparse tensor dataset takes about 5.7 GB of GPU memory. Not very efficient, but also not terrible. What about training speeds?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gpu_training_loop</span>(model):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> data, targets <span style="color:#f92672">in</span> iter(sparse_tensor_dataloader):
</span></span><span style="display:flex;"><span>        data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>to_dense()
</span></span><span style="display:flex;"><span>        sinabs<span style="color:#f92672">.</span>reset_states(model)
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>cross_entropy(output<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>), targets)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>time5 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: gpu_training_loop(sinabs_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>time6 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: gpu_training_loop(exodus_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div>
<div id="chart-result3" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result3.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result3', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>We&rsquo;re down to less than 9s per epoch for the EXODUS model, which is another 40% less than disk-caching and an almost 15-fold improvement over the original Sinabs model using the naïve dataloading approach! Now we&rsquo;re really exploiting the GPU as much as possible with a utilisation percentage of ~99%. All this without any qualitative impact on gradient computation.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The principle of caching can be applied to any data that you apply deterministic transformations to, but it pays off particularly well for event-based data. By using cached samples and not having to recompute the same transformations every time, we save ourselves a lot of time during training. If the data already sits on the GPU when it is requested, the speedup is really high. After all, there is a reason why neural network accelerators heavily optimise memory caching to minimise time and energy spent on data movement. So when should you use either disk- or GPU-caching?</p>
<ul>
<li><strong>Disk-caching</strong>: Broadly applicable, useful if you apply deterministic transformations to each sample and you train for many epochs. Not ideal if you&rsquo;re low on disk space.</li>
<li><strong>GPU-caching</strong>: Only really suitable for small datasets and a bit more intricate to setup, but well worth the effort if you want to explore many different architectures / training parameters due to the speed of iteration.</li>
</ul>
<p>As a last note, you might be wondering why we don&rsquo;t cache to the host memory instead of reading from a disk cache. This is totally possible, but the bottleneck at that point really is moving the data onto the GPU, which takes time. Whether the data sits in host memory or is loaded from disk using multiple worker threads doesn&rsquo;t make much of a difference, because the GPU cannot handle the data movement. Since on disk we have much more space available than in RAM, we normally choose to do that.</p>
<p>This tutorial is available <a href="https://github.com/biphasic/snn-training-templates/blob/main/posts/training-snns-faster/index.ipynb">here</a> for you to run, where you&rsquo;ll also find some other training templates.</p>
<p>Acknowledgements: Thanks a lot to Omar Oubari, Mina Khoei and Fabrizio Ottati for the feedback.</p>
]]></content:encoded></item><item><title>Rethinking the way our cameras see.</title><link>https://lenzgregor.com/posts/event-cameras/</link><pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/event-cameras/</guid><description>Neuromorphic vision takes inspiration from the biological vision system.</description><content:encoded><![CDATA[<p>We use them every day and take them for granted: cameras in all their endless shapes and forms. The field of modern computer vision, the ability of machines to see, is based on the common output format of those sensors: frames. However, the way we humans perceive the world with our eyes is very different. Most importantly, we do it with a fraction of the energy needed by a conventional camera. The field of neuromorphic vision tries to understand how our visual system processes information, in order to give modern cameras that same efficiency and it looks like a substantial shift in technology. But let&rsquo;s not get ahead of ourselves.</p>
<h2 id="conventional-imaging-technology">Conventional imaging technology</h2>
<p>We are so focused on working with data that modern cameras provide, that little thought is given about how to capture a scene more efficiently in the first place. Current cameras acquire frames by reading the brightness value of all pixels at the same time at a fixed time interval, the frame rate, regardless of whether the recorded information has actually changed. A single frame acts as a photo; as soon as we stack multiple of them per second it becomes a motion picture. So far so good. This synchronous mechanism makes acquisition and processing predictable. But it comes with a price, namely the recording of redundant data. And not too little of it!</p>
<p><img alt="events" loading="lazy" src="/posts/event-cameras/post-rethinking/frames.gif">
<em>Image blur can occur in a frame depending on the exposure time.</em></p>
<h2 id="our-visual-system">Our visual system</h2>
<p>The human retina has developed to encode information extremely efficiently. Narrowing down the stimuli of about 125 million photoreceptors, which are sensitive to light to just 1 million ganglion cells which relay information to the brain, the retina compresses a visual scene into its most essential parts.
<img alt="events" loading="lazy" src="/posts/event-cameras/post-rethinking/receptive-fields.png">
<em>Center surround receptive fields in the mammalian retina, <a href="https://upload.wikimedia.org/wikipedia/commons/1/16/Receptive_field.png">source</a></em></p>
<p>Photoreceptors are bundled into receptive fields of different sizes for each retinal ganglion cell. The way a receptive field is organised into center and surround cells allows ganglion cells to transmit information not merely about whether photoreceptor cells are exposed to light, but also about the differences in firing rates of cells in the center and surround. This allows them to transmit information about spatial contrast. They are furthermore capable of firing independently of other ganglion cells, thus decoupling the activity of receptive fields from each other. Even if not triggered, a retinal ganglion cell will have a spontaneous firing rate, resulting in millions of spikes per second that travel along the optic nerve. It is thought that in order to prevent the retinal image from fading and thus be able to see the non-moving objects, our eyes perform unintentional rapid ballistic jumps called micro-saccades. This movement only happens once or twice per second, so in between micro-saccades, our vision system probably relies on motion. To put it in a nutshell, our retina acts as a pre-processor for our visual system, extracting contrast as the most important information that then travels along the optical nerve to the visual cortex. In the cortex it is processed for higher-level image synthesis such as depth and motion perception.</p>
<h2 id="taking-inspiration-from-nature">Taking inspiration from nature</h2>
<p>Towards the end of the 80s, a scientist at Caltech named Carver Mead spawned the field of Neuromorphic Engineering, when one of his students called Misha Mahowald developed a new stereo vision system. Taking inspiration from the human visual system, she built what would become the first silicon retina in the early 90s. It was based on the same principle of center surround receptive fields in the human retina, that emit spikes independently of each other depending on the contrast pattern observed.</p>
<p><img alt="misha" loading="lazy" src="/posts/event-cameras/post-rethinking/misha.jpg">
<em>Misha Mahowald (circa 1992) in the ‘Carverland’ lab at Caltech, testing her stereocorrespondence chip. Photo credit: Rodney Douglas.</em></p>
<p>Although Misha drafted the beginning of a new imaging sensor, it did not provide a practical implementation at first. In response, the neuromorphic community simplified the problem by dropping the principle of center-surround pixels. Instead of encoding spatial contrast across multiple pixels which needed sophisticated circuits, the problem could be alleviated by realising a circuit that could encode temporal contrast for single pixels. That way, pixels could still operate individually as processing units just as receptive fields in the retina do and report any deviations in illuminance over time. It would take until 2001 when Tetsuya Yagi at Osaka University and Tobi Delbrück at UZH/ETH in 2008 publish about the first refined temporal contrast sensors, the event cameras as they are known today.</p>
<h2 id="paradigm-shift">Paradigm Shift</h2>
<p>Standard cameras capture absolute illuminance at the same time for all pixels driven by a clock and encoded as frames. One fundamental approach to dealing with temporal redundancy in classical videos is frame difference encoding. This simplest form of video compression includes transmitting only pixel values that exceed a defined intensity change threshold from frame to frame after an initial key-frame. Frame differencing is naturally performed in post-processing, when the data has already been recorded.</p>
<p>Trying to take inspiration from the way our eyes encode information, neuro-morphic cameras capture changes in illuminance over time for individual pixels corresponding to one retinal ganglion cell and its receptive field.</p>
<p><img alt="log-pixel-illuminance" loading="lazy" src="/posts/event-cameras/post-rethinking/log-pixel-illuminance.png">
<em>Principle of how ON and OFF events are generated for each pixel.</em></p>
<p>If light increases or decreases by a certain percentage, one pixel will trigger what&rsquo;s called an event, which is the technical equivalent of a cell&rsquo;s action potential. One event will have a timestamp, x/y coordinates and a polarity depending on the sign of the change. Pixels can fire completely independently of each other, resulting in an overall firing rate that is directly driven by the activity of the scene. It also means that if nothing moves in front of a static camera, no new information is available hence no pixels fire apart from some noise. The absence of accurate measurements of absolute lighting information is a direct result of recording change information. This information can be refreshed by moving the camera itself, much like a microsaccade.</p>
<p>So how do we now get an image from this camera? The short answer is: we don’t. Although we can of course add together all the events per pixel to get an idea of how much the brightness changed (‘binning’), in reality this will not be a reliable estimate, as the electronics of the camera will cause a bit of background noise. As such, the error of your estimate only grows over time.</p>
<p><img alt="events" loading="lazy" src="/posts/event-cameras/post-rethinking/events.gif">
<em>An event-camera will only record change in brightness and encode it as events in x, y and time. Colour is artificial in this visualisation. Note the fine-grained resolution on the t-axis in comparison with the frame animation earlier.</em></p>
<p>Overall an event-camera has three major advantages: Since pixel exposure times are decoupled of each other, very bright and very dark parts can be captured at the same time, resulting in a dynamic range of up to 125dB. In autonomous vehicles, where the lighting can change very quickly or exposure of a single bright spot such as the sun or a reflection should not interfere with the rest, this can save lives. The decoupled, asynchronous nature furthermore frees bandwidth so that changes for one pixel can be recorded at a temporal resolution and latency of microseconds. This makes it possible to track objects with very high speed and without blur. The third advantange is low power consumption due to the sparse output of events, which makes the camera suitable for mobile and embedded applications. Remember that when nothing in front of the camera moves, no redundant data is recorded by the sensor which reduces computational load overall. It also relieves the need for huge raw data files. Current drawbacks for most commercially event-cameras available today are actually further downstream, namely the lack of hardware and algorithms that properly exploit the sparse nature of an event-camera&rsquo;s data. Rethinking even the most basic computer vision algorithms without frames takes a considerable effort. I published some work about purely event-based face detection, have a <a href="https://www.youtube.com/watch?v=F5UzXQsr5Es">look at the video</a>!</p>
<p><img alt="face-detection" loading="lazy" src="/posts/event-cameras/post-rethinking/face-detection.jpeg">
<em>Some snapshots of my work on face detection with event cameras that relies on eye blinks.</em></p>
<h2 id="be-on-the-lookout">Be on the lookout</h2>
<p>So why are these sensors becoming interesting just now? Humanity has learnt to build powerful synchronous hardware such as GPUs that enable high performance, high throughput computing. They provide the power necessary to work with dense image information that are frames. But we’re only ever veering away from the efficiency of a biological system in terms of information processing. Understanding the biological principles of a complex system such as human vision will therefore help create artificial sensors that resemble their biological equivalents. This bears the potential of a low-power sensor with fine-grained temporal resolution.</p>
<p>Since their original inception a few decades ago, it has been quite a journey. Labs at <a href="https://www.ini.uzh.ch/">Zurich</a>, <a href="https://www.westernsydney.edu.au/icns">Sydney</a>, <a href="https://www.edpr.iit.it/">Genua</a>, <a href="http://neuromorphic-vision.com/">my lab in Paris</a>, <a href="https://www.grasp.upenn.edu/">Pittsburgh</a>, <a href="http://sinapse.nus.edu.sg/">Singapore</a> and many more are exploring the concept of event-based computation. Start-up companies such as <a href="https://www.prophesee.ai/">Prophesee</a> or <a href="https://www.celepixel.com/#/Home">Celex</a> compete with established players such as Samsung and Sony in the race to find promising applications for this interesting imaging technology. Potential industry candidates include the automotive industry, neural interfaces, space applications, autonomous agents, … you name it!</p>
<p>If you want to know more about algorithms for event-based cameras, I recommend this <a href="https://arxiv.org/pdf/1904.08405.pdf">survey paper</a> for you. Stay tuned for more articles about neuromorphic engineering on this space! You can also reach out to me via <a href="https://twitter.com/gregorlenz">Twitter</a> or check out some code on <a href="https://github.com/biphasic">Github</a>.</p>
<p>Last but not least I want to thank my former colleague Alexandre Marcireau who made the event visualisations for this article possible with <a href="https://github.com/neuromorphic-paris/command_line_tools">rainmaker</a>!</p>
<p><img alt="omar" loading="lazy" src="/posts/event-cameras/post-rethinking/omar.gif">
<em>My friend Omar causes the camera to heat up.</em></p>
]]></content:encoded></item><item><title>Why digital privacy matters.</title><link>https://lenzgregor.com/posts/digital-privacy/</link><pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/digital-privacy/</guid><description>Reclaim your online identity.</description><content:encoded><![CDATA[<p>Over the past years I started to take back the control of my data online. I deleted a bunch of accounts, some of which I replaced with an open source service. Although I had some doubts as to whether I would seriously miss some of the &lsquo;free&rsquo; services out there, I have to say that it has been a smooth and very rewarding ride so far.
I strongly believe that privacy is a key element for a functioning and free society and should therefore be protected. In the information age that we are in now, this concept has become somewhat opaque, while we citizens have become transparent. So let&rsquo;s have a look at why ditigal privacy matters.</p>
<h2 id="why-even-bother">Why even bother?</h2>
<p>Do I really take any harm if a private company or a government knows what I like and who I talk to? After all it&rsquo;s convenient, right? I think there are multiple aspects to those questions.</p>
<p>First of all it should be clear that being careless about the traces left online can have direct consequences. Ads become tailored to what you like and talk about. Algorithms have become <em>really</em> good at analysing you because they are fed a lot of information about you. Ever saw an ad pop up about something that you just talked about with your friend? That&rsquo;s <a href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/smartphone-apps-listening-privacy-alphonso-shazam-advertising-pool-3d-honey-quest-a8139451.html">no coincidence</a>. You might think, well, I&rsquo;m not influenced by ads. I&rsquo;m afraid a global <a href="https://www.statista.com/topics/990/global-advertising-market/">560 billion US Dollar</a> industry does make sure to get their returns on what they invest in. Everyone is affected by ads. Whether that is a bad idea might be debatable, but I&rsquo;d really rather have the choice.</p>
<p>Online tracking in general is flourishing. Data points collected about you now span multiple devices. If you are someone who earns a decent salary, you could end up <a href="https://www.csmonitor.com/Business/Saving-Money/2016/0405/How-retailers-use-dynamic-pricing-to-get-you-to-pay-more">paying more</a> for the same thing online. You might be <a href="https://www.theguardian.com/uk-news/cambridge-analytica">nudged to vote for a certain candidate</a> in office. And almost certainly your <a href="https://choosetoencrypt.com/search-engines/filter-bubbles-searchencrypt-com-avoids/">newsfeeds are altered accordingly</a>.
These things are not new. And some of them are very convenient. But they happen without the consent of the user, without them being able <a href="https://www.bbc.co.uk/news/technology-44640959">to control it</a>. And this is when we should pay attention I think.</p>
<p>Also private and public bodies should be liable to safeguard the data that they store about you. Nevertheless <a href="https://www.itgovernance.co.uk/blog/list-of-data-breaches-and-cyber-attacks-in-june-2020">June 2020 alone</a> has seen at least 92 security incidents and at least 7,021,195,399 breached records. Although <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a> has improved user rights a lot, it turns out that many institutions are incapable or simply unwilling to put the necessary protection in place. I am willing to believe that the <a href="https://www.theguardian.com/technology/2020/jul/14/huawei-to-be-stripped-of-role-in-uk-5g-network-by-2027-dowden-confirms">recent decision</a> to ban Huawei&rsquo;s 5G equipment in the entire UK was motivated by some legit concerns, however it&rsquo;s really only being stuck between a rock and a hard place when it comes to which country collects your mobile phone locations.</p>
<p>Then there are indirect consequences. People reportedly behave differently when they know that they are being monitored. It might just be that one joke that you decide not to tell on the phone or you might decide not to search for that term because you are going to take a flight to the US the next morning. But our words are subtly altered and our demeanor has changed. These small gestures go a long way and eventually people give up a bit of their freedom and get used to it.</p>
<p>The simple truth is: you just haven&rsquo;t realised your own worth online yet. Let me explain. You might be happy that a service such as facebook or gmail is free to use, but you&rsquo;re providing those companies with organic data that has essentially become a currency itself, or as some call it: <em>the new oil</em>. Companies are making huge profits from it, so why not claim a piece of the pie? I think at some point we will be able to set a price for the data that we&rsquo;re willing to share rather than giving it away for free. There are already <a href="https://datacoup.com/">companies</a> building business models on this.
As Andrew Trask, the clever mind behind <a href="http://openmined.org/">OpenMined</a>, puts it: We&rsquo;re <a href="https://www.youtube.com/watch?v=4zrU54VIK6k&amp;feature=youtu.be&amp;t=2500">spilling our data</a> everywhere, much like people 200 years ago disposed of their waste in public. Was that healthy? Surely not.</p>
<h2 id="is-there-a-way-out">Is there a way out?</h2>
<p>Once a measure of data collection or surveillance is in place, it is hard to get rid of it. That&rsquo;s why it&rsquo;s necessary to act early on. We should be able to explore parts of life privately and without embarrassment or threat.
Bear in mind that it&rsquo;s never too late to start taking ownership of your data as companies are interested in recent trends. If you replaced a service right now it might not affect how you are targeted immediately, but eventually your information will become out of date.
As to things everyone of us can do to reduce their digital footprint, let me say this:</p>
<ul>
<li>
<p>Use <a href="https://www.mozilla.org/en-US/firefox/new/">Firefox</a> or <a href="https://www.torproject.org/">Tor Browser</a>. They come with privacy features built in.
At the very least, check privacy settings and use an adblocker. There are <a href="https://privacybadger.org/">other plugins</a> available for major browsers that I&rsquo;ve found personally useful. If you want to know how trackable your browser is, have a look <a href="https://panopticlick.eff.org/">at this site</a>.</p>
</li>
<li>
<p>If you make sensitive searches related to your health, <a href="https://jezebel.com/what-happens-when-you-tell-the-internet-youre-pregnant-1794398989">pregnancy</a>, your financial situation or anything else you might prefer others not to know, then really use <a href="https://www.torproject.org/">Tor browser</a>.</p>
</li>
<li>
<p>Remember that your e-mail address acts as your unique identifier, which is the easiest way to tie together your data across different services. <a href="https://temp-mail.org/en/">Temporary email services</a> save you from spam when you have to create an account on a site you don&rsquo;t trust.</p>
</li>
<li>
<p>Please say goodbye to Alexa and co. It baffles me that 1 out of 5 households in the UK has a smart speaker at home. One might think that Orwell&rsquo;s 1984 would remind people of the <a href="https://en.wikipedia.org/wiki/Telescreen">Telescreens</a> in everyone&rsquo;s home. If you&rsquo;re not convinced then you might want to check your record of <a href="https://www.theverge.com/2018/5/28/17402154/amazon-echo-alexa-conversation-recording-history-listen-how-to">every conversation recorded</a> in your home by Alexa.</p>
</li>
<li>
<p>Keep your accounts safe. Use two-factor authentication and <a href="https://www.useapassphrase.com/">passphrases</a> rather than passwords. Even better is a reliable <a href="https://www.wired.com/story/best-password-managers/">password manager</a>.</p>
</li>
<li>
<p>Consider saying goodbye to facebook. Maybe <a href="https://en.wikipedia.org/wiki/Criticism_of_Facebook">this wiki page</a> with 556 references at the time of writing can convince you.</p>
</li>
</ul>
<p>How do you know that it works? If you start seeing ads that are really not relevant to you, you are on the right track. You can always check if your email has been part of a known <a href="https://monitor.firefox.com/">data breach</a>.</p>
<h2 id="digital-privacy-in-the-21st-century">Digital privacy in the 21st century</h2>
<p>Lawmakers are slowly moving to empower citizens. The US senate recently brought forward a bill to <a href="https://paleofuture.gizmodo.com/lets-kill-facial-recognition-for-good-1844168367">stop facial recognition</a> within law enforcement completely. The Court of Justice of the EU has <a href="https://noyb.eu/en/cjeu">recently invalidated</a> certain data transfer policies between the EU and the US, urging the US to reform their surveillance laws if they want to continue exchanging data with countries overseas. Public policy develops if people want it to.
In the end a large part of the web is based on trust, and companies will not always stop collecting your data <a href="https://www.businessinsider.com/google-lawsuit-app-tracking-without-permission-reuters-2020-7">even if you opt out</a> of their programs.</p>
<p>People say, <em>If you don&rsquo;t do anything wrong, you have nothing to hide</em>. The problem with this statement is that the definition of &lsquo;wrong&rsquo; can change over time. And when a lot of your online activity is on a <a href="https://www.goodreads.com/book/show/46223297-permanent-record">permanent record</a>, it might be disadvantageous for you under a new administration. And that is not to say that your records <a href="https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections">cannot be altered</a>.</p>
<p>No matter how securely a single entity might safeguard its users&rsquo; data, it still remains a single point of failure. No system is ultimately safe. That&rsquo;s why decentralization will play a key role in the near future. It surely is one hell of an engineering job to create systems that have their components located in different locations of the network, ensure distributed consensus and at the same time keep identities private, but blockchain technology shows that it&rsquo;s possible.</p>
<p>Digital privacy matters because automated collection and processing of personal data is possible like never before, in ways that ad companies and intelligence services in the analog days could have only dreamed of.
In the end it boils down to a trade-off between freedom and security. Do tailored ads and messenger backdoors limit your freedom? Certainly. Will a Big Brother government ensure security for its well-behaving citizens? <a href="https://time.com/collection/davos-2019/5502592/china-social-credit-score/">Certainly</a>. We just have to be careful not to lose both in the end.</p>
<p>One last thing: Think for a second where technological advancements have led us. Integrated circuit technology allowed devices to shrink from a room-sized to a palm-sized computer. The rate of progress is not going to slow down! In my opinion, we are already very much dependent on that palm-sized device. Think about what it means if we physically connect to the internet. I am convinced that we will see brain machine interfaces become a routine thing within the next decades. We will be able to directly transmit our thoughts to each other, not needing thumbs to interact with a messenger app via a screen. If we do not develop a good understanding of privacy-protecting policies in the digital world, how will you then be able to say that the thoughts you have are really yours? Tell that to Orwell for a start.</p>
<p><a href="https://judiciary.house.gov/news/documentsingle.aspx?DocumentID=3114">https://judiciary.house.gov/news/documentsingle.aspx?DocumentID=3114</a></p>
<p><a href="https://support.brave.com/hc/en-us/articles/360026361072-Brave-Ads-FAQ">https://support.brave.com/hc/en-us/articles/360026361072-Brave-Ads-FAQ</a></p>
]]></content:encoded></item></channel></rss>