<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Posts on Gregor's Blog</title><link>https://lenzgregor.com/posts/</link><description>Recent content in Posts on Gregor's Blog</description><generator>Hugo -- 0.147.8</generator><language>en-gb</language><atom:link href="https://lenzgregor.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>SNN library benchmarks</title><link>https://lenzgregor.com/posts/framework-benchmarking/</link><pubDate>Wed, 02 Aug 2023 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/framework-benchmarking/</guid><description>Comparing the most popular SNN frameworks for gradient-based optimization on top of PyTorch.</description><content:encoded>&lt;p>Check out the article on &lt;a href="https://open-neuromorphic.org/p/snn-library-benchmarks/">https://open-neuromorphic.org/p/snn-library-benchmarks/&lt;/a>&lt;/p>
</content:encoded></item><item><title>Efficient compression for event-based data</title><link>https://lenzgregor.com/posts/file-format-benchmarking/</link><pubDate>Tue, 28 Feb 2023 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/file-format-benchmarking/</guid><description>Choosing a good trade-off between disk footprint and file loading times.</description><content:encoded>&lt;p>Check out the article on &lt;a href="https://open-neuromorphic.org/p/efficient-compression-for-event-based-data/">https://open-neuromorphic.org/p/efficient-compression-for-event-based-data/&lt;/a>&lt;/p>
</content:encoded></item><item><title>Training spiking neural networks, fast.</title><link>https://lenzgregor.com/posts/train-snns-fast/</link><pubDate>Sun, 27 Nov 2022 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/train-snns-fast/</guid><description>How to use caching and EXODUS to speed up training by a factor of more than 10.</description><content:encoded><![CDATA[<p>When training a spiking neural network (SNN), one might think about how the learning rate or model size affect training time. But when it comes to training <em>faster</em>, optimizing data movement is crucial. 3 out of the first 4 points in <a href="https://www.reddit.com/r/MachineLearning/comments/kvs1ex/d_here_are_17_ways_of_making_pytorch_training/">this list</a> weighted after potential speed-up have to do with how data is shaped and moved around between actual computations. It makes a huge difference, because training faster means getting results faster!</p>
<p>For this post we train an SNN on the <a href="https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/">Heidelberg Spiking Speech Commands</a> dataset to do audio stream classification. We&rsquo;ll benchmark different data loading strategies using <a href="https://github.com/neuromorphs/tonic">Tonic</a> and show that with the right strategy, we can achieve a more than 10-fold speed-up compared to the naïve approach.</p>
<p>For all our benchmarks, we already assume multiple worker threads and pinning the host memory. We&rsquo;ll increase throughput by using different forms of caching to disk or GPU. By applying deterministic transformations upfront and saving the new tensor, we can save a lot of time during training.
This tutorial is run on a machine with Ubuntu 20.04, an Intel Core i7-8700K CPU @ 3.70GHz, a Samsung SSD 850 and an NVIDIA GeForce RTX 3080 GPU.</p>
<p>All data from neuromorphic datasets in Tonic is provided as NxD numpy arrays. We&rsquo;ll need to transform this into a dense tensor to serve it to the GPU, and we&rsquo;ll also do some downsampling of time steps. Let&rsquo;s first define the transform. We know that samples of audio input data in this dataset are 0.8-1.2s long across 700 frequency channels at microsecond resolution. We&rsquo;ll <a href="https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.Downsample.html#tonic.transforms.Downsample">downsample</a> each sample to 100 channels, <a href="https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.ToFrame.html#tonic.transforms.ToFrame">bin</a> events every 4 ms to one frame and <a href="https://tonic.readthedocs.io/en/latest/reference/generated/tonic.transforms.CropTime.html#tonic.transforms.CropTime">cut</a> samples that are longer than 1s. That leaves us with a maximum of 250 time steps per sample.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tonic <span style="color:#f92672">import</span> transforms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dt <span style="color:#f92672">=</span> <span style="color:#ae81ff">4000</span>  <span style="color:#75715e"># all time units in Tonic in us</span>
</span></span><span style="display:flex;"><span>encoding_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dense_transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>Downsample(spatial_factor<span style="color:#f92672">=</span>encoding_dim <span style="color:#f92672">/</span> <span style="color:#ae81ff">700</span>),
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>CropTime(max<span style="color:#f92672">=</span><span style="color:#ae81ff">1e6</span>),
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>ToFrame(
</span></span><span style="display:flex;"><span>            sensor_size<span style="color:#f92672">=</span>(encoding_dim, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), time_window<span style="color:#f92672">=</span>dt, include_incomplete<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        ),
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Next we load the training dataset and assign the transform.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tonic <span style="color:#f92672">import</span> datasets
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dense_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>SSC(<span style="color:#e6db74">&#34;./data&#34;</span>, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;train&#34;</span>, transform<span style="color:#f92672">=</span>dense_transform)
</span></span></code></pre></div><p>Let&rsquo;s plot one such dense tensor sample:</p>

<div id="chart-result0" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result0.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result0', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>Next we define a spiking model. We use a simple integrate-and-fire (IAF) feed-forward architecture. For each dataloading method, we&rsquo;re going to test two different models. One is a <a href="https://sinabs.readthedocs.io">Sinabs</a> model which is pretty much pure PyTorch plus for loops and the second one is an <a href="https://github.com/synsense/sinabs-exodus">EXODUS</a> model, which is also based on PyTorch but vectorizes gradient computation for the time dimension using custom CUDA code. Both models compute the same activations and gradients, but the latter provides a significant speedup.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sinabs.layers <span style="color:#66d9ef">as</span> sl
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sinabs.exodus.layers <span style="color:#66d9ef">as</span> el
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SNN</span>(nn<span style="color:#f92672">.</span>Sequential):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, backend, hidden_dim: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> backend <span style="color:#f92672">==</span> sl <span style="color:#f92672">or</span> backend <span style="color:#f92672">==</span> el
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(encoding_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            backend<span style="color:#f92672">.</span>IAF(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            backend<span style="color:#f92672">.</span>IAF(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            backend<span style="color:#f92672">.</span>IAF(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, <span style="color:#ae81ff">35</span>),
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sinabs_model <span style="color:#f92672">=</span> SNN(backend<span style="color:#f92672">=</span>sl)<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>exodus_model <span style="color:#f92672">=</span> SNN(backend<span style="color:#f92672">=</span>el)<span style="color:#f92672">.</span>cuda()
</span></span></code></pre></div><h2 id="1-naïve-dataloading">1. Naïve dataloading</h2>
<p>For the first benchmark we test the most common setup without any caching. We load every sample from an hdf5 file on disk which provides us with a numpy array in memory. For each sample, we apply our <code>dense_transform</code> defined earlier to create a dense tensor which we can then batch together with other samples and feed it to the network.</p>
<!-- raw HTML omitted -->
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> sinabs
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> timeit
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tonic
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataloader_kwargs <span style="color:#f92672">=</span> dict(
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>,
</span></span><span style="display:flex;"><span>    shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    drop_last<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    pin_memory<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    collate_fn<span style="color:#f92672">=</span>tonic<span style="color:#f92672">.</span>collation<span style="color:#f92672">.</span>PadTensors(batch_first<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>    num_workers<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>naive_dataloader <span style="color:#f92672">=</span> DataLoader(dense_dataset, <span style="color:#f92672">**</span>dataloader_kwargs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">training_loop</span>(dataloader, model):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> data, targets <span style="color:#f92672">in</span> iter(dataloader):
</span></span><span style="display:flex;"><span>        data, targets <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>cuda(), targets<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>        sinabs<span style="color:#f92672">.</span>reset_states(model)
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>cross_entropy(output<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>), targets)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> timeit <span style="color:#f92672">import</span> timeit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>time1 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(naive_dataloader, sinabs_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>time2 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(naive_dataloader, exodus_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div>
<div id="chart-result1" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result1.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result1', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>The Sinabs model takes more than two minutes per epoch using the simple strategy, which is far from exciting. By contrast, we can already see the huge speedup that EXODUS provides, reducing epoch time by a third! These results are our baseline with the basic dataloading.</p>
<h2 id="disk-caching">Disk caching</h2>
<p>Let&rsquo;s try to be a bit smarter now. <code>ToFrame</code> is a deterministic transform, so for the same sample we&rsquo;ll always receive the same transformed data. Given that we might train for 100 epochs, which looks at each sample 100 times, that&rsquo;s a lot of wasted compute! Now we&rsquo;re going to cache, which means save, those transformed samples to disk during the first epoch, so that we don&rsquo;t need to recompute them later on! To do this we simply wrap our previous dataset in a <a href="https://tonic.readthedocs.io/en/latest/reference/data_classes.html#tonic.DiskCachedDataset">DiskCachedDataset</a> and provide the cache path. When a new sample is about to be loaded, that class will first check if the transformed sample is already in the cache on disk and if it isn&rsquo;t, it will retrieve the original sample, apply the transform, cache it to disk and then serve it. This caching process slows down training in the first epoch, but it pays off afterwards!</p>
<!-- raw HTML omitted -->
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>disk_cached_dataset <span style="color:#f92672">=</span> tonic<span style="color:#f92672">.</span>DiskCachedDataset(
</span></span><span style="display:flex;"><span>    dataset<span style="color:#f92672">=</span>dense_dataset,
</span></span><span style="display:flex;"><span>    cache_path<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;cache/</span><span style="color:#e6db74">{</span>dense_dataset<span style="color:#f92672">.</span>__class__<span style="color:#f92672">.</span>__name__<span style="color:#e6db74">}</span><span style="color:#e6db74">/train/</span><span style="color:#e6db74">{</span>encoding_dim<span style="color:#e6db74">}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{</span>dt<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>disk_cached_dataloader <span style="color:#f92672">=</span> DataLoader(disk_cached_dataset, <span style="color:#f92672">**</span>dataloader_kwargs)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># cache on disk already available</span>
</span></span><span style="display:flex;"><span>time3 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(disk_cached_dataloader, sinabs_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>time4 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: training_loop(disk_cached_dataloader, exodus_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div>
<div id="chart-result2" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result2.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result2', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>We brought down epoch training time to 14s for the EXODUS model by not having to recompute the <code>ToFrame</code> transform! The speedup comes at the expense of disk space. How much disk space does it cost you may ask? The size of the original dataset file is 2.65 GB compared to the generated cache folder of 1.04 GB, which is not too bad!</p>
<p>The original dataset contained numpy events, whereas the cache folder contains dense tensors. We can compress the dense tensors that much because by default Tonic uses lightweight compression during caching. Disk-caching is generally applicable when training SNNs because it saves you the time to transform your events to dense tensors. Of course you could apply any other deterministic transform before caching it, and also easily apply augmentations to the cached samples as described in <a href="https://tonic.readthedocs.io/en/latest/tutorials/fast_dataloading.html">this tutorial</a>!</p>
<p>Now we notice one more thing. Overall GPU utilisation rate at this point is at ~80%, which means that the GPU is still idling the rest of the time, waiting for new data to arrive. So we can try to go even faster!</p>
<h2 id="gpu-caching">GPU caching</h2>
<p>Instead of loading dense tensors from disk, we can try to cram all our dataset onto the GPU! The issue is that with dense tensors this wouldn&rsquo;t work as they would occupy too much memory. But events are already an efficient format right? So we&rsquo;ll store the events on the GPU as sparse tensors and then simply inflate them as needed by calling to_dense() for each sample. This method is obviously bound by GPU memory so works with rather small datasets such as the one we&rsquo;re testing. However, once you&rsquo;re setup, you can train with <em>blazing</em> speed. For GPU caching we are going to:</p>
<ol>
<li>Create a new sparse dataset on the fly by loading them from the disk cache and calling to_sparse() on the transformed tensors.</li>
<li>Create a new dataloader that now uses a single thread.</li>
<li>Inflate sparse tensors to dense versions by calling to_dense() in the training loop.</li>
</ol>
<!-- raw HTML omitted -->
<p>The sparse tensor dataset takes about 5.7 GB of GPU memory. Not very efficient, but also not terrible. What about training speeds?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gpu_training_loop</span>(model):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> data, targets <span style="color:#f92672">in</span> iter(sparse_tensor_dataloader):
</span></span><span style="display:flex;"><span>        data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>to_dense()
</span></span><span style="display:flex;"><span>        sinabs<span style="color:#f92672">.</span>reset_states(model)
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>cross_entropy(output<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>), targets)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>time5 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: gpu_training_loop(sinabs_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>time6 <span style="color:#f92672">=</span> timeit(<span style="color:#66d9ef">lambda</span>: gpu_training_loop(exodus_model), number<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div>
<div id="chart-result3" style="width:100%; height:500px;"></div>

<script>
    fetch('https:\/\/lenzgregor.com\/data/charts/result3.json')
        .then(response => response.json())
        .then(data => {
            Plotly.newPlot('chart-result3', data.data, data.layout, {
                responsive: true,
                displayModeBar: true,
                displaylogo: false,
                modeBarButtonsToRemove: ['lasso2d', 'select2d']
            });
        });
</script>
<p>We&rsquo;re down to less than 9s per epoch for the EXODUS model, which is another 40% less than disk-caching and an almost 15-fold improvement over the original Sinabs model using the naïve dataloading approach! Now we&rsquo;re really exploiting the GPU as much as possible with a utilisation percentage of ~99%. All this without any qualitative impact on gradient computation.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The principle of caching can be applied to any data that you apply deterministic transformations to, but it pays off particularly well for event-based data. By using cached samples and not having to recompute the same transformations every time, we save ourselves a lot of time during training. If the data already sits on the GPU when it is requested, the speedup is really high. After all, there is a reason why neural network accelerators heavily optimise memory caching to minimise time and energy spent on data movement. So when should you use either disk- or GPU-caching?</p>
<ul>
<li><strong>Disk-caching</strong>: Broadly applicable, useful if you apply deterministic transformations to each sample and you train for many epochs. Not ideal if you&rsquo;re low on disk space.</li>
<li><strong>GPU-caching</strong>: Only really suitable for small datasets and a bit more intricate to setup, but well worth the effort if you want to explore many different architectures / training parameters due to the speed of iteration.</li>
</ul>
<p>As a last note, you might be wondering why we don&rsquo;t cache to the host memory instead of reading from a disk cache. This is totally possible, but the bottleneck at that point really is moving the data onto the GPU, which takes time. Whether the data sits in host memory or is loaded from disk using multiple worker threads doesn&rsquo;t make much of a difference, because the GPU cannot handle the data movement. Since on disk we have much more space available than in RAM, we normally choose to do that.</p>
<p>This tutorial is available <a href="https://github.com/biphasic/snn-training-templates/blob/main/posts/training-snns-faster/index.ipynb">here</a> for you to run, where you&rsquo;ll also find some other training templates.</p>
<p>Acknowledgements: Thanks a lot to Omar Oubari, Mina Khoei and Fabrizio Ottati for the feedback.</p>
]]></content:encoded></item><item><title>Rethinking the way our cameras see.</title><link>https://lenzgregor.com/posts/event-cameras/</link><pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/event-cameras/</guid><description>Neuromorphic vision takes inspiration from the biological vision system.</description><content:encoded><![CDATA[<p>We use them every day and take them for granted: cameras in all their endless shapes and forms. The field of modern computer vision, the ability of machines to see, is based on the common output format of those sensors: frames. However, the way we humans perceive the world with our eyes is very different. Most importantly, we do it with a fraction of the energy needed by a conventional camera. The field of neuromorphic vision tries to understand how our visual system processes information, in order to give modern cameras that same efficiency and it looks like a substantial shift in technology. But let&rsquo;s not get ahead of ourselves.</p>
<h2 id="conventional-imaging-technology">Conventional imaging technology</h2>
<p>We are so focused on working with data that modern cameras provide, that little thought is given about how to capture a scene more efficiently in the first place. Current cameras acquire frames by reading the brightness value of all pixels at the same time at a fixed time interval, the frame rate, regardless of whether the recorded information has actually changed. A single frame acts as a photo; as soon as we stack multiple of them per second it becomes a motion picture. So far so good. This synchronous mechanism makes acquisition and processing predictable. But it comes with a price, namely the recording of redundant data. And not too little of it!</p>
<p><img alt="events" loading="lazy" src="/posts/event-cameras/post-rethinking/frames.gif">
<em>Image blur can occur in a frame depending on the exposure time.</em></p>
<h2 id="our-visual-system">Our visual system</h2>
<p>The human retina has developed to encode information extremely efficiently. Narrowing down the stimuli of about 125 million photoreceptors, which are sensitive to light to just 1 million ganglion cells which relay information to the brain, the retina compresses a visual scene into its most essential parts.
<img alt="events" loading="lazy" src="/posts/event-cameras/post-rethinking/receptive-fields.png">
<em>Center surround receptive fields in the mammalian retina, <a href="https://upload.wikimedia.org/wikipedia/commons/1/16/Receptive_field.png">source</a></em></p>
<p>Photoreceptors are bundled into receptive fields of different sizes for each retinal ganglion cell. The way a receptive field is organised into center and surround cells allows ganglion cells to transmit information not merely about whether photoreceptor cells are exposed to light, but also about the differences in firing rates of cells in the center and surround. This allows them to transmit information about spatial contrast. They are furthermore capable of firing independently of other ganglion cells, thus decoupling the activity of receptive fields from each other. Even if not triggered, a retinal ganglion cell will have a spontaneous firing rate, resulting in millions of spikes per second that travel along the optic nerve. It is thought that in order to prevent the retinal image from fading and thus be able to see the non-moving objects, our eyes perform unintentional rapid ballistic jumps called micro-saccades. This movement only happens once or twice per second, so in between micro-saccades, our vision system probably relies on motion. To put it in a nutshell, our retina acts as a pre-processor for our visual system, extracting contrast as the most important information that then travels along the optical nerve to the visual cortex. In the cortex it is processed for higher-level image synthesis such as depth and motion perception.</p>
<h2 id="taking-inspiration-from-nature">Taking inspiration from nature</h2>
<p>Towards the end of the 80s, a scientist at Caltech named Carver Mead spawned the field of Neuromorphic Engineering, when one of his students called Misha Mahowald developed a new stereo vision system. Taking inspiration from the human visual system, she built what would become the first silicon retina in the early 90s. It was based on the same principle of center surround receptive fields in the human retina, that emit spikes independently of each other depending on the contrast pattern observed.</p>
<p><img alt="misha" loading="lazy" src="/posts/event-cameras/post-rethinking/misha.jpg">
<em>Misha Mahowald (circa 1992) in the ‘Carverland’ lab at Caltech, testing her stereocorrespondence chip. Photo credit: Rodney Douglas.</em></p>
<p>Although Misha drafted the beginning of a new imaging sensor, it did not provide a practical implementation at first. In response, the neuromorphic community simplified the problem by dropping the principle of center-surround pixels. Instead of encoding spatial contrast across multiple pixels which needed sophisticated circuits, the problem could be alleviated by realising a circuit that could encode temporal contrast for single pixels. That way, pixels could still operate individually as processing units just as receptive fields in the retina do and report any deviations in illuminance over time. It would take until 2001 when Tetsuya Yagi at Osaka University and Tobi Delbrück at UZH/ETH in 2008 publish about the first refined temporal contrast sensors, the event cameras as they are known today.</p>
<h2 id="paradigm-shift">Paradigm Shift</h2>
<p>Standard cameras capture absolute illuminance at the same time for all pixels driven by a clock and encoded as frames. One fundamental approach to dealing with temporal redundancy in classical videos is frame difference encoding. This simplest form of video compression includes transmitting only pixel values that exceed a defined intensity change threshold from frame to frame after an initial key-frame. Frame differencing is naturally performed in post-processing, when the data has already been recorded.</p>
<p>Trying to take inspiration from the way our eyes encode information, neuro-morphic cameras capture changes in illuminance over time for individual pixels corresponding to one retinal ganglion cell and its receptive field.</p>
<p><img alt="log-pixel-illuminance" loading="lazy" src="/posts/event-cameras/post-rethinking/log-pixel-illuminance.png">
<em>Principle of how ON and OFF events are generated for each pixel.</em></p>
<p>If light increases or decreases by a certain percentage, one pixel will trigger what&rsquo;s called an event, which is the technical equivalent of a cell&rsquo;s action potential. One event will have a timestamp, x/y coordinates and a polarity depending on the sign of the change. Pixels can fire completely independently of each other, resulting in an overall firing rate that is directly driven by the activity of the scene. It also means that if nothing moves in front of a static camera, no new information is available hence no pixels fire apart from some noise. The absence of accurate measurements of absolute lighting information is a direct result of recording change information. This information can be refreshed by moving the camera itself, much like a microsaccade.</p>
<p>So how do we now get an image from this camera? The short answer is: we don’t. Although we can of course add together all the events per pixel to get an idea of how much the brightness changed (‘binning’), in reality this will not be a reliable estimate, as the electronics of the camera will cause a bit of background noise. As such, the error of your estimate only grows over time.</p>
<p><img alt="events" loading="lazy" src="/posts/event-cameras/post-rethinking/events.gif">
<em>An event-camera will only record change in brightness and encode it as events in x, y and time. Colour is artificial in this visualisation. Note the fine-grained resolution on the t-axis in comparison with the frame animation earlier.</em></p>
<p>Overall an event-camera has three major advantages: Since pixel exposure times are decoupled of each other, very bright and very dark parts can be captured at the same time, resulting in a dynamic range of up to 125dB. In autonomous vehicles, where the lighting can change very quickly or exposure of a single bright spot such as the sun or a reflection should not interfere with the rest, this can save lives. The decoupled, asynchronous nature furthermore frees bandwidth so that changes for one pixel can be recorded at a temporal resolution and latency of microseconds. This makes it possible to track objects with very high speed and without blur. The third advantange is low power consumption due to the sparse output of events, which makes the camera suitable for mobile and embedded applications. Remember that when nothing in front of the camera moves, no redundant data is recorded by the sensor which reduces computational load overall. It also relieves the need for huge raw data files. Current drawbacks for most commercially event-cameras available today are actually further downstream, namely the lack of hardware and algorithms that properly exploit the sparse nature of an event-camera&rsquo;s data. Rethinking even the most basic computer vision algorithms without frames takes a considerable effort. I published some work about purely event-based face detection, have a <a href="https://www.youtube.com/watch?v=F5UzXQsr5Es">look at the video</a>!</p>
<p><img alt="face-detection" loading="lazy" src="/posts/event-cameras/post-rethinking/face-detection.jpeg">
<em>Some snapshots of my work on face detection with event cameras that relies on eye blinks.</em></p>
<h2 id="be-on-the-lookout">Be on the lookout</h2>
<p>So why are these sensors becoming interesting just now? Humanity has learnt to build powerful synchronous hardware such as GPUs that enable high performance, high throughput computing. They provide the power necessary to work with dense image information that are frames. But we’re only ever veering away from the efficiency of a biological system in terms of information processing. Understanding the biological principles of a complex system such as human vision will therefore help create artificial sensors that resemble their biological equivalents. This bears the potential of a low-power sensor with fine-grained temporal resolution.</p>
<p>Since their original inception a few decades ago, it has been quite a journey. Labs at <a href="https://www.ini.uzh.ch/">Zurich</a>, <a href="https://www.westernsydney.edu.au/icns">Sydney</a>, <a href="https://www.edpr.iit.it/">Genua</a>, <a href="http://neuromorphic-vision.com/">my lab in Paris</a>, <a href="https://www.grasp.upenn.edu/">Pittsburgh</a>, <a href="http://sinapse.nus.edu.sg/">Singapore</a> and many more are exploring the concept of event-based computation. Start-up companies such as <a href="https://www.prophesee.ai/">Prophesee</a> or <a href="https://www.celepixel.com/#/Home">Celex</a> compete with established players such as Samsung and Sony in the race to find promising applications for this interesting imaging technology. Potential industry candidates include the automotive industry, neural interfaces, space applications, autonomous agents, … you name it!</p>
<p>If you want to know more about algorithms for event-based cameras, I recommend this <a href="https://arxiv.org/pdf/1904.08405.pdf">survey paper</a> for you. Stay tuned for more articles about neuromorphic engineering on this space! You can also reach out to me via <a href="https://twitter.com/gregorlenz">Twitter</a> or check out some code on <a href="https://github.com/biphasic">Github</a>.</p>
<p>Last but not least I want to thank my former colleague Alexandre Marcireau who made the event visualisations for this article possible with <a href="https://github.com/neuromorphic-paris/command_line_tools">rainmaker</a>!</p>
<p><img alt="omar" loading="lazy" src="/posts/event-cameras/post-rethinking/omar.gif">
<em>My friend Omar causes the camera to heat up.</em></p>
]]></content:encoded></item><item><title>Why digital privacy matters.</title><link>https://lenzgregor.com/posts/digital-privacy/</link><pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate><guid>https://lenzgregor.com/posts/digital-privacy/</guid><description>Reclaim your online identity.</description><content:encoded><![CDATA[<p>Over the past years I started to take back the control of my data online. I deleted a bunch of accounts, some of which I replaced with an open source service. Although I had some doubts as to whether I would seriously miss some of the &lsquo;free&rsquo; services out there, I have to say that it has been a smooth and very rewarding ride so far.
I strongly believe that privacy is a key element for a functioning and free society and should therefore be protected. In the information age that we are in now, this concept has become somewhat opaque, while we citizens have become transparent. So let&rsquo;s have a look at why ditigal privacy matters.</p>
<h2 id="why-even-bother">Why even bother?</h2>
<p>Do I really take any harm if a private company or a government knows what I like and who I talk to? After all it&rsquo;s convenient, right? I think there are multiple aspects to those questions.</p>
<p>First of all it should be clear that being careless about the traces left online can have direct consequences. Ads become tailored to what you like and talk about. Algorithms have become <em>really</em> good at analysing you because they are fed a lot of information about you. Ever saw an ad pop up about something that you just talked about with your friend? That&rsquo;s <a href="https://www.independent.co.uk/life-style/gadgets-and-tech/news/smartphone-apps-listening-privacy-alphonso-shazam-advertising-pool-3d-honey-quest-a8139451.html">no coincidence</a>. You might think, well, I&rsquo;m not influenced by ads. I&rsquo;m afraid a global <a href="https://www.statista.com/topics/990/global-advertising-market/">560 billion US Dollar</a> industry does make sure to get their returns on what they invest in. Everyone is affected by ads. Whether that is a bad idea might be debatable, but I&rsquo;d really rather have the choice.</p>
<p>Online tracking in general is flourishing. Data points collected about you now span multiple devices. If you are someone who earns a decent salary, you could end up <a href="https://www.csmonitor.com/Business/Saving-Money/2016/0405/How-retailers-use-dynamic-pricing-to-get-you-to-pay-more">paying more</a> for the same thing online. You might be <a href="https://www.theguardian.com/uk-news/cambridge-analytica">nudged to vote for a certain candidate</a> in office. And almost certainly your <a href="https://choosetoencrypt.com/search-engines/filter-bubbles-searchencrypt-com-avoids/">newsfeeds are altered accordingly</a>.
These things are not new. And some of them are very convenient. But they happen without the consent of the user, without them being able <a href="https://www.bbc.co.uk/news/technology-44640959">to control it</a>. And this is when we should pay attention I think.</p>
<p>Also private and public bodies should be liable to safeguard the data that they store about you. Nevertheless <a href="https://www.itgovernance.co.uk/blog/list-of-data-breaches-and-cyber-attacks-in-june-2020">June 2020 alone</a> has seen at least 92 security incidents and at least 7,021,195,399 breached records. Although <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a> has improved user rights a lot, it turns out that many institutions are incapable or simply unwilling to put the necessary protection in place. I am willing to believe that the <a href="https://www.theguardian.com/technology/2020/jul/14/huawei-to-be-stripped-of-role-in-uk-5g-network-by-2027-dowden-confirms">recent decision</a> to ban Huawei&rsquo;s 5G equipment in the entire UK was motivated by some legit concerns, however it&rsquo;s really only being stuck between a rock and a hard place when it comes to which country collects your mobile phone locations.</p>
<p>Then there are indirect consequences. People reportedly behave differently when they know that they are being monitored. It might just be that one joke that you decide not to tell on the phone or you might decide not to search for that term because you are going to take a flight to the US the next morning. But our words are subtly altered and our demeanor has changed. These small gestures go a long way and eventually people give up a bit of their freedom and get used to it.</p>
<p>The simple truth is: you just haven&rsquo;t realised your own worth online yet. Let me explain. You might be happy that a service such as facebook or gmail is free to use, but you&rsquo;re providing those companies with organic data that has essentially become a currency itself, or as some call it: <em>the new oil</em>. Companies are making huge profits from it, so why not claim a piece of the pie? I think at some point we will be able to set a price for the data that we&rsquo;re willing to share rather than giving it away for free. There are already <a href="https://datacoup.com/">companies</a> building business models on this.
As Andrew Trask, the clever mind behind <a href="http://openmined.org/">OpenMined</a>, puts it: We&rsquo;re <a href="https://www.youtube.com/watch?v=4zrU54VIK6k&amp;feature=youtu.be&amp;t=2500">spilling our data</a> everywhere, much like people 200 years ago disposed of their waste in public. Was that healthy? Surely not.</p>
<h2 id="is-there-a-way-out">Is there a way out?</h2>
<p>Once a measure of data collection or surveillance is in place, it is hard to get rid of it. That&rsquo;s why it&rsquo;s necessary to act early on. We should be able to explore parts of life privately and without embarrassment or threat.
Bear in mind that it&rsquo;s never too late to start taking ownership of your data as companies are interested in recent trends. If you replaced a service right now it might not affect how you are targeted immediately, but eventually your information will become out of date.
As to things everyone of us can do to reduce their digital footprint, let me say this:</p>
<ul>
<li>
<p>Use <a href="https://www.mozilla.org/en-US/firefox/new/">Firefox</a> or <a href="https://www.torproject.org/">Tor Browser</a>. They come with privacy features built in.
At the very least, check privacy settings and use an adblocker. There are <a href="https://privacybadger.org/">other plugins</a> available for major browsers that I&rsquo;ve found personally useful. If you want to know how trackable your browser is, have a look <a href="https://panopticlick.eff.org/">at this site</a>.</p>
</li>
<li>
<p>If you make sensitive searches related to your health, <a href="https://jezebel.com/what-happens-when-you-tell-the-internet-youre-pregnant-1794398989">pregnancy</a>, your financial situation or anything else you might prefer others not to know, then really use <a href="https://www.torproject.org/">Tor browser</a>.</p>
</li>
<li>
<p>Remember that your e-mail address acts as your unique identifier, which is the easiest way to tie together your data across different services. <a href="https://temp-mail.org/en/">Temporary email services</a> save you from spam when you have to create an account on a site you don&rsquo;t trust.</p>
</li>
<li>
<p>Please say goodbye to Alexa and co. It baffles me that 1 out of 5 households in the UK has a smart speaker at home. One might think that Orwell&rsquo;s 1984 would remind people of the <a href="https://en.wikipedia.org/wiki/Telescreen">Telescreens</a> in everyone&rsquo;s home. If you&rsquo;re not convinced then you might want to check your record of <a href="https://www.theverge.com/2018/5/28/17402154/amazon-echo-alexa-conversation-recording-history-listen-how-to">every conversation recorded</a> in your home by Alexa.</p>
</li>
<li>
<p>Keep your accounts safe. Use two-factor authentication and <a href="https://www.useapassphrase.com/">passphrases</a> rather than passwords. Even better is a reliable <a href="https://www.wired.com/story/best-password-managers/">password manager</a>.</p>
</li>
<li>
<p>Consider saying goodbye to facebook. Maybe <a href="https://en.wikipedia.org/wiki/Criticism_of_Facebook">this wiki page</a> with 556 references at the time of writing can convince you.</p>
</li>
</ul>
<p>How do you know that it works? If you start seeing ads that are really not relevant to you, you are on the right track. You can always check if your email has been part of a known <a href="https://monitor.firefox.com/">data breach</a>.</p>
<h2 id="digital-privacy-in-the-21st-century">Digital privacy in the 21st century</h2>
<p>Lawmakers are slowly moving to empower citizens. The US senate recently brought forward a bill to <a href="https://paleofuture.gizmodo.com/lets-kill-facial-recognition-for-good-1844168367">stop facial recognition</a> within law enforcement completely. The Court of Justice of the EU has <a href="https://noyb.eu/en/cjeu">recently invalidated</a> certain data transfer policies between the EU and the US, urging the US to reform their surveillance laws if they want to continue exchanging data with countries overseas. Public policy develops if people want it to.
In the end a large part of the web is based on trust, and companies will not always stop collecting your data <a href="https://www.businessinsider.com/google-lawsuit-app-tracking-without-permission-reuters-2020-7">even if you opt out</a> of their programs.</p>
<p>People say, <em>If you don&rsquo;t do anything wrong, you have nothing to hide</em>. The problem with this statement is that the definition of &lsquo;wrong&rsquo; can change over time. And when a lot of your online activity is on a <a href="https://www.goodreads.com/book/show/46223297-permanent-record">permanent record</a>, it might be disadvantageous for you under a new administration. And that is not to say that your records <a href="https://www.theverge.com/2020/2/18/21142782/india-politician-deepfakes-ai-elections">cannot be altered</a>.</p>
<p>No matter how securely a single entity might safeguard its users&rsquo; data, it still remains a single point of failure. No system is ultimately safe. That&rsquo;s why decentralization will play a key role in the near future. It surely is one hell of an engineering job to create systems that have their components located in different locations of the network, ensure distributed consensus and at the same time keep identities private, but blockchain technology shows that it&rsquo;s possible.</p>
<p>Digital privacy matters because automated collection and processing of personal data is possible like never before, in ways that ad companies and intelligence services in the analog days could have only dreamed of.
In the end it boils down to a trade-off between freedom and security. Do tailored ads and messenger backdoors limit your freedom? Certainly. Will a Big Brother government ensure security for its well-behaving citizens? <a href="https://time.com/collection/davos-2019/5502592/china-social-credit-score/">Certainly</a>. We just have to be careful not to lose both in the end.</p>
<p>One last thing: Think for a second where technological advancements have led us. Integrated circuit technology allowed devices to shrink from a room-sized to a palm-sized computer. The rate of progress is not going to slow down! In my opinion, we are already very much dependent on that palm-sized device. Think about what it means if we physically connect to the internet. I am convinced that we will see brain machine interfaces become a routine thing within the next decades. We will be able to directly transmit our thoughts to each other, not needing thumbs to interact with a messenger app via a screen. If we do not develop a good understanding of privacy-protecting policies in the digital world, how will you then be able to say that the thoughts you have are really yours? Tell that to Orwell for a start.</p>
<p><a href="https://judiciary.house.gov/news/documentsingle.aspx?DocumentID=3114">https://judiciary.house.gov/news/documentsingle.aspx?DocumentID=3114</a></p>
<p><a href="https://support.brave.com/hc/en-us/articles/360026361072-Brave-Ads-FAQ">https://support.brave.com/hc/en-us/articles/360026361072-Brave-Ads-FAQ</a></p>
]]></content:encoded></item></channel></rss>